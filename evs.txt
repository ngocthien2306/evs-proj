
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\main.py
--------------------------------------------------------------------------------
import multiprocessing
import os
import sys
import time
from PyQt5.QtWidgets import QApplication
from src.infrastructure.utils.system_utils import restart_usb_devices
from src.interface.gui.gui import EventProcessorGUI

def get_base_path():
    if getattr(sys, 'frozen', False):
        return sys._MEIPASS
    return os.path.dirname(os.path.abspath(__file__))


if __name__ == '__main__':
    
    base_path = get_base_path()

    
    if sys.platform.startswith('win'):
        multiprocessing.freeze_support()
    
    if os.getpid() == multiprocessing.current_process().pid:
        app = QApplication(sys.argv)
        restart_usb_devices()
        time.sleep(2)
        window = EventProcessorGUI(base_path)
        window.show()
        sys.exit(app.exec_())
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\hooks\hook-cupy.py
--------------------------------------------------------------------------------
import cupy
import sys
import os
from PyInstaller.utils.hooks import collect_dynamic_libs, collect_data_files

# Auto collect all cupy related modules
hiddenimports = [x for x in sys.modules.keys() 
                 if x.startswith('cupy.') or 
                    x.startswith('cupyx.') or 
                    x.startswith('cupy_backends.')] + [
                    'fastrlock',
                    'fastrlock.rlock'
                ]
# Collect binaries and data files
binaries = collect_dynamic_libs('cupy')
datas = collect_data_files('cupy')

# Add the specific CUDA DLLs that are needed
cuda_path = os.environ.get('CUDA_PATH', 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1')
cuda_bin = os.path.join(cuda_path, 'bin')

additional_dlls = [
    'nvrtc64_121_0.dll',  # Also commonly needed
    'cublas64_12.dll',    # May be needed for CuPy operations
    'cufft64_11.dll',     # May be needed for CuPy operations
]

for dll in additional_dlls:
    dll_path = os.path.join(cuda_bin, dll)
    if os.path.exists(dll_path):
        binaries.append((dll_path, '.'))
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\hooks\hook-cv.py
--------------------------------------------------------------------------------
from PyInstaller.utils.hooks import collect_dynamic_libs, collect_data_files

hiddenimports = ['numpy']
binaries = collect_dynamic_libs('cv2')
datas = collect_data_files('cv2')
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\hooks\hook-numpy.py
--------------------------------------------------------------------------------
import os
import numpy
from PyInstaller.utils.hooks import collect_dynamic_libs, collect_data_files

numpy_dir = r'C:\Users\pclab321\miniconda3\envs\evs_build\lib\site-packages\numpy'
numpy_core_dir = os.path.join(numpy_dir, 'core')

hiddenimports = [
    'numpy',
    'numpy.core._multiarray_umath',
    'numpy.core.multiarray',
    'numpy.core.numeric',
    'numpy.core._dtype_ctypes',
    'numpy.lib.format',
    'numpy.core._multiarray_tests',
    'numpy.core.numerictypes',
    'numpy.core.fromnumeric',
    'numpy.core.arrayprint'
]

binaries = []
core_libs = []

# Collect all PYD files from numpy.core
for f in os.listdir(numpy_core_dir):
    if f.endswith('.pyd'):
        full_path = os.path.join(numpy_core_dir, f)
        rel_path = os.path.join('numpy', 'core')
        core_libs.append((full_path, rel_path))

binaries.extend(core_libs)
binaries.extend(collect_dynamic_libs('numpy'))
datas = collect_data_files('numpy')
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\interfaces\counter.py
--------------------------------------------------------------------------------
from abc import ABC, abstractmethod
from typing import List
from ...domain.entities.detection import Detection
from ...domain.value_objects.counting_zone import CountingZone

class ICounter(ABC):
    @abstractmethod
    def update(self, tracks: List[Detection], zones: List[CountingZone]) -> List[CountingZone]:
        """Update counting based on tracks and zones"""
        pass
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\interfaces\detector.py
--------------------------------------------------------------------------------
from abc import ABC, abstractmethod
from typing import List
from ...domain.entities.detection import Detection, Frame

class IDetector(ABC):
    @abstractmethod
    def detect(self, frame: Frame) -> List[Detection]:
        """Perform detection on a frame"""
        pass
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\interfaces\event_iterator.py
--------------------------------------------------------------------------------
from abc import ABC, abstractmethod
from typing import Iterator, Any

class IEventIterator(ABC):
    @abstractmethod
    def __iter__(self) -> Iterator[Any]:
        pass

    @abstractmethod
    def get_size(self) -> int:
        pass
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\interfaces\tracker.py
--------------------------------------------------------------------------------
from abc import ABC, abstractmethod
from typing import List
from ...domain.entities.detection import Detection

class ITracker(ABC):
    @abstractmethod
    def update(self, detections: List[Detection]) -> List[Detection]:
        """Update tracks with new detections"""
        pass
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\services\config_service.py
--------------------------------------------------------------------------------
import yaml
from dataclasses import dataclass
from typing import List, Optional, Dict, Any, Union, Tuple
from pathlib import Path

from src.domain.entities.config import CountingConfig, CountingZoneConfig, InputConfig, ModelConfig, SystemConfig, TrackingConfig, VisualizationConfig


class ConfigService:
    @staticmethod
    def load_config(config_path: str = "configs/default_config.yaml") -> SystemConfig:
        """Load system configuration from YAML file"""
        with open(config_path, 'r') as f:
            config_dict = yaml.safe_load(f)

        # Convert dictionary to typed config objects
        input_config = InputConfig(
            type=config_dict['input'].get('type', 'event'),
            file_path=config_dict['input']['file_path'],
            width=config_dict['input']['width'],
            height=config_dict['input']['height'],
            target_fps=config_dict['input'].get('target_fps'),
            bias_file=config_dict['input'].get('bias_file'),
            delta_t=config_dict['input'].get('delta_t', 10000),
            crop_coordinates=config_dict['input'].get('crop_coordinates')
        )
        
        model_config = ModelConfig(**config_dict['model'])
        tracking_config = TrackingConfig(**config_dict['tracking'])
        visualization_config = VisualizationConfig(**config_dict['visualization'])
        
        # Convert zone configs
        zones = [
            CountingZoneConfig(**zone)
            for zone in config_dict['counting']['zones']
        ]
        
        # Create counting config
        counting_config = CountingConfig(
            type=config_dict['counting']['type'],
            zones=zones,
            alpha=config_dict['counting'].get('alpha', 0.3),
            count_threshold=config_dict['counting'].get('count_threshold', 2),
            temporal_window=config_dict['counting'].get('temporal_window', 5)
        )

        return SystemConfig(
            input=input_config,
            model=model_config,
            tracking=tracking_config,
            visualization=visualization_config,
            counting=counting_config
        )

    @staticmethod
    def get_default_config() -> SystemConfig:
        """Get default configuration when no config file is provided"""
        return ConfigService.load_config()

    @staticmethod
    def validate_config(config: SystemConfig) -> bool:
        """Validate configuration values"""
        try:
            # Validate input type
            if config.input.type not in ["video", "event"]:
                raise ValueError(f"Invalid input type: {config.input.type}")

            # Validate input file path
            if not Path(config.input.file_path).exists():
                raise FileNotFoundError(f"Input file not found: {config.input.file_path}")

            # Validate dimensions
            if config.input.width <= 0 or config.input.height <= 0:
                raise ValueError("Invalid dimensions")

            # Validate bias file if provided
            if config.input.bias_file and not Path(config.input.bias_file).exists():
                raise FileNotFoundError(f"Bias file not found: {config.input.bias_file}")

            # Validate model path
            if not Path(config.model.path).exists():
                raise FileNotFoundError(f"Model file not found: {config.model.path}")

            # Validate confidence threshold
            if not 0 <= config.model.confidence_threshold <= 1:
                raise ValueError("Confidence threshold must be between 0 and 1")

            # Validate tracking parameters
            if config.tracking.max_age <= 0:
                raise ValueError("max_age must be positive")
            if config.tracking.min_hits <= 0:
                raise ValueError("min_hits must be positive")
            if not 0 <= config.tracking.iou_threshold <= 1:
                raise ValueError("iou_threshold must be between 0 and 1")

            # Validate counting parameters
            if config.counting.type not in ["line", "frame"]:
                raise ValueError(f"Invalid counting type: {config.counting.type}")

            if config.counting.type == "line" and not config.counting.zones:
                raise ValueError("Line counting requires at least one zone")

            return True

        except Exception as e:
            print(f"Configuration validation failed: {str(e)}")
            return False
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\services\detection_service.py
--------------------------------------------------------------------------------
from typing import List, Optional, Union
import logging
import time
from queue import Queue, Full, Empty
from threading import Event

from src.infrastructure.counting.frame_counter import FrameCounter
from src.infrastructure.counting.line_counter import LineCounter
from src.application.interfaces.counter import ICounter
from src.application.interfaces.detector import IDetector
from src.application.interfaces.tracker import ITracker
from src.application.interfaces.event_iterator import IEventIterator
from src.domain.entities.detection import Frame
from src.infrastructure.event_camera.processors.event_processor import EventProcessor
from src.domain.entities.detection import Detection
from src.domain.value_objects.counting_zone import CountingZone


class DetectionService:
    def __init__(self,
                 event_iterator: IEventIterator,
                 detector: IDetector,
                 tracker: ITracker,
                 counter: Optional[Union[LineCounter, FrameCounter]] = None,
                 event_processor: Optional[EventProcessor] = None,
                 max_buffer_size: int = 1):
        
        self.event_iterator = event_iterator
        self.detector = detector
        self.tracker = tracker
        self.counter = counter
        self.event_processor = event_processor or EventProcessor()
        self.image_queue = Queue(maxsize=max_buffer_size)
        self.running = Event()
        self._frame_count = 0
        self._fps_start_time = time.time()
        self._current_fps = 0
        
    def process_events(self) -> None:
        self.running.set()
        try:
            for events in self.event_iterator:
                if not self.running.is_set():
                    break

                # Process events into frame
                event_frame = self.event_processor.create_frame(events)
                
                try:
                    frame = Frame(
                        data=event_frame.data,
                        height=event_frame.data.shape[0],
                        width=event_frame.data.shape[1]
                    )
                    self.image_queue.put(frame, timeout=0.1)
                except Full:
                    logging.warning("Image queue is full, skipping frame.")
                    continue

        except Exception as e:
            logging.error(f"Error in process_events: {e}")
        finally:
            self.running.clear()

    def process_frames(self, counting_zones: Optional[List[CountingZone]] = None) -> None:
        self.running.set()
        try:
            while self.running.is_set():
                try:
                    frame = self.image_queue.get(timeout=0.001)
                except Empty:
                    continue

                self._update_fps()
                frame.fps = self._current_fps

                # Detect objects
                detections = self.detector.detect(frame)
                
                if isinstance(self.counter, LineCounter):
                    tracked_detections = self.tracker.update(detections)
                    frame.detections = tracked_detections
                    
                    if counting_zones:
                        counting_zones = self.counter.update(
                            tracked_detections, 
                            counting_zones
                        )
                    yield frame, counting_zones
                    
                elif isinstance(self.counter, FrameCounter):
                    frame.detections = detections
                    
                    counting_result = self.counter.update(detections)
                    yield frame, counting_result
                    
                else:
                    frame.detections = detections
                    yield frame, None

                self.image_queue.task_done()

        except Exception as e:
            logging.error(f"Error in process_frames: {e}")
        finally:
            self.running.clear()

    def _update_fps(self) -> None:
        self._frame_count += 1
        if time.time() - self._fps_start_time >= 1.0:
            self._current_fps = self._frame_count
            self._frame_count = 0
            self._fps_start_time = time.time()
            
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\services\event_processing_service.py
--------------------------------------------------------------------------------
from typing import Optional, Tuple
import numpy as np
from ...domain.value_objects.event_frame import EventFrame
from ...infrastructure.event_camera.processors.event_processor import EventProcessor

class EventProcessingService:
    def __init__(self, 
                 width: int = 346,
                 height: int = 260,
                 crop_coordinates: Optional[Tuple[int, int, int, int]] = None):
        self.processor = EventProcessor(
            width=width,
            height=height,
            crop_coordinates=crop_coordinates
        )

    def process_events(self, events: np.ndarray) -> EventFrame:
        """
        Process events into an image frame
        
        Args:
            events: Numpy array of events
            
        Returns:
            EventFrame containing the processed image
        """
        return self.processor.create_frame(events)

    def reset(self) -> None:
        """Reset the event processor's internal state"""
        self.processor.reset_buffer()
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\services\event_service.py
--------------------------------------------------------------------------------
from typing import Optional, Union
from ...domain.value_objects.bias_settings import BiasSettings
from ..interfaces.event_iterator import IEventIterator
from ...infrastructure.event_camera.iterators.buffered_iterator import BufferedEventIterator
from ...infrastructure.event_camera.iterators.non_buffered_iterator import NonBufferedEventIterator
from ...infrastructure.event_camera.utils.bias_file_loader import load_bias_file

class EventService:
    @staticmethod
    def create_iterator(
        delta_t: float,
        input_filename: Optional[str] = None,
        bias_file: Optional[str] = None,
        buffered: bool = True
    ) -> IEventIterator:
        bias_settings = None
        if bias_file:
            bias_settings = load_bias_file(bias_file)
        elif not input_filename:  # Live camera with default settings
            bias_settings = BiasSettings.create_default()

        Iterator = BufferedEventIterator if buffered else NonBufferedEventIterator
        return Iterator(delta_t, input_filename, bias_settings)
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\services\main_dll.py
--------------------------------------------------------------------------------
import logging
import multiprocessing
import signal
import traceback
import cv2
import time
import numpy as np
from collections import deque
from queue import Full, Empty
from multiprocessing import Process, Queue, Event

from src.domain.value_objects.config import RadarConfig
from src.domain.entities.entities import ProcessingParams
from src.core.filters import ActivityNoiseFilterGPU, BoundingBox, filter_events
from src.core.radar import ActivityMonitor, RadarViewer
from src.infrastructure.ml_models.evs_fall import evs_fall_exist
from src.infrastructure.ml_models.onnx_detect import ONNXDetector
from src.infrastructure.event_camera.iterators.bias_events_iterator import BiasEventsIterator
from src.infrastructure.event_camera.processors.event_processor import EventProcessor
from src.infrastructure.utils.utils import draw_radar_background, make_binary_histo
import cupy as cp
import os
import sys
from multiprocessing.shared_memory import SharedMemory
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
os.environ['PYTHONWARNINGS'] = 'ignore'

MAX_BBOXES = 3
SENTINEL = -1.0

def get_bbox_shm():
    shm_name = "bbox_shm"
    size = MAX_BBOXES * 4 * 5
    try:
        shm = SharedMemory(name=shm_name)
    except FileNotFoundError:
        shm = SharedMemory(name=shm_name, create=True, size=size)
    return shm

def get_base_path():
    if getattr(sys, 'frozen', False):
        return sys._MEIPASS
    return os.path.dirname(os.path.abspath(__file__))

base_path = get_base_path()
yolo_path = os.path.join(base_path, "yolo_models")
tflite_path = os.path.join(base_path, "tflite_models")
onnx_path = os.path.join(base_path, "onnx_models")
config_path = os.path.join(base_path, "configs")

class PeopleFall:
    def __init__(self, model_type='onnx', buffer_size=60):
        """Initialize PeopleFall

        Args:
            fall_model: Fall detection model instance.
            buffer_size: Size of the buffer for smoothing fall and existence detection.
        """
        self.fall_model = os.path.join(onnx_path, 'WQ_TiVi.onnx')
        self.Frame_Array = deque(maxlen=buffer_size)
        self.idx_a = np.array([ii for ii in range(buffer_size) if ii % 2 == 0])

        # Placeholder for the fall detector instance
        self.model_type = model_type
        self.evs_fall_detector = evs_fall_exist(self.fall_model,self.model_type, num_threads=1)

        # Buffers for fall and existence smoothing
        self.FALL_Arr = deque([False] * 10, maxlen=10)
        self.EXIST_Arr = deque([False] * 10, maxlen=10)

    def process_frame(self, frame):
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        self.Frame_Array.append(gray_frame)
        if len(self.Frame_Array) == 60:
            start_time = time.time()
            
            Frame_list = list(self.Frame_Array)
           
            # Run the fall detection model
            output_fall, output_exists = self.evs_fall_detector(Frame_list, self.idx_a)
            # print(output_fall)
            pred_fall = np.squeeze(output_fall)
            pred_exists = np.squeeze(output_exists)

            # Update fall buffer
            if pred_fall >= 0.5:
                class_action = 'FALL'
                self.FALL_Arr.append(True)
            else:
                class_action = 'Normal'
                self.FALL_Arr.append(False)

            # Update existence buffer
            if pred_exists >= 0.5:
                class_exists = 'Person'
                self.EXIST_Arr.append(True)
            else:
                class_exists = 'No Person'
                self.EXIST_Arr.append(False)

            # Determine if a person exists
            class_person_exists = 'Person' if self.EXIST_Arr.count(True) > 5 else ''

            # print('Num of Fall: ', self.FALL_Arr.count(True))
            if self.FALL_Arr.count(True) >= 5:
                param = '*** FALL detected!  sending message.'
                
                self.FALL_Arr = deque([False] * 10, maxlen=10)  # Reset fall buffer
                self.Frame_Array.clear()  # Clear frame buffer
            else:
                self.Frame_Array = deque(Frame_list[5:], maxlen=60)
               
            end_time = time.time()
            print("Fall detect time: ", (round((end_time - start_time) * 1000, 2)))
        
            return class_action
        else:
            return "Normal"
      
def restart_usb_devices():
    try:
        import subprocess
        print("Starting Setup ...")
        subprocess.run(['pnputil', '/restart-device', 'USB\\VID*'], 
                      capture_output=True, 
                      text=True, 
                      creationflags=subprocess.CREATE_NO_WINDOW)
        time.sleep(2)  
        return True
    except Exception as e:
        print(f"Failed to restart USB: {e}")
        return False

def event_process(frame_queue, frame_queue_frame, event_queue , running, input_filename, camera_resolution):
    try:
        w, h = camera_resolution
        processor = EventProcessor(
            width=w,
            height=h,
            crop_coordinates=None
        )
        
        iterator = None if not input_filename else BiasEventsIterator(
            delta_t=33000,
            input_filename=input_filename,
            bias_file=os.path.join(config_path, 'bias.bias')
        )
        
        zero_event_counter = 0
        max_zero_events = 10
        error_counter = 0
        max_errors = 3
        
        while running.is_set():
            try:
                if iterator is None and not input_filename:
                    try:
                        iterator = BiasEventsIterator(
                            delta_t=33000,
                            input_filename=input_filename,
                            bias_file=os.path.join(config_path, 'bias.bias')
                        )
                        error_counter = 0  
                    except Exception as e:
                        print(f"Failed to create iterator: {e}")
                        error_counter += 1
                        if error_counter >= max_errors:
                            restart_usb_devices()
                            error_counter = 0
                        time.sleep(1)
                        continue
                
                for events in iterator:
                    
                    if not running.is_set():
                        break
                    
                    start_time = time.time()
                    
                    event_count = len(events)
                    # print(event_count)

                    if event_count == 0:
                        zero_event_counter += 1
                        if zero_event_counter >= max_zero_events:
                            print("Too many zero events, reloading USB...")
                            if iterator:
                                del iterator
                                iterator = None
                            if restart_usb_devices():
                                print("USB restarted successfully")
                            else:
                                print("Failed to restart USB")
                            zero_event_counter = 0
                            break
                    else:
                        zero_event_counter = 0 
                    if event_queue.full():
                        try:
                            event_queue.get_nowait()
                        except Empty:
                            pass
                    try:
                        event_queue.put((events, time.time()))
                    except Full:
                        pass
                    event_frame = processor.create_frame(events)
                    # cv2.imwrite("event_frame.png", event_frame.data)
                    end_time = time.time()
                    processing_time = round((end_time - start_time) * 1000, 2)
                    
                    
                    
                    if frame_queue.full():
                        try:
                            frame_queue.get_nowait()
                        except Empty:
                            pass

                    try:
                        frame_queue.put((event_frame, time.time(), processing_time))
                    except Full:
                        pass
                    

                    if frame_queue_frame.full():
                        try:
                            frame_queue_frame.get_nowait()
                        except Empty:
                            pass

                    try:
                        frame_queue_frame.put((event_frame, time.time(), processing_time))
                    except Full:
                        pass
                    
                if input_filename:  # File mode
                    print("File processing completed")
                    frame_queue.put(("EOF", time.time()))
                    break
                else:  # Camera mode
                    if iterator:
                        del iterator
                        iterator = None

            except Exception as e:
                print(f"Iterator error: {e}")
                if "TimeHigh discrepancy" in str(e):
                    print("TimeHigh discrepancy detected, reloading USB...")
                    if iterator:
                        del iterator
                        iterator = None
                    restart_usb_devices()
                    continue
                
                error_counter += 1
                if error_counter >= max_errors:
                    print(f"Too many errors ({error_counter}), restarting USB...")
                    restart_usb_devices()
                    error_counter = 0
                
                if iterator:
                    del iterator
                    iterator = None
                
                if not input_filename:  # Chỉ retry với camera mode
                    time.sleep(1)
                else:
                    break

    except Exception as e:
        print(traceback.print_exc())
        logging.error(f"Error in event process: {e}")
    finally:
        if iterator:
            del iterator
        running.clear()

def people_detection_process_onnx(frame_queue, people_queue, running, model_path, args: ProcessingParams):
    """
    Process frames for people detection using ONNX model
    
    Args:
        frame_queue: Queue for input frames
        people_queue: Queue for detection results
        running: Event to control process
        model_path: Path to ONNX model
        args: Processing parameters
    """
    try:
        # Initialize detector
        detector = ONNXDetector(model_path, args)
        last_frame = None
        
        while running.is_set():
            try:
                # Get frame from queue
                event_frame, timestamp, _ = frame_queue.get(timeout=0.1)
                frame = event_frame.data
                last_frame = frame
            except Empty:
                if last_frame is None:
                    continue
                frame = last_frame
                timestamp = time.time()
            
            # Process frame
            start_time = time.time()
            
            # Run detection
            boxes, boxes_n, current_count = detector.detect_and_track(frame)
            
            # Create tracked frame
            tracked_frame = frame.copy()
            
            # Draw detections
            if current_count > 0:
                for box in boxes:
                    x1, y1, x2, y2 = box.astype(int)
                    cv2.rectangle(tracked_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(tracked_frame, "Person", (x1, y1 - 10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            # Calculate processing time
            end_time = time.time()
            processing_time = round((end_time - start_time) * 1000, 2)
            
            # cv2.imwrite("tracked_frame.png", tracked_frame)
            
            # Handle queue operations
            if people_queue.full():
                try:
                    people_queue.get_nowait()
                except Empty:
                    pass
            
            try:
                people_queue.put((tracked_frame, boxes_n, current_count, timestamp, processing_time))
            except Full:
                people_queue.put((frame, [], 0, timestamp, 0))
                continue
                
    except Exception as e:
        traceback.print_exc()
        logging.error(f"Error in people detection process: {e}")
    finally:
        running.clear()
        
def people_detection_process(frame_queue, people_queue, running, model_path, args: ProcessingParams):
    bbox_shm = get_bbox_shm()
    boxes_shared = np.ndarray((MAX_BBOXES, 4), dtype=np.float32, buffer=bbox_shm.buf)
    
    try:
        # model = YOLO(model_path, task="detect")
        model = None
        last_frame = None
        while running.is_set():
            try:
                event_frame, timestamp, _ = frame_queue.get(timeout=0.1)
                frame = event_frame.data
                last_frame = frame
            except Empty:
                if last_frame is None:
                    continue
                frame = last_frame
                timestamp = time.time()
                
            start_time = time.time()
            results = model.track(
                frame, 
                tracker=os.path.join(os.path.dirname(os.path.abspath(__file__)), "configs", "botsort.yaml"),
                persist=True,
                verbose=False,
                conf=args.yolo_conf,
                iou=args.yolo_iou,
                imgsz=320
            )
            
            tracked_frame = frame.copy()
            current_count = 0
            boxes_n = []
            
            if results and results[0].boxes.id is not None:
                boxes_n = results[0].boxes.xyxyn.cpu().numpy()
                track_ids = results[0].boxes.id.cpu().numpy()
                current_count = len(track_ids)
                num_boxes = min(current_count, MAX_BBOXES)
                boxes_shared[:num_boxes] = boxes_n[:num_boxes].astype(np.float32)
                if num_boxes < MAX_BBOXES:
                    boxes_shared[num_boxes:] = SENTINEL
                    
                for box in results[0].boxes.xyxy.cpu().numpy():
                    x1, y1, x2, y2 = box.astype(int)
                    cv2.rectangle(tracked_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(tracked_frame, "Person", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            else:
                boxes_shared[:] = SENTINEL
            end_time = time.time()
            processing_time = round((end_time - start_time) * 1000, 2)
            if people_queue.full():
                try:
                    people_queue.get_nowait()
                except Empty:
                    pass
            try:
                people_queue.put((tracked_frame, boxes_n, current_count, timestamp, processing_time))
            except Full:
                people_queue.put((frame, [], 0, timestamp, 0))
                continue
    except Exception as e:
        traceback.print_exc()
        logging.error(f"Error in people detection process: {e}")
    finally:
        running.clear()

# def convert_tracker_boxes(events, normalized_boxes, frame_width: int, frame_height: int, expansion_ratio: float = -0.2, max_peak: int = 3):
#     bb_list = [BoundingBox(det[0], det[1], det[2], det[3], frame_width, frame_height, expansion_ratio)
#                for det in normalized_boxes]
#     if len(normalized_boxes) == 0:
#         return np.empty(0, dtype=events.dtype)
#     x = events['x']
#     y = events['y']
#     if y.dtype != np.int32:
#         y = y.astype(np.int32)
#     valid_mask = (x >= 0) & (x < frame_width) & (y >= 0) & (y < frame_height)
#     if len(normalized_boxes) == 1:
#         det = normalized_boxes[0]
#         bb = BoundingBox(det[0], det[1], det[2], det[3], frame_width, frame_height, expansion_ratio)
#         bbox_mask = (x >= bb.x_min) & (x <= bb.x_max) & (y >= bb.y_min) & (y <= bb.y_max)
#     else:
#         bb_list = [BoundingBox(det[0], det[1], det[2], det[3], frame_width, frame_height, expansion_ratio)
#                    for det in normalized_boxes[:max_peak]]
#         bbox_masks = [(x >= bb.x_min) & (x <= bb.x_max) & (y >= bb.y_min) & (y <= bb.y_max) for bb in bb_list]
#         bbox_mask = np.logical_or.reduce(bbox_masks)
#     combined_mask = valid_mask & bbox_mask
#     filtered_events = events[combined_mask]
#     return filtered_events
def convert_tracker_boxes(events, normalized_boxes, frame_width: int, frame_height: int, expansion_ratio: float = -0.2, max_threads: int = 3):
    bb_list = [BoundingBox(det[0], det[1], det[2], det[3], frame_width, frame_height, expansion_ratio)
               for det in normalized_boxes]
    x = events['x']
    y = events['y'].astype(np.int32)
    print("Bbox length:", len(normalized_boxes))
    masks = [((x >= bb.x_min) & (x <= bb.x_max) & (y >= bb.y_min) & (y <= bb.y_max))
             for bb in bb_list[:max_threads]]
    combined_mask = np.logical_or.reduce(masks)
    x_filtered = x[combined_mask]
    y_filtered = y[combined_mask]
    filtered_size = len(x_filtered)
    valid_indices = (x >= 0) & (x < frame_width) & (y >= 0) & (y < frame_height)
    filtered_events = np.empty(filtered_size, dtype=events.dtype)
    filtered_events['t'] = events['t'][valid_indices][:filtered_size]
    filtered_events['p'] = events['p'][valid_indices][:filtered_size]
    filtered_events['x'] = x_filtered
    filtered_events['y'] = y_filtered
    return filtered_events

def convert_events_data(events: np.ndarray,):
    if events.dtype != [('x', '<u2'), ('y', '<u2'), ('p', '<i2'), ('t', '<i8')]:
            events = np.array(events, 
                            dtype=[('x', '<u2'), ('y', '<u2'), 
                                  ('p', '<i2'), ('t', '<i8')])
    return events

def person_direction_process(event_queue, radar_queue, running, args: ProcessingParams):
    bbox_shm = get_bbox_shm()
    boxes_shared = np.ndarray((MAX_BBOXES, 4), dtype=np.float32, buffer=bbox_shm.buf)
    last_valid_angles = []
    no_movement_counter = 0
    last_frame_time = time.time()
    last_peak_time = time.time()
    last_bbox = None
    last_bbox_time = 0
    frame_width = args.camera_width
    frame_height = args.camera_height
    radar_height = int(frame_height * 0.95)
    radar_width = int(frame_width * 1.47)
    config = RadarConfig()
    lateral_fov_deg = np.rad2deg(config.lateral_fov)
    pixel_margin = int(config.trim_angle_deg / (lateral_fov_deg / frame_width))
    activity_monitor = ActivityMonitor(config, frame_width)
    radar_viewer = RadarViewer(config, radar_width, radar_height)
    if config.use_filter:
        activity_filter = ActivityNoiseFilterGPU(
            width=frame_width,
            height=frame_height,
            threshold=config.delta_t,
            neighborhood_size=args.neighborhood_size,
            time_tolerance=args.time_tolerance
        )
    else:
        activity_filter = None

    def update_radar_visualization(ev_rate_gpu: cp.ndarray) -> np.ndarray:
        nonlocal last_frame_time, last_valid_angles, last_peak_time
        radar_img = np.copy(radar_viewer.linear_bins_cpu)
        draw_radar_background(radar_img, radar_width, radar_height, config.lateral_fov)
        frame_delta = time.time() - last_frame_time
        last_frame_time = time.perf_counter()
        if no_movement_counter > 10 and last_valid_angles:
            angles_to_show = [float(a + v) for a, v in zip(last_valid_angles, radar_viewer.angle_velocities)]
        else:
            angles_found = radar_viewer.compute_view(ev_rate_gpu, radar_img, frame_delta)
            angles_to_show = radar_viewer.stabilize_angles(angles_found)
            if angles_to_show:
                last_valid_angles = angles_to_show.copy()
                last_peak_time = time.time()  
        if (not angles_to_show or len(angles_to_show) == 0) and (time.time() - last_peak_time > 1):
            radar_viewer.tracked_peaks.clear()
            last_valid_angles = []
            draw_radar_background(radar_img, radar_width, radar_height, config.lateral_fov)
        
        return radar_img

    try:
        last_events = None
        while running.is_set():
            try:
                events_data, timestamp = event_queue.get(timeout=0.1)
                events = convert_events_data(events_data)
                event_image = make_binary_histo(events, width=args.camera_width, height=args.camera_height)
                # cv2.imwrite("filter.png", event_image)
            except Empty:
                if last_events is None:
                    continue
                events = last_events
                timestamp = time.time()
            last_events = events
            start_time = time.time()
            if activity_filter:
                processed_events = activity_filter.process_events_gpu(events)
                if processed_events is None or processed_events["x"].size == 0:
                    continue
                x_gpu = processed_events['x']
                y_gpu = processed_events['y']
            else:
                x_gpu = cp.asarray(events['x'], dtype=cp.int32)
                y_gpu = cp.asarray(events['y'], dtype=cp.int32)
            if pixel_margin > 0:
                mask = (x_gpu >= pixel_margin) & (x_gpu < (frame_width - pixel_margin))
                x_filtered_gpu = x_gpu[mask]
                y_filtered_gpu = y_gpu[mask]
            else:
                x_filtered_gpu = x_gpu
                y_filtered_gpu = y_gpu
            current_time = time.time()
            valid_boxes = [box for box in boxes_shared if not np.allclose(box, [SENTINEL]*4)]
            count = len(valid_boxes)
            if valid_boxes:
                last_bbox = np.array(valid_boxes)
                last_bbox_time = current_time
                radar_viewer.update_max_objects(max(2,count))
            else:
                if last_bbox is not None and (current_time - last_bbox_time) < 1:
                    pass
                else:
                    boxes_shared[:] = SENTINEL
                    last_bbox = None
                    x_filtered_gpu = cp.asarray([], dtype=cp.int32)
                    y_filtered_gpu = cp.asarray([], dtype=cp.int32)
                    

            activity_monitor.process_events(x_filtered_gpu)
            activity_monitor.reset()
            ev_rate_gpu = activity_monitor.get_ev_rate_per_bin(return_to_cpu=False)
            total_events = activity_monitor.get_total_event_count()
            if total_events < config.low_event_threshold:
                no_movement_counter += 1
                if no_movement_counter > 5:
                    last_valid_angles = []
                    radar_viewer.angle_velocities = {}
            else:
                no_movement_counter = 0
            radar_img = update_radar_visualization(ev_rate_gpu)
            proc_time = round((time.time() - start_time) * 1000, 2)
            if radar_queue.full():
                try:
                    radar_queue.get_nowait()
                except Empty:
                    pass
            try:
                print(proc_time)
                radar_queue.put((radar_img, timestamp, proc_time))
            except Full:
                continue
    except Exception as e:
        traceback.print_exc()
        logging.error(f"Error in person direction process: {e}")
    finally:
        running.clear()

def fall_detection_process(frame_queue, fall_queue, running):
    try:
        fall_detector = PeopleFall()
        last_frame = None
        
        while running.is_set():
            try:
                event_frame, timestamp, _ = frame_queue.get(timeout=0.1)
                frame = event_frame.data
                h, w, _ = frame.shape
                if h != 320 or w != 320:
                    frame = cv2.resize(frame, (320, 320))
                    
                last_frame = frame
            except Empty:
                if last_frame is None:
                    continue
                frame = last_frame
                timestamp = time.time()

            start_time = time.time()
            # Run fall detection
            fall_status = fall_detector.process_frame(frame)
            end_time = time.time()
            processing_time = round((end_time - start_time) * 1000, 2)
            if fall_queue.full():
                try:
                    fall_queue.get_nowait()
                except Empty:
                    pass

            try:
                fall_queue.put((fall_status, timestamp, processing_time))
            except Full:
                continue

    except Exception as e:
        traceback.print_exc()
        logging.error(f"Error in fall detection process: {e}")
    finally:
        running.clear()
        
def visualization_process(people_queue, fall_queue, radar_queue, result_queue, running, frame_height, args: ProcessingParams):
    TARGET_HEIGHT = frame_height
    fall_display_counter = 0

    last_stats_update = time.time()
    current_fps = 0
    current_people_time = 0
    current_radar_time = 0 
    current_fall_time = 0
    update_interval = 1.0  
    
    try:
        while running.is_set():
            try:
                # Get data from queues
                frame, boxes, count, p_timestamp, people_time = people_queue.get(timeout=0.01)
                frame_radar, r_timestamp, radar_time = radar_queue.get(timeout=0.01)
                try:
                    fall_status, f_timestamp, fall_time = fall_queue.get_nowait()
                except Empty:
                    fall_status = "Normal"
                    fall_time = 0

                # 1. Process event image (left side)
                event_target_height = TARGET_HEIGHT
                event_target_width = TARGET_HEIGHT  # Keep it square as original
                
                event_image_resized = cv2.resize(frame, 
                                            (event_target_width, event_target_height),
                                            interpolation=cv2.INTER_AREA)
                
                # 2. Process radar image
                radar_target_height = int(TARGET_HEIGHT * 2/3)
                radar_aspect_ratio = frame_radar.shape[1] / frame_radar.shape[0]
                radar_target_width = int(radar_target_height * radar_aspect_ratio)
                
                if radar_target_width > event_target_width:
                    radar_target_width = event_target_width
                    radar_target_height = int(radar_target_width / radar_aspect_ratio)
                
                radar_image_resized = cv2.resize(frame_radar,
                    (radar_target_width, radar_target_height),
                    interpolation=cv2.INTER_AREA)
                
                # 3. Create right side canvas
                right_side = np.zeros((TARGET_HEIGHT, event_target_width, 3), dtype=np.uint8)
    
                # Draw radar on top
                x_offset = (event_target_width - radar_target_width) // 2
                y_offset = 0
                right_side[y_offset:y_offset+radar_target_height, 
                        x_offset:x_offset+radar_target_width] = radar_image_resized

                # Stats area
                stats_height = TARGET_HEIGHT - radar_target_height
                stats_y = radar_target_height
                
                # Split stats area into two parts
                stats_width = event_target_width
                stats_mid = stats_width // 2
                
                # Fill background
                right_side[stats_y:, :] = [255, 255, 255]
                
                # Calculate FPS
                current_time = time.time()
                if current_time - last_stats_update >= update_interval:
                    max_time = max(people_time, 0, fall_time)
                    if max_time > 0:
                        current_fps = 1000 / max_time
                    else:
                        current_fps = 0
                    
                    current_people_time = people_time
                    current_radar_time = 0
                    current_fall_time = fall_time
                    
                    last_stats_update = current_time

                # Setup font params based on TARGET_HEIGHT
                font = cv2.FONT_HERSHEY_SIMPLEX
                base_font_scale = TARGET_HEIGHT / 720.0  # Scale based on 720p
                text_color = (0, 0, 0)
                line_gap = int(40 * base_font_scale)
                margin = int(20 * base_font_scale)
                base_thickness = max(1, int(2 * base_font_scale))

                # Left side stats
                stats_x = margin
                text_y = stats_y + int(40 * base_font_scale)

                # Status with color
                
                print("count fall: ", fall_display_counter)
                if fall_status == "FALL":
                    fall_display_counter += 1

                if fall_display_counter > 3:
                    fall_text = "Status: Fall Detected!"
                    fall_color = (0, 0, 255)
                    cv2.putText(right_side, fall_text,
                              (margin, text_y), font,
                              0.8 * base_font_scale, fall_color, base_thickness + 1, cv2.LINE_AA)
                    fall_display_counter += 1
                    if fall_display_counter > 20:
                        fall_display_counter = 0
                else:
                    fall_text = "Status: Normal"
                    fall_color = (0, 255, 0)
                    cv2.putText(right_side, fall_text,
                              (margin, text_y), font,
                              0.8 * base_font_scale, fall_color, base_thickness + 1, cv2.LINE_AA)

                # People count
                text_y += line_gap
                cv2.putText(right_side, f"People: {count}",
                           (stats_x, text_y), font,
                           1.2 * base_font_scale, text_color, base_thickness + 1, cv2.LINE_AA)

                # FPS
                if hasattr(args, 'show_fps') and args.show_fps:
                    text_y += line_gap
                    cv2.putText(right_side, f"FPS: {current_fps:.1f}",
                              (stats_x, text_y), font,
                              0.8 * base_font_scale, text_color, base_thickness , cv2.LINE_AA)

                # Right side timing stats
                if hasattr(args, 'show_timing') and args.show_timing:
                    # Draw vertical divider
                    divider_thickness = max(1, int(2 * base_font_scale))
                    cv2.line(right_side,
                            (stats_mid, stats_y),
                            (stats_mid, TARGET_HEIGHT),
                            (200, 200, 200), divider_thickness)

                    timing_x = stats_mid + margin
                    text_y = stats_y + int(40 * base_font_scale)

                    # Title
                    cv2.putText(right_side, "Processing Time (ms):",
                              (timing_x, text_y), font,
                              0.7 * base_font_scale, text_color, base_thickness, cv2.LINE_AA)

                    # Timing details
                    text_y += line_gap
                    detail_scale = 0.7 * base_font_scale
                    cv2.putText(right_side, f"Detection: {current_people_time:.1f}",
                              (timing_x, text_y), font,
                              detail_scale, text_color, base_thickness, cv2.LINE_AA)

                    text_y += line_gap
                    cv2.putText(right_side, f"Radar: {current_radar_time:.1f}",
                              (timing_x, text_y), font,
                              detail_scale, text_color, base_thickness, cv2.LINE_AA)

                    text_y += line_gap
                    cv2.putText(right_side, f"Fall: {current_fall_time:.1f}",
                              (timing_x, text_y), font,
                              detail_scale, text_color, base_thickness, cv2.LINE_AA)

                # Combine images
                combined_image = np.hstack((event_image_resized, right_side))

                # Handle queue
                if result_queue.full():
                    try:
                        result_queue.get_nowait()
                    except Empty:
                        pass

                try:
                    result_queue.put((combined_image, frame, count, fall_status, time.time()))
                except Full:
                    continue

            except Empty:
                continue

    except Exception as e:
        logging.error(f"Error in visualization process: {e}")
    finally:
        running.clear()

def all_one_thread(result_queue, input_filename, camera_resolution, model_path, args, frame_height):
    try:
        bbox_shm = get_bbox_shm()
        boxes_shared = np.ndarray((MAX_BBOXES, 4), dtype=np.float32, buffer=bbox_shm.buf)
        detector = ONNXDetector(model_path, args)
        fall_detector = PeopleFall()
        TARGET_HEIGHT = frame_height
        fall_display_counter = 0
        fall_status = "Normal"
        
        last_valid_angles = []
        no_movement_counter = 0
        last_frame_time = time.time()
        current_bbox = []
        last_peak_time = time.time()
        frame_width = args.camera_width
        frame_height = args.camera_height
        radar_height = int(frame_height * 0.95)
        radar_width = int(frame_width * 1.47)
        
        config = RadarConfig()
        lateral_fov_deg = np.rad2deg(config.lateral_fov)
        pixel_margin = int(config.trim_angle_deg / (lateral_fov_deg / frame_width))
        activity_monitor = ActivityMonitor(config, frame_width)
        radar_viewer = RadarViewer(config, radar_width, radar_height)
        
        if config.use_filter:
            activity_filter = ActivityNoiseFilterGPU(
                width=frame_width,
                height=frame_height,
                threshold=config.delta_t,
                neighborhood_size=args.neighborhood_size,
                time_tolerance=args.time_tolerance
            )
        else:
            activity_filter = None
            
        def update_radar_visualization(ev_rate_gpu: cp.ndarray) -> np.ndarray:
            nonlocal last_frame_time, last_valid_angles, last_peak_time
            radar_img = np.copy(radar_viewer.linear_bins_cpu)
            draw_radar_background(radar_img, radar_width, radar_height, config.lateral_fov)
            frame_delta = time.time() - last_frame_time
            last_frame_time = time.perf_counter()
            if no_movement_counter > 10 and last_valid_angles:
                angles_to_show = [float(a + v) for a, v in zip(last_valid_angles, radar_viewer.angle_velocities)]
            else:
                angles_found = radar_viewer.compute_view(ev_rate_gpu, radar_img, frame_delta)
                angles_to_show = radar_viewer.stabilize_angles(angles_found)
                if angles_to_show:
                    last_valid_angles = angles_to_show.copy()
                    last_peak_time = time.time()  
            if (not angles_to_show or len(angles_to_show) == 0) and (time.time() - last_peak_time > 1):
                radar_viewer.tracked_peaks.clear()
                last_valid_angles = []
                draw_radar_background(radar_img, radar_width, radar_height, config.lateral_fov)
            
            return radar_img
        
        w, h = camera_resolution
        processor = EventProcessor(
            width=w,
            height=h,
            crop_coordinates=None
        )
        
        iterator = BiasEventsIterator(
            delta_t=33000,
            input_filename=input_filename,
            bias_file=os.path.join(config_path, 'bias.bias')
        )
        
        frame_count = 0
        total_time = 0
        for events in iterator:
            start_time = time.time()
            
            event_frame = processor.create_frame(events)
            frame = event_frame.data
            
            boxes, boxes_n, current_count = detector.detect_and_track(frame)
            
            tracked_frame = frame.copy()
            if current_count > 0:
                for box in boxes:
                    x1, y1, x2, y2 = box.astype(int)
                    cv2.rectangle(tracked_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(tracked_frame, "Person", (x1, y1 - 10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                    
            events_filtered = convert_events_data(events.copy())
            
                
            if activity_filter:
                processed_events = activity_filter.process_events_gpu(events_filtered)
                if processed_events is None or processed_events["x"].size == 0:
                    x_gpu = cp.array([], dtype=cp.int32)
                    y_gpu = cp.array([], dtype=cp.int32)
                else:
                    x_gpu = processed_events['x']
                    y_gpu = processed_events['y']
            else:
                x_gpu = cp.asarray(events_filtered['x'], dtype=cp.int32)
                y_gpu = cp.asarray(events_filtered['y'], dtype=cp.int32)
                
            if pixel_margin > 0 and x_gpu.size > 0:
                mask = (x_gpu >= pixel_margin) & (x_gpu < (frame_width - pixel_margin))
                x_filtered_gpu = x_gpu[mask]
                y_filtered_gpu = y_gpu[mask]
            else:
                x_filtered_gpu = x_gpu
                y_filtered_gpu = y_gpu
                
            current_time = time.time()
            valid_boxes = [box for box in boxes_shared if not np.allclose(box, [SENTINEL]*4)]
            count = len(valid_boxes)
            if valid_boxes:
                last_bbox = np.array(valid_boxes)
                last_bbox_time = current_time
                radar_viewer.update_max_objects(max(2,count))
            else:
                if last_bbox is not None and (current_time - last_bbox_time) < 1:
                    pass
                else:
                    boxes_shared[:] = SENTINEL
                    last_bbox = None
                    x_filtered_gpu = cp.asarray([], dtype=cp.int32)
                    y_filtered_gpu = cp.asarray([], dtype=cp.int32)

                    
            activity_monitor.process_events(x_filtered_gpu)
            activity_monitor.reset()
            ev_rate_gpu = activity_monitor.get_ev_rate_per_bin(return_to_cpu=False)
            total_events = activity_monitor.get_total_event_count()
            
            if total_events < config.low_event_threshold:
                no_movement_counter += 1
                if no_movement_counter > 5:
                    last_valid_angles = []
                    radar_viewer.angle_velocities = {}
            else:
                no_movement_counter = 0
                
            radar_img = update_radar_visualization(ev_rate_gpu)
            
            frame_fall = frame.copy()
            h, w, _ = frame_fall.shape
            if h != 320 or w != 320:
                frame_fall = cv2.resize(frame_fall, (320, 320))
            
            fall_status = fall_detector.process_frame(frame_fall)
            
            event_target_height = TARGET_HEIGHT
            event_target_width = TARGET_HEIGHT
            
            event_image_resized = cv2.resize(tracked_frame, 
                                        (event_target_width, event_target_height),
                                        interpolation=cv2.INTER_AREA)
            
            radar_target_height = int(TARGET_HEIGHT * 2/3)
            radar_aspect_ratio = radar_img.shape[1] / radar_img.shape[0]
            radar_target_width = int(radar_target_height * radar_aspect_ratio)
            
            if radar_target_width > event_target_width:
                radar_target_width = event_target_width
                radar_target_height = int(radar_target_width / radar_aspect_ratio)
            
            radar_image_resized = cv2.resize(radar_img,
                (radar_target_width, radar_target_height),
                interpolation=cv2.INTER_AREA)
            
            right_side = np.zeros((TARGET_HEIGHT, event_target_width, 3), dtype=np.uint8)
            
            x_offset = (event_target_width - radar_target_width) // 2
            y_offset = 0
            right_side[y_offset:y_offset+radar_target_height, 
                    x_offset:x_offset+radar_target_width] = radar_image_resized
            
            stats_height = TARGET_HEIGHT - radar_target_height
            stats_y = radar_target_height
            stats_width = event_target_width
            stats_mid = stats_width // 2
            
            right_side[stats_y:, :] = [255, 255, 255]
            
            font = cv2.FONT_HERSHEY_SIMPLEX
            base_font_scale = TARGET_HEIGHT / 720.0
            text_color = (0, 0, 0)
            line_gap = int(40 * base_font_scale)
            margin = int(20 * base_font_scale)
            base_thickness = max(1, int(2 * base_font_scale))
            
            stats_x = margin
            text_y = stats_y + int(40 * base_font_scale)
            
            print("count fall: ", fall_display_counter)
            
            if fall_status == "FALL":
                fall_display_counter += 1
                
            if fall_display_counter > 3:
                fall_text = "Status: Fall Detected!"
                fall_color = (0, 0, 255)
                cv2.putText(right_side, fall_text,
                            (margin, text_y), font,
                            0.8 * base_font_scale, fall_color, base_thickness + 1, cv2.LINE_AA)
                fall_display_counter += 1
                if fall_display_counter > 20:
                    fall_display_counter = 0
            else:
                fall_text = "Status: Normal"
                fall_color = (0, 255, 0)
                cv2.putText(right_side, fall_text,
                            (margin, text_y), font,
                            0.8 * base_font_scale, fall_color, base_thickness + 1, cv2.LINE_AA)
                            
            text_y += line_gap
            cv2.putText(right_side, f"People: {current_count}",
                        (stats_x, text_y), font,
                        1.2 * base_font_scale, text_color, base_thickness + 1, cv2.LINE_AA)
            
            combined_image = np.hstack((event_image_resized, right_side))
            
            end_time = time.time()
            processing_time = round((end_time - start_time) * 1000, 2)
            
            frame_count += 1
            total_time += processing_time
            avg_time = total_time / frame_count
            
            if hasattr(args, 'show_fps') and args.show_fps:
                fps = 1000 / avg_time if avg_time > 0 else 0
                text_y += line_gap
                cv2.putText(combined_image, f"FPS: {fps:.1f}",
                          (stats_x, text_y), font,
                          0.8 * base_font_scale, text_color, base_thickness, cv2.LINE_AA)
                
                if frame_count % 100 == 0:
                    print(f"Frame: {frame_count}, Avg Processing Time: {avg_time:.1f}ms, FPS: {fps:.1f}")
            
            while True:
                try:
                    result_queue.put((combined_image, frame, count, fall_status, time.time()), block=True, timeout=0.1)
                    break
                except Full:
                    time.sleep(0.01) 
        
        print(f"File processing completed. Processed {frame_count} frames")
        print(f"Average processing time: {total_time/frame_count:.2f}ms")
        
        result_queue.put(("EOF", None, 0, "Normal", time.time()))
        
    except Exception as e:
        print(traceback.print_exc())
        logging.error(f"Error in event process: {e}")
    finally:
        if 'iterator' in locals() and iterator:
            del iterator
               
class StreamProcessor:
    def __init__(self, frame_queue_size=5, resize_dims=(320, 320), target_fps=30, input_filename="", args: ProcessingParams = None):
        # Queues for inter-process communication
        self.frame_queue = Queue(maxsize=frame_queue_size)
        self.frame_queue_fall = Queue(maxsize=64)
        self.event_queue = Queue(maxsize=frame_queue_size)
        self.people_queue = Queue(maxsize=frame_queue_size)
        self.fall_queue = Queue(maxsize=frame_queue_size)
        self.result_queue = Queue(maxsize=frame_queue_size)
        self.radar_queue = Queue(maxsize=frame_queue_size)
        self.result_queue_single = Queue(maxsize=256)
        self.running = Event()
        self.config = RadarConfig()
        self.args = args
        
        self.frame_height = args.output_height
        self.camera_resolution = (args.camera_width, args.camera_height)
        
        self.resize_dims = resize_dims
        self.target_fps = target_fps
        self.input_filename = input_filename
        self.model_path = os.path.join(yolo_path, "best_s_22_1.onnx")
        
        # Stats
        self.fps_buffer = deque(maxlen=30)
        self.people_buffer = deque(maxlen=5)
        self.prev_time = time.time()
        
    def update_people_count(self, count):
        self.people_buffer.append(count)
        counts = list(self.people_buffer)
        return max(set(counts), key=counts.count) if counts else 0

    def draw_stats(self, frame, fps, people_count, fall_status):
        cv2.rectangle(frame, (10, 2), (200, 85), (0, 0, 0), -1)
        cv2.putText(frame, f'FPS: {fps:.1f}', (15, 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        cv2.putText(frame, f'People: {people_count}', (15, 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        color = (0, 0, 255) if fall_status == 'FALL' else (0, 255, 0)
        cv2.putText(frame, f'Status: {fall_status}', (15, 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        return frame
    
    def start_ui_one_thread(self):
        self.all_process = Process(
            name="AllProcess",
            target=all_one_thread, 
            args=(self.result_queue_single, self.input_filename, self.camera_resolution, self.model_path, self.args, self.frame_height),
            daemon=True
        )
        self.all_process.start()
        return self.result_queue_single
        
    def start_ui(self):
        """Start processing for UI display without OpenCV window"""
        self.running.set()
        
        # Start all processes
        self.event_process = Process(
            name="EventProcess",
            target=event_process, 
            args=(self.frame_queue, self.frame_queue_fall, self.event_queue, self.running, self.input_filename, self.camera_resolution),
            daemon=True
        )
        
        self.people_process = Process(
            name="PeopleProcess",
            target=people_detection_process_onnx, 
            args=(self.frame_queue, self.people_queue, self.running, self.model_path, self.args),
            daemon=True
        )
        
        self.fall_process = Process(
            name="FallProcess",
            target=fall_detection_process,
            args=(self.frame_queue, self.fall_queue, self.running),
            daemon=True
        )
        
        self.person_direction = Process(
            name="PersonDirection",
            target=person_direction_process,
            args=(self.event_queue , self.radar_queue, self.running, self.args),
            daemon=True
        )
        
        self.viz_process = Process(
            name="VizProcess",
            target=visualization_process,
            args=(self.people_queue, self.fall_queue, self.radar_queue, self.result_queue, self.running, self.frame_height, self.args),
            daemon=True
        )
        
        self.event_process.start()
        self.person_direction.start()
        self.people_process.start()
        self.fall_process.start()
        self.viz_process.start()

        # Return result_queue to ProcessingThread can be get frame
        return self.result_queue

    def start(self):
        self.running.set()
        
        # Start all processes
        self.event_process = Process(
            name="EventProcess",
            target=event_process, 
            args=(self.frame_queue, self.event_queue, self.running, self.input_filename, self.camera_resolution),
            daemon=True
        )
        
        self.people_process = Process(
            name="PeopleProcess",
            target=people_detection_process, 
            args=(self.frame_queue, self.people_queue, self.running, self.model_path, self.args),
            daemon=True
        )
        
        self.fall_process = Process(
            name="FallProcess",
            target=fall_detection_process,
            args=(self.frame_queue, self.fall_queue, self.running),
            daemon=True
        )
        
        self.person_direction = Process(
            name="PersonDirection",
            target=person_direction_process,
            args=(self.event_queue, self.radar_queue, self.running, self.args),
            daemon=True
        )
        
        self.viz_process = Process(
            name="VizProcess",
            target=visualization_process,
            args=(self.people_queue, self.fall_queue, self.radar_queue, self.result_queue, self.running, self.frame_height, self.args),
            daemon=True
        )
        
        self.event_process.start()
        self.person_direction.start()
        self.people_process.start()
        self.fall_process.start()
        self.viz_process.start()
        
        last_result = None
        
        try:
            while self.running.is_set():
                try:
                    result = self.result_queue.get(timeout=0.1)
                    last_result = result
                except Empty:
                    if last_result is None:
                        continue
                    result = last_result

                tracked_frame, _, current_count, fall_status, _ = result

                stable_count = self.update_people_count(current_count)
                
                # Draw stats and display
                # tracked_frame = self.draw_stats(tracked_frame, fps, stable_count, fall_status)
                cv2.imshow("WQ EVS TIVI", tracked_frame)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        except KeyboardInterrupt:
            pass
        finally:
            self.stop()

    def stop(self):
        print("Stopping all processes...")
        self.running.clear()
        
        with multiprocessing.Pool() as pool:
            pool.terminate()
            pool.join()
        
        for queue in [self.frame_queue, self.people_queue, self.fall_queue, 
                    self.result_queue, self.radar_queue]:
            while not queue.empty():
                try:
                    queue.get_nowait()
                except Empty:
                    break
        
        cv2.destroyAllWindows()
        for _ in range(5):
            cv2.waitKey(1)

def signal_handler(signum, frame):
    print(f"\nReceived signal {signum}")
    try:
        current_process = multiprocessing.current_process()
        for child in current_process.children():
            child.terminate()
        
        for child in current_process.children():
            child.join(timeout=3)
    except Exception as e:
        print(f"Error in signal handler: {e}")
    
    sys.exit(0)
    
def main():
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    processor = None
    try:
        params = ProcessingParams()
        processor = StreamProcessor(args=params)
        processor.start()
    except Exception as e:
        traceback.print_exc()
        print(f"Error occurred: {e}")
    finally:
        if processor:
            processor.stop()

def cleanup_old_processes():
    import psutil
    current_process = psutil.Process()
    process_name = current_process.name()
    
    for proc in psutil.process_iter(['pid', 'name']):
        try:
            if proc.info['name'] == process_name and proc.pid != current_process.pid:
                psutil.Process(proc.pid).terminate()
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass

if __name__ == "__main__":
    if sys.platform.startswith('win'):
        multiprocessing.freeze_support()
        
    restart_usb_devices()
    cleanup_old_processes()
    main()

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\services\main_service.py
--------------------------------------------------------------------------------
import multiprocessing
import cv2
import time
from collections import deque
from queue import Empty
from multiprocessing import Process, Queue, Event
from src.infrastructure.utils.module_process import (all_one_thread, event_process, 
                                                     fall_detection_process, people_detection_process_onnx, 
                                                     person_direction_process, visualization_process)
from src.domain.value_objects.config import RadarConfig
from src.domain.entities.entities import ProcessingParams
import os
       
class LogicFactoryService:
    def __init__(self, base_path="", frame_queue_size=5, resize_dims=(320, 320), target_fps=30, input_filename="", args: ProcessingParams = None):
        # Queues for inter-process communications
        self.base_path = base_path
        self.yolo_path = os.path.join(base_path, "public", "yolo_models", "best_s_22_1.onnx")
        self.tflite_path = os.path.join(base_path, "public", "tflite_models", "FE.V03.D05.tflite")
        self.fall_model_path = os.path.join(base_path, "public", "onnx_models", "WQ_TiVi.onnx")
        self.config_path = os.path.join(base_path, "public", "bias", 'bias.bias')
        
        self.frame_queue = Queue(maxsize=frame_queue_size)
        self.frame_queue_fall = Queue(maxsize=64)
        self.event_queue = Queue(maxsize=frame_queue_size)
        self.people_queue = Queue(maxsize=frame_queue_size)
        self.fall_queue = Queue(maxsize=frame_queue_size)
        self.result_queue = Queue(maxsize=frame_queue_size)
        self.radar_queue = Queue(maxsize=frame_queue_size)
        self.result_queue_single = Queue(maxsize=256)
        self.running = Event()
        self.config = RadarConfig()
        self.args = args
        
        self.frame_height = args.output_height
        self.camera_resolution = (args.camera_width, args.camera_height)
        
        self.resize_dims = resize_dims
        self.target_fps = target_fps
        self.input_filename = input_filename
        
        # Stats
        self.fps_buffer = deque(maxlen=30)
        self.people_buffer = deque(maxlen=5)
        self.prev_time = time.time()
        
    def update_people_count(self, count):
        self.people_buffer.append(count)
        counts = list(self.people_buffer)
        return max(set(counts), key=counts.count) if counts else 0

    def draw_stats(self, frame, fps, people_count, fall_status):
        cv2.rectangle(frame, (10, 2), (200, 85), (0, 0, 0), -1)
        cv2.putText(frame, f'FPS: {fps:.1f}', (15, 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        cv2.putText(frame, f'People: {people_count}', (15, 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        color = (0, 0, 255) if fall_status == 'FALL' else (0, 255, 0)
        cv2.putText(frame, f'Status: {fall_status}', (15, 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        return frame
    
    def start_ui_one_thread(self):
        self.all_process = Process(
            name="AllProcess",
            target=all_one_thread, 
            args=(self.result_queue_single, self.input_filename, self.camera_resolution, self.yolo_path, self.args, self.frame_height, self.config_path, self.fall_model_path),
            daemon=True
        )
        self.all_process.start()
        return self.result_queue_single
        
    def start_ui(self):
        """Start processing for UI display without OpenCV window"""
        self.running.set()
        
        # Start all processes
        self.event_process = Process(
            name="EventProcess",
            target=event_process, 
            args=(self.frame_queue, self.frame_queue_fall, self.event_queue, self.running, self.input_filename, self.camera_resolution, self.config_path),
            daemon=True
        )
        
        self.people_process = Process(
            name="PeopleProcess",
            target=people_detection_process_onnx, 
            args=(self.frame_queue, self.people_queue, self.running, self.yolo_path, self.args),
            daemon=True
        )
        
        self.fall_process = Process(
            name="FallProcess",
            target=fall_detection_process,
            args=(self.frame_queue, self.fall_queue, self.running, self.fall_model_path),
            daemon=True
        )
        
        self.person_direction = Process(
            name="PersonDirection",
            target=person_direction_process,
            args=(self.event_queue , self.radar_queue, self.running, self.args),
            daemon=True
        )
        
        self.viz_process = Process(
            name="VizProcess",
            target=visualization_process,
            args=(self.people_queue, self.fall_queue, self.radar_queue, self.result_queue, self.running, self.frame_height, self.args),
            daemon=True
        )
        
        self.event_process.start()
        self.person_direction.start()
        self.people_process.start()
        self.fall_process.start()
        self.viz_process.start()

        # Return result_queue to ProcessingThread can be get frame
        return self.result_queue


    def stop(self):
        print("Stopping all processes...")
        self.running.clear()
        
        with multiprocessing.Pool() as pool:
            pool.terminate()
            pool.join()
        
        for queue in [self.frame_queue, self.people_queue, self.fall_queue, 
                    self.result_queue, self.radar_queue]:
            while not queue.empty():
                try:
                    queue.get_nowait()
                except Empty:
                    break
        
        cv2.destroyAllWindows()
        for _ in range(5):
            cv2.waitKey(1)



--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\services\video_detection_service.py
--------------------------------------------------------------------------------
from typing import List, Optional, Union
import logging
import time
from queue import Queue, Full, Empty
from threading import Event

from src.application.interfaces.counter import ICounter
from src.application.interfaces.detector import IDetector
from src.application.interfaces.tracker import ITracker
from src.application.interfaces.event_iterator import IEventIterator
from src.domain.entities.detection import Frame
from src.domain.entities.detection import Detection
from src.domain.value_objects.counting_zone import CountingZone

class VideoDetectionService:
    def __init__(self,
                 frame_iterator: IEventIterator,
                 detector: IDetector,
                 tracker: ITracker,
                 counter: Optional[ICounter] = None,
                 max_buffer_size: int = 1):
        
        self.frame_iterator = frame_iterator
        self.detector = detector
        self.tracker = tracker
        self.counter = counter
        self.frame_queue = Queue(maxsize=max_buffer_size)
        self.running = Event()
        self._frame_count = 0
        self._fps_start_time = time.time()
        self._current_fps = 0
        
    def process_frames_thread(self) -> None:
        """Thread function to read frames from video source"""
        self.running.set()
        try:
            for frame_data in self.frame_iterator:
                if not self.running.is_set():
                    break

                try:
                    frame = Frame(
                        data=frame_data,
                        height=frame_data.shape[0],
                        width=frame_data.shape[1]
                    )
                    self.frame_queue.put(frame, timeout=0.1)
                except Full:
                    logging.warning("Frame queue is full, skipping frame")
                    continue

        except Exception as e:
            logging.error(f"Error in process_frames_thread: {e}")
        finally:
            self.running.clear()

    def process_detections(self, counting_zones: Optional[List[CountingZone]] = None):
        """Main processing loop for detections"""
        self.running.set()
        try:
            while self.running.is_set():
                try:
                    frame = self.frame_queue.get(timeout=0.1)
                except Empty:
                    continue

                self._update_fps()
                frame.fps = self._current_fps

                # Detect objects
                detections = self.detector.detect(frame)
                
                # Update tracking and counting based on counter type
                if self.counter:
                    if hasattr(self.counter, 'update_tracks'):  # Line counter
                        tracked_detections = self.tracker.update(detections)
                        frame.detections = tracked_detections
                        if counting_zones:
                            counting_zones = self.counter.update(tracked_detections, counting_zones)
                        yield frame, counting_zones
                    else:  # Frame counter
                        frame.detections = detections
                        counting_result = self.counter.update(detections)
                        yield frame, counting_result
                else:
                    frame.detections = detections
                    yield frame, None

                self.frame_queue.task_done()

        except Exception as e:
            logging.error(f"Error in process_detections: {e}")
        finally:
            self.running.clear()

    def _update_fps(self) -> None:
        """Update FPS calculation"""
        self._frame_count += 1
        if time.time() - self._fps_start_time >= 1.0:
            self._current_fps = self._frame_count
            self._frame_count = 0
            self._fps_start_time = time.time()
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\use_cases\CountPeopleUseCase.py
--------------------------------------------------------------------------------
# code coming soon
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\application\use_cases\DetectAndTrackUseCase.py
--------------------------------------------------------------------------------
# code coming soon
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\core\filters.py
--------------------------------------------------------------------------------
# filters.py
import cupy as cp
import numpy as np
from typing import List

class BoundingBox:
    def __init__(self, x_center: float, y_center: float, width: float, height: float, 
                 frame_width: int, frame_height: int, expansion_ratio: float = -0.2):
        self.x_center = x_center * frame_width
        self.y_center = y_center * frame_height
        self.width = width * frame_width
        self.height = height * frame_height
        self.x_min = int(self.x_center - self.width / 2)
        self.x_max = int(self.x_center + self.width / 2)
        self.y_min = int(self.y_center - self.height / 2)
        self.y_max = int(self.y_center + self.height / 2)
        self.expand(expansion_ratio, frame_width, frame_height)
    
    def expand(self, ratio: float, frame_width: int, frame_height: int):
        expand_width = self.width * abs(ratio)
        expand_height = self.height * abs(ratio)
        if ratio < 0:
            self.x_min = int(min(max(self.x_min + expand_width / 2, 0), frame_width - 1))
            self.x_max = int(max(min(self.x_max - expand_width / 2, frame_width - 1), 0))
            self.y_min = int(min(max(self.y_min + expand_height / 2, 0), frame_height - 1))
            self.y_max = int(max(min(self.y_max - expand_height / 2, frame_height - 1), 0))
        else:
            self.x_min = int(max(self.x_min - expand_width / 2, 0))
            self.x_max = int(min(self.x_max + expand_width / 2, frame_width - 1))
            self.y_min = int(max(self.y_min - expand_height / 2, 0))
            self.y_max = int(min(self.y_max + expand_height / 2, frame_height - 1))
    
    def contains(self, x: int, y: int) -> bool:
        return (self.x_min <= x <= self.x_max) and (self.y_min <= y <= self.y_max)
# class BoundingBox:
#     def __init__(self, x_center: float, y_center: float, width: float, height: float, 
#                  frame_width: int, frame_height: int, expansion_ratio: float = -0.2,
#                  expansion_ratio_x: float = None, expansion_ratio_y: float = None):
#         self.x_center = x_center * frame_width
#         self.y_center = y_center * frame_height
#         self.width = width * frame_width
#         self.height = height * frame_height
#         self.x_min = int(self.x_center - self.width / 2)
#         self.x_max = int(self.x_center + self.width / 2)
#         self.y_min = int(self.y_center - self.height / 2)
#         self.y_max = int(self.y_center + self.height / 2)
#         # Nếu không có giá trị riêng cho x hay y thì dùng expansion_ratio chung
#         if expansion_ratio_x is None:
#             expansion_ratio_x = expansion_ratio
#         if expansion_ratio_y is None:
#             expansion_ratio_y = expansion_ratio
#         self.expand(expansion_ratio_x, expansion_ratio_y, frame_width, frame_height)
    
#     def expand(self, expansion_ratio_x: float, expansion_ratio_y: float, frame_width: int, frame_height: int):
#         # Xử lý mở rộng theo chiều x
#         expand_width = self.width * abs(expansion_ratio_x)
#         if expansion_ratio_x < 0:
#             self.x_min = int(min(max(self.x_min + expand_width / 2, 0), frame_width - 1))
#             self.x_max = int(max(min(self.x_max - expand_width / 2, frame_width - 1), 0))
#         else:
#             self.x_min = int(max(self.x_min - expand_width / 2, 0))
#             self.x_max = int(min(self.x_max + expand_width / 2, frame_width - 1))
        
#         # Xử lý mở rộng theo chiều y
#         expand_height = self.height * abs(expansion_ratio_y)
#         if expansion_ratio_y < 0:
#             self.y_min = int(min(max(self.y_min + expand_height / 2, 0), frame_height - 1))
#             self.y_max = int(max(min(self.y_max - expand_height / 2, frame_height - 1), 0))
#         else:
#             self.y_min = int(max(self.y_min - expand_height / 2, 0))
#             self.y_max = int(min(self.y_max + expand_height / 2, frame_height - 1))
    
#     def contains(self, x: int, y: int) -> bool:
#         return (self.x_min <= x <= self.x_max) and (self.y_min <= y <= self.y_max)

def filter_events(x: np.ndarray, y: np.ndarray, bounding_boxes: List[BoundingBox], max_threads: int = 3):
    if x.size == 0 or len(bounding_boxes) == 0:
        return np.array([], dtype=x.dtype), np.array([], dtype=y.dtype)
    
    def filter_single_box(bb: BoundingBox):
        mask = ((x >= bb.x_min) & (x <= bb.x_max) &
                (y >= bb.y_min) & (y <= bb.y_max))
        return mask
    
    combined_mask = filter_single_box(bounding_boxes[0])
    for bb in bounding_boxes[1:max_threads]:
        mask = filter_single_box(bb)
        combined_mask = np.logical_or(combined_mask, mask)
    
    x_filtered = x[combined_mask]
    y_filtered = y[combined_mask]
    return x_filtered, y_filtered

class ActivityNoiseFilterGPU:
    def __init__(self, width: int, height: int, threshold: int, neighborhood_size: int = 5, time_tolerance: int = 1000):
        self.width = width
        self.height = height
        self.threshold = threshold
        self.neighborhood_size = neighborhood_size
        self.time_tolerance = time_tolerance
        padding = neighborhood_size // 2
        self.w_plus_pad = width + 2 * padding
        self.h_plus_pad = height + 2 * padding
        self.last_ts_gpu = cp.full((self.w_plus_pad * self.h_plus_pad,), 0, dtype=cp.uint32)
        self.activity_filter_kernel = cp.RawKernel(r'''
        extern "C" __global__
        void activityFilterKernel(
            const unsigned int* t_arr,
            const int* x_arr,
            const int* y_arr,
            unsigned int* last_ts, 
            bool* out_valid,
            const int num_events,
            const int w_plus_pad,
            const unsigned int threshold,
            const int neighborhood_size,
            const unsigned int time_tolerance
        ) {
            int i = blockDim.x * blockIdx.x + threadIdx.x;
            if (i >= num_events) return;
            unsigned int t_i = t_arr[i];
            int x_i = x_arr[i];
            int y_i = y_arr[i];
            int center_idx = (y_i + neighborhood_size/2) * w_plus_pad + (x_i + neighborhood_size/2);
            unsigned int time_min = (t_i > threshold + time_tolerance) ? (t_i - threshold - time_tolerance) : 0;
            unsigned int time_max = t_i + time_tolerance;
            bool is_valid = false;
            int radius = neighborhood_size / 2;
            for (int dy = -radius; dy <= radius && !is_valid; dy++) {
                for (int dx = -radius; dx <= radius; dx++) {
                    if (dx == 0 && dy == 0) continue;
                    unsigned int nb_ts = last_ts[center_idx + dy * w_plus_pad + dx];
                    if (nb_ts >= time_min && nb_ts <= time_max) {
                        is_valid = true;
                        break;
                    }
                }
            }
            last_ts[center_idx] = t_i;
            out_valid[i] = is_valid;
        }
        ''', 'activityFilterKernel')

    def process_events_gpu(self, events: np.ndarray):
        if events.size == 0:
            return None 

        event_fields = events.dtype.names

        t_gpu = cp.asarray(events['t'], dtype=cp.uint32)
        x_gpu = cp.asarray(events['x'], dtype=cp.int32)
        y_gpu = cp.asarray(events['y'], dtype=cp.int32)
        
        num_events = x_gpu.size
        out_valid = cp.zeros(num_events, dtype=cp.bool_)
        block_size = 256
        grid_size = (num_events + block_size - 1) // block_size
        
        self.activity_filter_kernel(
            (grid_size,), (block_size,),
            (t_gpu, x_gpu, y_gpu,
            self.last_ts_gpu,
            out_valid,
            num_events,
            self.w_plus_pad,
            self.threshold,
            self.neighborhood_size,
            self.time_tolerance)
        )
        
        valid_idx = cp.where(out_valid)[0]
        
        if 'p' in event_fields:
            p_gpu = cp.asarray(events['p'], dtype=cp.int16)  # Sử dụng int16 để giữ giá trị âm nếu có
            valid_p_mask = p_gpu[valid_idx] >= 0
            valid_idx = valid_idx[valid_p_mask]
        
        num_valid = valid_idx.size
        if num_valid == 0:
            return None 
        
        t_filtered_gpu = t_gpu[valid_idx]
        x_filtered_gpu = x_gpu[valid_idx]
        y_filtered_gpu = y_gpu[valid_idx]
        
        result = {'t': t_filtered_gpu, 'x': x_filtered_gpu, 'y': y_filtered_gpu}
        if 'p' in event_fields:
            result['p'] = p_gpu[valid_idx]
        return result


--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\core\radar.py
--------------------------------------------------------------------------------
# radar.py
import cupy as cp
import numpy as np
import cv2
from cupyx.scipy import signal
from scipy.optimize import linear_sum_assignment
import time
from src.domain.value_objects.config import RadarConfig
from src.infrastructure.utils.utils import interpolate_bin

class ActivityMonitor:
    def __init__(self, config: RadarConfig, sensor_width: int):
        self.conf = config
        self.n_bins = config.n_bins
        self.bin_width = sensor_width / self.n_bins
        self.histogram = cp.zeros(self.n_bins, dtype=cp.float32)
        self.prev_histogram = None

    def process_events(self, x_gpu: cp.ndarray):
        if x_gpu.size == 0:
            return
        x_bins = cp.floor(x_gpu / self.bin_width).astype(cp.int32)
        x_bins = cp.clip(x_bins, 0, self.n_bins - 1)
        counts = cp.bincount(x_bins, minlength=self.n_bins).astype(cp.float32)
        self.histogram += counts

    def reset(self):
        if self.histogram is not None:
            self.prev_histogram = cp.copy(self.histogram)
        self.histogram.fill(0)

    def get_ev_rate_per_bin(self, return_to_cpu=True):
        if self.prev_histogram is None:
            zeros = cp.zeros(self.n_bins, dtype=cp.float32)
            return zeros.get() if return_to_cpu else zeros
        ev_rate_per_bin_gpu = self.prev_histogram / (self.conf.delta_t * 1e-6)
        return ev_rate_per_bin_gpu.get() if return_to_cpu else ev_rate_per_bin_gpu

    def get_total_event_count(self) -> float:
        if self.prev_histogram is None:
            return 0.0
        total_gpu = cp.sum(self.prev_histogram)
        return float(total_gpu.get())

class RadarViewer:
    def __init__(self, config, width, height, max_missed_frames=0, max_objects = 3):
        self.conf = config
        self.max_objects = max_objects
        self.width = width
        self.height = height
        self.n_bins = config.n_bins
        self.n_bins_x = config.n_bins_x
        self.n_bins_y = config.n_bins_y
        self.lateral_fov = config.lateral_fov
        self.min_ev_rate = config.min_ev_rate
        self.max_ev_rate = config.max_ev_rate
        self.max_range = config.max_range
        self.min_peak_prominence = config.min_peak_prominence
        self.last_update_time = 0
        self.update_interval = 0.01
        self.target_peaks = []
        self.peak_move_threshold = 30.0
        self.max_missed_frames = max_missed_frames
        self.tracked_peaks = {}
        self.radar_center_x = self.width // 2
        self.radar_center_y = self.height
        self.scale_factor = self.height / self.conf.max_range
        self.mapx, self.mapy = self.compute_radar_maps_gpu()
        self.linear_bins_cpu = np.zeros((height, width, 3), dtype=np.uint8)
        self.reset_grid()
        self.stabilized_angles = cp.array([], dtype=cp.float32)
        self.radar_bg = np.zeros((height, width, 3), dtype=np.uint8)
        self.last_angles = []
        self.angle_velocities = {}
        self.max_dist_change = 20
        self.next_peak_id = 0
        self.k_spring = 3
        self.c_damp = 1
        self.peak_velocities = {}
        self.max_angle_velocity = np.deg2rad(20.0)
        self.max_dist_velocity = 20.0
        self.angle_cost_weight = 2
        self.dist_cost_weight = 1.5
        self.predefined_max_threshold = 150.0
    def update_max_objects(self, new_max_objects: int):
        self.max_objects = new_max_objects
        
    def wrap_angle(self, angle_rad):
        half_fov = self.lateral_fov / 2
        angle_rad = (angle_rad + np.pi) % (2 * np.pi) - np.pi
        return np.clip(angle_rad, -half_fov, half_fov)

    def constrain_distance(self, distance):
        return np.clip(distance, 0, self.max_range)

    def match_peaks_hungarian(self, new_peaks, current_peaks):
        if not new_peaks or not current_peaks:
            return [], list(range(len(new_peaks))), list(range(len(current_peaks)))
        
        try:
            new_angles = cp.array([peak[0] for peak in new_peaks], dtype=cp.float32)
            new_dists = cp.array([peak[1] for peak in new_peaks], dtype=cp.float32)
            curr_angles = cp.array([peak[0] for peak in current_peaks], dtype=cp.float32)
            curr_dists = cp.array([peak[1] for peak in current_peaks], dtype=cp.float32)
            
            angle_diff_matrix = cp.abs(new_angles[:, None] - curr_angles[None, :])
            
            half_fov = self.lateral_fov / 2
            angle_diff_matrix = (angle_diff_matrix + cp.pi) % (2 * cp.pi) - cp.pi
            angle_diff_matrix = cp.clip(angle_diff_matrix, -half_fov, half_fov)
            
            angle_diff_deg = cp.rad2deg(angle_diff_matrix)
            dist_diff_matrix = cp.abs(new_dists[:, None] - curr_dists[None, :])
            
            angle_cost = self.angle_cost_weight * angle_diff_deg
            dist_cost = self.dist_cost_weight * dist_diff_matrix
            
            over_angle = angle_diff_deg > self.peak_move_threshold
            if cp.any(over_angle):
                angle_cost[over_angle] += self.angle_cost_weight * (
                    angle_diff_deg[over_angle] - self.peak_move_threshold
                )
            
            over_dist = dist_diff_matrix > self.max_dist_change
            if cp.any(over_dist):
                dist_cost[over_dist] += self.dist_cost_weight * (
                    dist_diff_matrix[over_dist] - self.max_dist_change
                )
            
            total_cost = angle_cost + dist_cost
            
            finite_mask = total_cost < 1e6
            if cp.any(finite_mask):
                finite_costs = total_cost[finite_mask]
                mean_cost = float(cp.mean(finite_costs).get())
                std_cost = float(cp.std(finite_costs).get())
                dynamic_threshold = min(mean_cost + 2 * std_cost, self.predefined_max_threshold)
                self.conf.mot_gating_threshold = dynamic_threshold
            else:
                self.conf.mot_gating_threshold = self.predefined_max_threshold
            
            cost_matrix = cp.full((len(new_peaks), len(current_peaks)), 1e6, dtype=cp.float32)
            mask = total_cost < self.conf.mot_gating_threshold
            cost_matrix[mask] = total_cost[mask]
            
            cost_matrix_cpu = cost_matrix.get()
            
            try:
                row_ind, col_ind = linear_sum_assignment(cost_matrix_cpu)
            except ValueError as e:
                return [], list(range(len(new_peaks))), list(range(len(current_peaks)))
            
            matched_pairs = []
            unmatched_new = set(range(len(new_peaks)))
            unmatched_current = set(range(len(current_peaks)))
            
            for r, c in zip(row_ind, col_ind):
                if r < len(new_peaks) and c < len(current_peaks) and cost_matrix_cpu[r, c] < 1e6:
                    matched_pairs.append((r, c))
                    unmatched_new.discard(r)
                    unmatched_current.discard(c)
            
            result_peaks = []
            for new_idx, curr_idx in matched_pairs:
                new_angle, new_dist = new_peaks[new_idx]
                curr_id = current_peaks[curr_idx][2]
                result_peaks.append((new_angle, new_dist, curr_id))
            
            return result_peaks, list(unmatched_new), list(unmatched_current)
            
        except Exception as e:
            return [], list(range(len(new_peaks))), list(range(len(current_peaks)))


    def compute_radar_maps_gpu(self):
        mapx = cp.full((self.height, self.width), -1, dtype=cp.int32)
        mapy = cp.full((self.height, self.width), -1, dtype=cp.int32)
        j_indices, i_indices = cp.meshgrid(cp.arange(self.width), cp.arange(self.height))
        x_c = self.width / 2.0
        y_c = self.height
        theta = (j_indices / self.width - 0.5) * self.lateral_fov
        r = self.height - i_indices
        x_pol = cp.round(x_c + r * cp.sin(theta)).astype(cp.int32)
        y_pol = cp.round(y_c - r * cp.cos(theta)).astype(cp.int32)
        valid_mask = (x_pol >= 0) & (x_pol < self.width) & (y_pol >= 0) & (y_pol < self.height)
        mapx[y_pol[valid_mask], x_pol[valid_mask]] = j_indices[valid_mask]
        mapy[y_pol[valid_mask], x_pol[valid_mask]] = i_indices[valid_mask]
        return mapx, mapy

    def reset_grid(self):
        self.linear_bins_cpu.fill(0)
        bin_width = self.width // self.n_bins_x
        bin_height = self.height // self.n_bins_y
        for i in range(self.n_bins_x + 1):
            x = i * bin_width
            cv2.line(self.linear_bins_cpu, (x, 0), (x, self.height), (0, 255, 0), 1)
        for k in range(self.n_bins_y + 1):
            y = k * bin_height
            cv2.line(self.linear_bins_cpu, (0, y), (self.width, y), (0, 255, 0), 1)

    def compute_real_angle(self, computed_angle_rad):
        real_angle_rad = computed_angle_rad + self.lateral_fov / 2
        real_angle_deg = np.rad2deg(real_angle_rad) + 45
        return real_angle_deg

    def stabilize_angles(self, angles):
        if len(angles) != len(self.stabilized_angles):
            self.stabilized_angles = cp.asarray(angles)
            return angles
        angles_gpu = cp.asarray(angles)
        stable = []
        for i, a in enumerate(angles_gpu):
            if i < len(self.stabilized_angles):
                prev = self.stabilized_angles[i]
                new_a = self.conf.stability_smooth_alpha * a + (1 - self.conf.stability_smooth_alpha) * prev
            else:
                new_a = a
            stable.append(float(new_a))
        self.stabilized_angles = cp.asarray(stable)
        return stable

    def update_peak_positions(self, delta_t):
        if not self.tracked_peaks:
            return
        try:
            peak_ids = list(self.tracked_peaks.keys())
            positions = cp.array([
                (data['angle'], data['distance']) 
                for data in self.tracked_peaks.values()
            ], dtype=cp.float32)
            
            velocities = cp.array([
                self.peak_velocities.get(pid, (0.0, 0.0)) 
                for pid in peak_ids
            ], dtype=cp.float32)
            curr_angles = positions[:, 0]
            curr_dists = positions[:, 1]
            v_angles = velocities[:, 0]
            v_dists = velocities[:, 1]
            half_fov = self.lateral_fov / 2
            curr_angles = (curr_angles + cp.pi) % (2 * cp.pi) - cp.pi
            curr_angles = cp.clip(curr_angles, -half_fov, half_fov)
            angle_diffs = cp.zeros_like(curr_angles)
            dist_diffs = cp.zeros_like(curr_dists)
            
            force_angles = self.k_spring * angle_diffs
            force_dists = self.k_spring * dist_diffs

            angle_damps = self.c_damp * (1 + 0.5 * cp.abs(v_angles) / self.max_angle_velocity)
            dist_damps = self.c_damp * (1 + 0.5 * cp.abs(v_dists) / self.max_dist_velocity)
            
            v_angles_new = v_angles + (force_angles - angle_damps * v_angles) * delta_t
            v_dists_new = v_dists + (force_dists - dist_damps * v_dists) * delta_t
            
            v_angles_new = cp.clip(
                v_angles_new, 
                -self.max_angle_velocity, 
                self.max_angle_velocity
            )
            v_dists_new = cp.clip(
                v_dists_new,
                -self.max_dist_velocity,
                self.max_dist_velocity
            )
            
            angles_new = curr_angles + v_angles_new * delta_t
            dists_new = curr_dists + v_dists_new * delta_t
            
            angles_new = (angles_new + cp.pi) % (2 * cp.pi) - cp.pi
            angles_new = cp.clip(angles_new, -half_fov, half_fov)
            dists_new = cp.clip(dists_new, 0, self.max_range)
            
            max_angle_change = cp.deg2rad(15.0)
            angle_changes = cp.abs(angles_new - curr_angles)
            large_changes = angle_changes > max_angle_change
            if cp.any(large_changes):
                angles_new[large_changes] = curr_angles[large_changes] + \
                    cp.sign(angles_new[large_changes] - curr_angles[large_changes]) * max_angle_change
                v_angles_new[large_changes] *= 0.5
            
            positions_new = cp.stack([angles_new, dists_new], axis=1)
            velocities_new = cp.stack([v_angles_new, v_dists_new], axis=1)
            
            positions_cpu = positions_new.get()
            velocities_cpu = velocities_new.get()
            
            updated_peaks = {}
            for i, pid in enumerate(peak_ids):
                updated_peaks[pid] = {
                    'angle': float(positions_cpu[i, 0]),
                    'distance': float(positions_cpu[i, 1]),
                    'missed': self.tracked_peaks[pid]['missed']
                }
                self.peak_velocities[pid] = (
                    float(velocities_cpu[i, 0]),
                    float(velocities_cpu[i, 1])
                )
            
            self.tracked_peaks = updated_peaks
            
        except Exception as e:
            pass

    def match_and_update_peaks(self, new_peaks, delta_t):
        current_peaks_list = [(data['angle'], data['distance'], pid) 
                                for pid, data in self.tracked_peaks.items()]
        matched_peaks, unmatched_new, unmatched_current = self.match_peaks_hungarian(new_peaks, current_peaks_list)
        updated_peaks = {}
        for angle, distance, pid in matched_peaks:
            updated_peaks[pid] = {
                'angle': angle,
                'distance': distance,
                'missed': 0
            }
        merge_threshold = 0.1  
        for i in sorted(unmatched_new, key=lambda i: new_peaks[i][1], reverse=True):
            new_angle, new_distance = new_peaks[i]
            duplicate_found = False
            for peak in updated_peaks.values():
                if abs(peak['angle'] - new_angle) < merge_threshold:
                    duplicate_found = True
                    break
            if not duplicate_found and len(updated_peaks) < self.max_objects:
                updated_peaks[self.next_peak_id] = {
                    'angle': new_angle,
                    'distance': new_distance,
                    'missed': 0
                }
                self.next_peak_id += 1
        for j in unmatched_current:
            existing_pid = list(self.tracked_peaks.keys())[j]
            if existing_pid in self.tracked_peaks:
                new_missed = self.tracked_peaks[existing_pid]['missed'] + 1
                if new_missed <= self.max_missed_frames:
                    updated_peaks[existing_pid] = {
                        'angle': self.tracked_peaks[existing_pid]['angle'],
                        'distance': self.tracked_peaks[existing_pid]['distance'],
                        'missed': new_missed
                    }
        self.tracked_peaks = updated_peaks
        self.update_peak_positions(delta_t)

    def compute_view(self, histo_gpu, radar_image_cpu, delta_t):
        current_time = time.time()
        for peak_id, peak_data in self.tracked_peaks.items():
            self.draw_peak_on_radar(radar_image_cpu, peak_data['angle'], peak_data['distance'], peak_id)
        if current_time - self.last_update_time < self.update_interval:
            return self.last_angles
        self.last_update_time = current_time
        new_peaks = []
        new_angles = []
        if histo_gpu.size > 0:
            peaks_gpu, _ = signal.find_peaks(histo_gpu, prominence=self.min_peak_prominence)
            if len(histo_gpu) > 1:
                if (histo_gpu[0] > (histo_gpu[1] + self.min_peak_prominence)) and (0 not in peaks_gpu):
                    peaks_gpu = cp.append(peaks_gpu, 0)
                if (histo_gpu[-1] > (histo_gpu[-2] + self.min_peak_prominence)) and ((len(histo_gpu)-1) not in peaks_gpu):
                    peaks_gpu = cp.append(peaks_gpu, len(histo_gpu) - 1)
            if len(peaks_gpu) > 0:
                peak_values_gpu = histo_gpu[peaks_gpu]
                valid_mask = (peak_values_gpu >= self.min_ev_rate) & (peak_values_gpu <= self.max_ev_rate)
                valid_peaks_gpu = peaks_gpu[valid_mask]
                valid_values_gpu = peak_values_gpu[valid_mask]
                if len(valid_peaks_gpu) > 0:
                    sorted_idx = cp.argsort(valid_values_gpu)[::-1]
                    if len(sorted_idx) > self.max_objects:
                        sorted_idx = sorted_idx[:self.max_objects]
                    position_idx = cp.argsort(valid_peaks_gpu[sorted_idx])
                    final_peaks_gpu = valid_peaks_gpu[sorted_idx][position_idx]
                    final_values_gpu = valid_values_gpu[sorted_idx][position_idx]
                    final_peaks_cpu = final_peaks_gpu.get()
                    final_values_cpu = final_values_gpu.get()
                    histo_cpu = histo_gpu.get()
                    for peak, val in zip(final_peaks_cpu, final_values_cpu):
                        peak_idx = int(peak)
                        bin_position = interpolate_bin(histo_cpu, peak_idx)
                        angle_rad = (bin_position / (self.n_bins - 1) - 0.5) * self.lateral_fov
                        real_angle_deg = self.compute_real_angle(angle_rad)
                        new_angles.append(real_angle_deg)
                        intensity_ratio = (val - self.min_ev_rate) / (self.max_ev_rate - self.min_ev_rate)
                        intensity_ratio = np.clip(intensity_ratio, 0, 1)
                        distance = self.conf.calculate_distance(intensity_ratio, return_cpu=True)
                        new_peaks.append((angle_rad, distance))
        if len(new_peaks) > 0:
            self.match_and_update_peaks(new_peaks, delta_t)
        smoothed = self.stabilize_angles(new_angles)
        self.last_angles = smoothed
        return smoothed

    def draw_peak_on_radar(self, radar_image_cpu, angle_rad, distance, peak_id):
        angle_rad = self.wrap_angle(angle_rad)
        distance = self.constrain_distance(distance)
        scaled_distance = distance * self.scale_factor
        x = int(self.radar_center_x + scaled_distance * np.sin(angle_rad))
        y = int(self.radar_center_y - scaled_distance * np.cos(angle_rad))
        color = (0, 0, 255)
        cv2.circle(radar_image_cpu, (x, y), 8, color, -1)
        cv2.circle(radar_image_cpu, (x, y), 6, color, -1)
        cv2.circle(radar_image_cpu, (x, y), 4, color, -1)
        cv2.circle(radar_image_cpu, (x, y), 2, color, -1)
        display_angle = np.rad2deg(angle_rad) + 45
        # if int(distance)>95:
        #     distance = f">1m"
        # else:
        #     distance = f"{distance:.1f}cm"
        # # text = f"{display_angle:.1f}o\n{distance}"
        text = f"{display_angle:.1f}o"
        y_offset = y
        (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.putText(radar_image_cpu, text, (x + 10, y_offset+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)
        y_offset += 15
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\entities\camera_bias.py
--------------------------------------------------------------------------------
from dataclasses import dataclass
from typing import Dict, Tuple

@dataclass
class CameraBias:
    name: str
    value: int
    limits: Tuple[int, int]

    def increase(self, step: int = 1) -> int:
        new_value = min(self.value + step, self.limits[1])
        self.value = new_value
        return new_value

    def decrease(self, step: int = 1) -> int:
        new_value = max(self.value - step, self.limits[0])
        self.value = new_value
        return new_value
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\entities\config.py
--------------------------------------------------------------------------------
from dataclasses import dataclass
from typing import List, Optional


@dataclass
class InputConfig:
    type: str  # "video" or "event"
    file_path: str
    width: int
    height: int
    
    # Video specific settings
    target_fps: Optional[float] = None
    
    # Event camera specific settings
    bias_file: Optional[str] = None
    delta_t: Optional[int] = 10000
    crop_coordinates: Optional[List[int]] = None

@dataclass
class ModelConfig:
    path: str
    confidence_threshold: float

@dataclass
class TrackingConfig:
    max_age: int
    min_hits: int
    iou_threshold: float

@dataclass
class CountingZoneConfig:
    name: str
    points: List[List[int]]

@dataclass
class CountingConfig:
    type: str  # "line" or "frame"
    zones: List[CountingZoneConfig]
    alpha: Optional[float] = 0.3
    count_threshold: Optional[float] = 2
    temporal_window: Optional[int] = 5

@dataclass
class VisualizationConfig:
    track_history_length: int
    display_fps: bool
    display_tracks: bool

@dataclass
class SystemConfig:
    input: InputConfig
    model: ModelConfig
    tracking: TrackingConfig
    visualization: VisualizationConfig
    counting: CountingConfig

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\entities\detection.py
--------------------------------------------------------------------------------
from dataclasses import dataclass
from typing import List, Tuple, Optional
import numpy as np

@dataclass
class BoundingBox:
    x1: float
    y1: float
    x2: float
    y2: float
    confidence: float
    class_id: int

@dataclass
class Keypoints:
    points: List[Tuple[float, float]]
    confidences: List[float]

@dataclass
class Detection:
    bbox: BoundingBox
    track_id: Optional[int] = None
    keypoints: Optional[Keypoints] = None

@dataclass
class Frame:
    data: np.ndarray
    height: int
    width: int
    fps: float = 0.0
    detections: List[Detection] = None
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\entities\entities.py
--------------------------------------------------------------------------------
from dataclasses import dataclass

@dataclass
class ProcessingParams:
    def __init__(
        self,
        neighborhood_size: int = 3,
        time_tolerance: int = 1000,
        buffer_size: int = 256,
        use_filter: bool = True,
        output_height: int = 720,
        show_fps: bool = False,
        show_timing: bool = False,
        yolo_conf: float = 0.6,
        yolo_iou: float = 0.5,
        camera_width: int = 320,
        camera_height: int = 320
    ):
        self.neighborhood_size = neighborhood_size
        self.time_tolerance = time_tolerance
        self.buffer_size = buffer_size
        self.use_filter = use_filter
        self.output_height = output_height
        self.show_fps = show_fps
        self.show_timing = show_timing
        self.yolo_conf = yolo_conf
        self.yolo_iou = yolo_iou
        self.camera_width = camera_width
        self.camera_height = camera_height

    def __str__(self):
        return (
            f"ProcessingParams("
            f"neighborhood_size={self.neighborhood_size}, "
            f"time_tolerance={self.time_tolerance}, "
            f"buffer_size={self.buffer_size}, "
            f"use_filter={self.use_filter}, "
            f"output_height={self.output_height}, "
            f"show_fps={self.show_fps}, "
            f"show_timing={self.show_timing}, "
            f"yolo_conf={self.yolo_conf}, "
            f"yolo_iou={self.yolo_iou}, "
            f"camera_width={self.camera_width}, "
            f"camera_height={self.camera_height})"
        )
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\value_objects\biases.py
--------------------------------------------------------------------------------
def load_bias_file(path: str) -> "dict[str,int]":
    """function to load bias values from text file"""
    biases = {}
    with open(path, "r") as file:
        for line in file.readlines():
            line = line.split("%")
            if len(line[0].strip()):
                biases[line[1].strip()] = int(line[0].strip())
    return biases


class Biases:
    """Class to store current camera biases, only works with Gen3.0 cameras"""

    __default_biases = {
        "bias_diff": 300,
        "bias_diff_off": 225,
        "bias_diff_on": 375,
        "bias_fo": 1725,
        "bias_hpf": 1500,
        "bias_pr": 1500,
        "bias_refr": 1500,
    }
    __default_biases_limits = {
        "bias_diff": (300, 300),
        "bias_diff_off": (0, 299),
        "bias_diff_on": (301, 1800),
        "bias_fo": (1650, 1800),
        "bias_hpf": (0, 1800),
        "bias_pr": (1200, 1800),
        "bias_refr": (1300, 1700),
    }

    def __init__(self, biases: "dict[str,int]" = None, biases_limits: "dict[str,tuple[int,int]]" = None):
        """Init"""
        # Check is biases are provided, if not set default biases
        if biases is None:
            self.biases = self.__default_biases
        else:
            self.biases = biases
        # Check is biases are provided, if not set default biases
        if biases_limits is None:
            self.biases_limits = self.__default_biases_limits
        else:
            self.biases_limits = biases_limits

        # Init list with bias names
        self.bias_keys: list[str] = list(self.biases.keys())
        # Init initial trackers for interaction
        self.current_bias_idx: int = 0
        self.current_bias: str = self.bias_keys[self.current_bias_idx]

    def cycle_current_bias(self) -> str:
        """Function to increase currently selected bias for interaction
        and cycle to first if index goes past the last one"""
        self.current_bias_idx += 1
        self.current_bias_idx %= len(self.biases)
        self.current_bias = self.bias_keys[self.current_bias_idx]
        return self.current_bias

    def increase_current(self, step_size=1):
        """Function to increase the currently selected bias by step size"""
        if (self.biases[self.current_bias] + step_size) <= self.biases_limits[self.current_bias][1]:
            self.biases[self.current_bias] += step_size
        else:
            self.biases[self.current_bias] = self.biases_limits[self.current_bias][1]
        return self.biases[self.current_bias]

    def decrease_current(self, step_size=1):
        """Function to decrease the currently selected bias by step size"""
        if (self.biases[self.current_bias] - step_size) >= self.biases_limits[self.current_bias][0]:
            self.biases[self.current_bias] -= step_size
        else:
            self.biases[self.current_bias] = self.biases_limits[self.current_bias][0]
        return self.biases[self.current_bias]

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\value_objects\bias_settings.py
--------------------------------------------------------------------------------
# src/domain/value_objects/bias_settings.py
from dataclasses import dataclass
from typing import Dict, Optional
import os
from ..entities.camera_bias import CameraBias

@dataclass
class BiasSettings:
    DEFAULT_BIASES = {
        "bias_diff": (300, (300, 300)),
        "bias_diff_off": (225, (0, 299)),
        "bias_diff_on": (375, (301, 1800)),
        "bias_fo": (1725, (1650, 1800)),
        "bias_hpf": (1500, (0, 1800)),
        "bias_pr": (1500, (1200, 1800)),
        "bias_refr": (1500, (1300, 1700)),
    }

    biases: Dict[str, CameraBias]
    current_bias_name: str

    @classmethod
    def create_default(cls) -> 'BiasSettings':
        biases = {
            name: CameraBias(name, value, limits)
            for name, (value, limits) in cls.DEFAULT_BIASES.items()
        }
        return cls(biases=biases, current_bias_name=next(iter(biases)))

    @classmethod
    def from_dict(cls, bias_dict: Dict[str, int]) -> 'BiasSettings':
        biases = {}
        for name, value in bias_dict.items():
            if name in cls.DEFAULT_BIASES:
                limits = cls.DEFAULT_BIASES[name][1]
                biases[name] = CameraBias(name, value, limits)
        return cls(biases=biases, current_bias_name=next(iter(biases)))

    @classmethod
    def from_file(cls, file_path: str) -> 'BiasSettings':
        """Load bias settings from a file"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Bias file not found: {file_path}")
            
        bias_dict = {}
        with open(file_path, "r") as file:
            for line in file.readlines():
                line = line.split("%")
                if len(line[0].strip()):
                    bias_dict[line[1].strip()] = int(line[0].strip())
        
        return cls.from_dict(bias_dict)

    def cycle_current_bias(self) -> str:
        current_idx = list(self.biases.keys()).index(self.current_bias_name)
        next_idx = (current_idx + 1) % len(self.biases)
        self.current_bias_name = list(self.biases.keys())[next_idx]
        return self.current_bias_name
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\value_objects\config.py
--------------------------------------------------------------------------------
# config.py

import cupy as cp
import numpy as np

class RadarConfig:
    """
    Configuration class for radar visualization and event processing.
    Manages parameters for event filtering, radar display, and object tracking.
    """
    def __init__(self):
        ## Old config
        self.n_bins = 15
        self.n_bins_x = 15
        # self.n_bins = 10
        # self.n_bins_x = 10
        self.n_bins_y = 5
        self.lateral_fov = np.pi / 2  
        
        self.min_ev_rate_unfiltered = 5e3
        self.max_ev_rate_unfiltered = 1e7
        self.min_peak_prominence_unfiltered = 1e4
        self.low_event_threshold_unfiltered = 5e3
        
        ## New config
        # self.min_ev_rate_filtered = 1e3
        # self.max_ev_rate_filtered = 1e7
        # self.min_peak_prominence_filtered = 6e3
        # self.low_event_threshold_filtered = 4e3
        # Old config (better)
        self.min_ev_rate_filtered = 8e2
        self.max_ev_rate_filtered = 1e7
        self.min_peak_prominence_filtered = 5e3
        self.low_event_threshold_filtered = 1e3
        
        self.use_filter = True
        self.min_ev_rate = self.min_ev_rate_filtered
        self.max_ev_rate = self.max_ev_rate_filtered
        self.min_peak_prominence = self.min_peak_prominence_filtered
        self.low_event_threshold = self.low_event_threshold_filtered
        
        self.delta_t = 33000 
        self.max_range = 100.0  
        self.trim_angle_deg = 1.0  
        self.max_objects = 3  
        
        self.stability_smooth_alpha = 1 
        self.frame_history = 10  
        self.angle_history = cp.zeros((self.frame_history,), dtype=cp.float32)
        self.angle_diff_threshold = 15.0 
        
        self.mot_gating_threshold = 40.0  
    
    def set_filter_mode(self, use_filter: bool) -> None:
        """
        Updates event processing thresholds based on filter mode.
        
        Args:
            use_filter (bool): If True, uses filtered mode thresholds; otherwise, uses unfiltered thresholds.
        """
        if use_filter:
            self.min_ev_rate = self.min_ev_rate_filtered
            self.max_ev_rate = self.max_ev_rate_filtered
            self.min_peak_prominence = self.min_peak_prominence_filtered
            self.low_event_threshold = self.low_event_threshold_filtered
        else:
            self.use_filter = False
            self.min_ev_rate = self.min_ev_rate_unfiltered
            self.max_ev_rate = self.max_ev_rate_unfiltered
            self.min_peak_prominence = self.min_peak_prominence_unfiltered
            self.low_event_threshold = self.low_event_threshold_unfiltered
    
    def calculate_distance(self, intensity_ratio: float, return_cpu: bool = True) -> float:
        """
        Calculates distance based on event intensity ratio.
        
        Args:
            intensity_ratio (float): Ratio of event intensity (0-1).
            return_cpu (bool): If True, returns a CPU float; otherwise, returns a GPU array.
        
        Returns:
            float or cupy.ndarray: Calculated distance in centimeters.
        """
        distance = (1.0 - intensity_ratio) * self.max_range
        return float(distance.get()) if return_cpu and isinstance(distance, cp.ndarray) else distance

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\value_objects\counting_result.py
--------------------------------------------------------------------------------
from dataclasses import dataclass
from collections import deque

@dataclass
class CountingResult:
    current_count: int = 0
    smoothed_count: int = 0
    max_count: int = 0
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\value_objects\counting_zone.py
--------------------------------------------------------------------------------
from dataclasses import dataclass
from typing import Tuple, List

@dataclass
class CountingZone:
    points: List[Tuple[int, int]]  # Polygon points defining the counting zone
    name: str
    in_count: int = 0
    out_count: int = 0

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\domain\value_objects\event_frame.py
--------------------------------------------------------------------------------
from dataclasses import dataclass
import numpy as np
from typing import Optional, Tuple

@dataclass
class EventFrame:
    data: np.ndarray
    original_dimensions: Tuple[int, int]  # (width, height)
    crop_coordinates: Optional[Tuple[int, int, int, int]] = None  # (x1, y1, x2, y2)
    events: np.ndarray = None
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\counting\frame_counter.py
--------------------------------------------------------------------------------
from collections import deque
from typing import List
from src.application.interfaces.counter import ICounter
from src.domain.entities.detection import Detection
from src.domain.value_objects.counting_result import CountingResult


class FrameCounter(ICounter):
    def __init__(self, 
                 alpha: float = 0.3, 
                 count_threshold: int = 2,
                 temporal_window: int = 5):
        self.counting_result = CountingResult()
        self.alpha = alpha
        self.ema_count = 0
        self.is_initialized = False
        self.count_threshold = count_threshold
        self.temporal_window = temporal_window
        self.recent_counts = deque(maxlen=temporal_window)

    def update(self, detections: List[Detection]) -> CountingResult:
        # Lọc detections có confidence cao
        reliable_detections = [
            det for det in detections 
            if det.bbox.confidence >= self.count_threshold
        ]
        
        current_count = len(reliable_detections)
        self.recent_counts.append(current_count)

        if len(self.recent_counts) == self.temporal_window:
            avg_count = sum(self.recent_counts) / len(self.recent_counts)
            count_diff = abs(avg_count - self.ema_count)
            
            if count_diff >= self.count_threshold:
                if not self.is_initialized:
                    self.ema_count = avg_count
                    self.is_initialized = True
                else:
                    self.ema_count = (self.alpha * avg_count + 
                                    (1 - self.alpha) * self.ema_count)

        smoothed_count = int(round(self.ema_count))
        
        self.counting_result.current_count = current_count
        self.counting_result.smoothed_count = smoothed_count
        
        if smoothed_count > self.counting_result.max_count:
            self.counting_result.max_count = smoothed_count
            
        return self.counting_result
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\counting\line_counter.py
--------------------------------------------------------------------------------
from typing import List, Tuple
import numpy as np
from shapely.geometry import Point, Polygon
from ...domain.entities.detection import BoundingBox, Detection
from ...domain.value_objects.counting_zone import CountingZone
from ...application.interfaces.counter import ICounter

class LineCounter(ICounter):
    def __init__(self):
        self.previous_positions = {}  # track_id -> previous_position

    def update(self, tracks: List[Detection], zones: List[CountingZone]) -> List[CountingZone]:
        current_positions = {
            t.track_id: self._get_center(t.bbox)
            for t in tracks if t.track_id is not None
        }
        
        for zone in zones:
            polygon = Polygon(zone.points)
            
            for track_id, current_pos in current_positions.items():
                if track_id in self.previous_positions:
                    prev_pos = self.previous_positions[track_id]
                    current_point = Point(current_pos)
                    prev_point = Point(prev_pos)
                    
                    # Check if crossing happened
                    if polygon.contains(current_point) != polygon.contains(prev_point):
                        if polygon.contains(current_point):
                            zone.in_count += 1
                        else:
                            zone.out_count += 1
        
        self.previous_positions = current_positions
        return zones
    
    def _get_center(self, bbox: BoundingBox) -> Tuple[float, float]:
        return ((bbox.x1 + bbox.x2) / 2, (bbox.y1 + bbox.y2) / 2)
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\event_camera\exceptions\camera_exceptions.py
--------------------------------------------------------------------------------

class CameraException(Exception):
    """Base exception for camera-related errors"""
    pass

class CameraNotFoundError(CameraException):
    """Raised when camera device cannot be found"""
    pass

class InvalidFileError(CameraException):
    """Raised when input file is invalid"""
    pass
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\event_camera\iterators\bias_events_iterator.py
--------------------------------------------------------------------------------
# src/infrastructure/event_camera/iterators/bias_events_iterator.py
from typing import Optional
import os.path
import metavision_hal as mv_hal
from metavision_core.event_io import EventsIterator
from src.domain.value_objects.bias_settings import BiasSettings
from src.application.interfaces.event_iterator import IEventIterator

class BiasEventsIterator(IEventIterator):
    def __init__(
        self,
        delta_t: int,
        input_filename: Optional[str] = None,
        bias_file: Optional[str] = None
    ):
        self.__is_live = not input_filename

        # Load bias settings if bias file is provided
        bias_settings = None
        if bias_file:
            bias_settings = BiasSettings.from_file(bias_file)
        elif self.__is_live:  # Use default settings for live camera
            bias_settings = BiasSettings.create_default()

        if not self.__is_live:
            if not os.path.exists(input_filename):
                raise FileNotFoundError(f"Input file not found: {input_filename}")
            self.__ev_it = EventsIterator(input_filename, delta_t=delta_t)
        else:
            self.device = mv_hal.DeviceDiscovery.open("")
            if not self.device:
                raise RuntimeError("No live camera found")

            # Configure camera biases
            if bias_settings:
                biases = self.device.get_i_ll_biases()
                for name, bias in bias_settings.biases.items():
                    biases.set(name, bias.value)

            self.__ev_it = EventsIterator(self.device, delta_t=delta_t)

    def __iter__(self):
        yield from self.__ev_it
        
    def __del__(self):
        if hasattr(self, '__ev_it'):
            self.__ev_it.close()  
        # if hasattr(self, 'device'):
        #     self.device.close()   

    def get_size(self) -> int:
        return self.__ev_it.get_size()
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\event_camera\iterators\buffered_iterator.py
--------------------------------------------------------------------------------
from typing import Optional, Iterator, Any
import os.path
import metavision_hal as mv_hal
from metavision_core.event_io import EventsIterator

from ....application.interfaces.event_iterator import IEventIterator
from ....domain.value_objects.bias_settings import BiasSettings
from ..exceptions.camera_exceptions import CameraNotFoundError, InvalidFileError

class BufferedEventIterator(IEventIterator):
    def __init__(self, delta_t: float, input_filename: Optional[str] = None, 
                 bias_settings: Optional[BiasSettings] = None):
        self.__is_live = not input_filename

        if not self.__is_live:
            if not os.path.isfile(input_filename):
                raise InvalidFileError(f"Invalid input file: {input_filename}")
            self.__ev_it = EventsIterator(input_filename, delta_t=delta_t)
        else:
            device = mv_hal.DeviceDiscovery.open("")
            if not device:
                raise CameraNotFoundError("No live camera found")

            if bias_settings:
                for name, bias in bias_settings.biases.items():
                    device.get_i_ll_biases().set(name, bias.value)

            self.__ev_it = EventsIterator(device, delta_t=delta_t)

    def __iter__(self) -> Iterator[Any]:
        yield from self.__ev_it

    def get_size(self) -> int:
        return self.__ev_it.get_size()
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\event_camera\iterators\non_buffered_iterator.py
--------------------------------------------------------------------------------
from typing import Optional, Iterator, Any
import os.path
import metavision_hal as mv_hal
from metavision_core.event_io.raw_reader import RawReaderBase

from ....application.interfaces.event_iterator import IEventIterator
from ....domain.value_objects.bias_settings import BiasSettings
from ..exceptions.camera_exceptions import CameraNotFoundError, InvalidFileError

class NonBufferedEventIterator(IEventIterator):
    def __init__(self, delta_t: float, input_filename: Optional[str] = None,
                 bias_settings: Optional[BiasSettings] = None):
        self.__is_live = not input_filename

        if not self.__is_live:
            if not os.path.isfile(input_filename):
                raise InvalidFileError(f"Invalid input file: {input_filename}")
            self.reader = RawReaderBase(input_filename, delta_t=delta_t)
        else:
            device = mv_hal.DeviceDiscovery.open("")
            if not device:
                raise CameraNotFoundError("No live camera found")

            geometry = device.get_i_geometry()
            print(f"Camera resolution: {geometry.get_width()} x {geometry.get_height()}")

            if bias_settings:
                for name, bias in bias_settings.biases.items():
                    device.get_i_ll_biases().set(name, bias.value)

            self.reader = RawReaderBase("", device=device, delta_t=delta_t, 
                                      initiate_device=False)

    def __iter__(self) -> Iterator[Any]:
        while not self.reader.is_done():
            yield self.reader.load_delta_t(-1)

    def get_size(self) -> int:
        return self.reader.get_size()

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\event_camera\iterators\video_frame_iterator.py
--------------------------------------------------------------------------------
from typing import Iterator, Optional
import cv2
from src.application.interfaces.event_iterator import IEventIterator
import numpy as np

class VideoFrameIterator(IEventIterator):
    """Iterator class for reading frames from video files or camera streams"""
    
    def __init__(self, 
                 input_source: Optional[str] = None,
                 target_fps: Optional[float] = None,
                 resize_dims: Optional[tuple] = None):
        """
        Initialize video frame iterator
        
        Args:
            input_source: Path to video file or camera index (0 for default camera)
            target_fps: Target frame rate to process video at (None for original video fps)
            resize_dims: Tuple of (width, height) to resize frames to (None for original size)
        """
        # Handle camera index (int) or video file path (str)
        self.input_source = 0 if input_source is None else input_source
        self.cap = cv2.VideoCapture(self.input_source)
        
        if not self.cap.isOpened():
            raise RuntimeError(f"Failed to open video source: {input_source}")
            
        # Get video properties
        self.original_fps = self.cap.get(cv2.CAP_PROP_FPS)
        print("FPS of webcam: ", self.original_fps)
        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # Set target properties
        self.target_fps = self.original_fps if target_fps is None else target_fps
        self.resize_dims = resize_dims
        
        # Calculate frame skip based on target FPS
        if self.target_fps and self.target_fps < self.original_fps:
            self.frame_skip = max(1, int(self.original_fps / self.target_fps))
        else:
            self.frame_skip = 1
            
    def __iter__(self) -> Iterator[np.ndarray]:
        """Iterate over video frames"""
        frame_count = 0
        
        while True:
            ret, frame = self.cap.read()
            
            if not ret:
                break
                
            # Skip frames based on target FPS
            frame_count += 1
            if frame_count % self.frame_skip != 0:
                continue
                
            # Resize frame if dimensions specified
            if self.resize_dims:
                frame = cv2.resize(frame, self.resize_dims)
                
            yield frame
            
    def __del__(self):
        """Release video capture resources"""
        if hasattr(self, 'cap'):
            self.cap.release()
            
    def get_size(self) -> int:
        """Get total number of frames in video"""
        return self.frame_count
        
    def get_frame_size(self) -> tuple:
        """Get frame dimensions (width, height)"""
        if self.resize_dims:
            return self.resize_dims
        return (self.width, self.height)
        
    def get_fps(self) -> float:
        """Get effective frame rate"""
        return min(self.target_fps, self.original_fps) if self.target_fps else self.original_fps
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\event_camera\processors\event_processor.py
--------------------------------------------------------------------------------
import numpy as np
from typing import Optional, Union, Tuple
from src.domain.value_objects.event_frame import EventFrame

class EventProcessor:
    """Handles conversion of events to image frames"""
    
    def __init__(self, 
                 width: int = 320, 
                 height: int = 320,
                 crop_coordinates: Optional[Tuple[int, int, int, int]] = (225, 50, 720, 600)):
        self.width = width
        self.height = height
        self.crop_coordinates = crop_coordinates
        self._buffer = None if crop_coordinates else np.ones((height, width, 3), dtype=np.uint8) * 127

    def create_frame(self, 
                    events: np.ndarray, 
                    reuse_buffer: bool = True) -> EventFrame:
        """
        Convert events to a binary histogram image frame.
        
        Args:
            events: Structured numpy array with fields ('x', 'y', 'p', 't')
            reuse_buffer: Whether to reuse the existing image buffer
        
        Returns:
            EventFrame object containing the processed image
        """
        # Convert events to correct dtype if needed
        if events.dtype != [('x', '<u2'), ('y', '<u2'), ('p', '<i2'), ('t', '<i8')]:
            events = np.array(events, 
                            dtype=[('x', '<u2'), ('y', '<u2'), 
                                  ('p', '<i2'), ('t', '<i8')])

        # Create or reuse image buffer
        if self._buffer is None or not reuse_buffer:
            img = np.ones((self.height, self.width, 3), dtype=np.uint8) * 127
        else:
            img = self._buffer
            img[...] = 127

        if events.size:
            assert events['x'].max() < self.width, "out of bound events: x = {}, w = {}".format(events['x'].max(), self.width)
            assert events['y'].max() < self.height, "out of bound events: y = {}, h = {}".format(events['y'].max(), self.height)

            img[events['y'], events['x'], :] = 255 * events['p'][:, None]

        # Apply cropping if specified
        if self.crop_coordinates:
            x1, y1, x2, y2 = self.crop_coordinates
            img = img[y1:y2, x1:x2]

        # Store buffer for reuse if needed
        if reuse_buffer:
            self._buffer = img

        return EventFrame(
            data=img,
            original_dimensions=(self.width, self.height),
            crop_coordinates=self.crop_coordinates,
            events=events
        )

    def reset_buffer(self) -> None:
        """Reset the internal image buffer"""
        if self._buffer is not None:
            self._buffer[...] = 127




--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\event_camera\utils\bias_file_loader.py
--------------------------------------------------------------------------------
from typing import Dict
from ....domain.value_objects.bias_settings import BiasSettings

def load_bias_file(path: str) -> BiasSettings:
    """Load bias values from text file and return BiasSettings"""
    bias_dict = {}
    with open(path, "r") as file:
        for line in file.readlines():
            line = line.split("%")
            if len(line[0].strip()):
                bias_dict[line[1].strip()] = int(line[0].strip())
    return BiasSettings.from_dict(bias_dict)

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\ml_models\evs_fall.py
--------------------------------------------------------------------------------
import time
import numpy as np
import onnxruntime as ort

class evs_fall_exist:
    def __init__(self, path, model_type='tflite', num_threads=1):
        """Initialize the fall detection model.
        
        Args:
            path (str): Path to the model file
            model_type (str): Type of model ('tflite' or 'onnx')
            num_threads (int): Number of threads for TFLite inference
        """
        print(f"Initializing model with type: {model_type}")
        # if model_type == 'onnx':
            # print(f"ONNX Runtime version: {ort.__version__}")
            # print(f"Available providers: {ort.get_available_providers()}")
        
        self.model_type = model_type
        self.model_path = path
        
        if model_type == 'onnx':
            self.initialize_onnx_model(path)
        else:
            raise ValueError("Unsupported model type. Use 'tflite' or 'onnx'.")
        
        # Perform warm-up during initialization
        self.is_warmed_up = False
        self.warm_up()
        
    def warm_up(self, num_iterations=3):
        """Warm up the model by running multiple inferences with dummy data.
        
        Args:
            num_iterations (int): Number of warm-up iterations
            
        Returns:
            bool: True if warm-up was successful, False otherwise
        """
        try:
            print(f"Warming up {self.model_type} model...")
            start_time = time.time()
            
            # Create dummy input based on expected input shape
            if self.model_type == 'tflite':
                input_shape = tuple(self.input_shape[1:])  # Remove batch dimension
                dummy_frame = np.random.random(input_shape).astype(np.float32)
                dummy_frames = [dummy_frame for _ in range(max(10, input_shape[0]))]
            else:  # onnx
                # For ONNX, create dummy data matching expected dimensions (320x30)
                batch, channels, width, height = self.input_shape  # Note: width is 320, height is 30
                dummy_frame = np.random.random((height, width, channels)).astype(np.float32)
                dummy_frames = [dummy_frame for _ in range(10)]  # Fixed number of frames for warm-up
            dummy_idx = 0
            
            # Run warm-up iterations
            for i in range(num_iterations):
                _ = self.detect_fall_exist(dummy_frames, dummy_idx)
            
            elapsed_time = time.time() - start_time
            print(f"Model warm-up completed: {num_iterations} iterations in {elapsed_time:.2f} seconds")
            
            self.is_warmed_up = True
            return True
            
        except Exception as e:
            print(f"Error during model warm-up: {str(e)}")
            return False
    
    def __call__(self, frame_array, idx_a):
        """Detect falls in the given frame array.
        
        Args:
            frame_array: Array of frames
            idx_a: Index for frame selection
            
        Returns:
            tuple: (fall_detection, existence_detection)
        """
        return self.detect_fall_exist(frame_array, idx_a)
    

    def initialize_onnx_model(self, path):
        """Initialize ONNX model with CUDA support and CPU fallback.
        
        Args:
            path (str): Path to ONNX model
        """
        try:
            # First try with CUDA
            providers = ['CUDAExecutionProvider', "CPUExecutionProvider"]
            provider_options = [
                {
                    'device_id': 0
                },
                {}
            ]
            
            self.session = ort.InferenceSession(
                path,
                providers=providers,
                provider_options=provider_options
            )
        
            print("ONNX model initialized with CUDA support")
            
        except Exception as e:
            print(f"Failed to initialize with CUDA, falling back to CPU. Error: {e}")
            try:
                self.session = ort.InferenceSession(
                    path,
                    providers=['CPUExecutionProvider']
                )
                print("ONNX model initialized with CPU support")
            except Exception as e:
                raise RuntimeError(f"Failed to initialize ONNX model: {str(e)}")
        
        # Get model information
        self.input_name = self.session.get_inputs()[0].name
        self.input_shape = self.session.get_inputs()[0].shape
        self.output_names = [output.name for output in self.session.get_outputs()]
        
        print(f"Input Name: {self.input_name}")
        print(f"Input Shape: {self.input_shape}")
        print(f"Output Names: {self.output_names}")
    
    def detect_fall_exist(self, frame_array, idx_a):
        """Detect falls and existence in frames.
        
        Args:
            frame_array: Array of frames
            idx_a: Index for frame selection
            
        Returns:
            tuple: (fall_detection, existence_detection)
        """
        try:
            x = self.prepare_input(frame_array, idx_a)
            
            outputs = self.session.run(self.output_names, {self.input_name: x})
            output_001 = outputs[0]
            output_002 = outputs[1] if len(outputs) > 1 else outputs[0]
        
            output_fall, output_exists = self.process_outputs(output_001, output_002)
            return output_fall, output_exists
            
        except Exception as e:
            print(f"Error during inference: {str(e)}")
            return None, None
    
    def process_outputs(self, output_001, output_002):
        """Process model outputs to determine fall and existence detection.
        
        Args:
            output_001: First model output
            output_002: Second model output
            
        Returns:
            tuple: (fall_detection, existence_detection)
        """
        # Extract single values from outputs if they're arrays
        if isinstance(output_001, np.ndarray):
            output_001 = output_001.item()
        if isinstance(output_002, np.ndarray):
            output_002 = output_002.item()
        
        if output_001 < 0:
            output_fall = output_002
            output_exists = -output_001
        elif output_002 < 0:
            output_fall = output_001
            output_exists = -output_002
        elif output_001 > 0:
            output_fall = output_001
            output_exists = output_002
        else:
            output_fall = output_002
            output_exists = output_001
            
        return float(output_fall), float(output_exists)
    
    def prepare_input(self, frame_array, idx_a):
        """Prepare input frames for model inference.
        
        Args:
            frame_array: Array of frames
            idx_a: Index for frame selection
            
        Returns:
            numpy.ndarray: Processed input array
        """
        try:
            Frame_Array = np.array(frame_array)
            x = Frame_Array[idx_a]
            x = np.expand_dims(x, axis=0).astype('float32')
            
            if self.model_type == 'onnx':
                # Check and fix input shape if needed
                expected_shape = tuple(self.input_shape)
                current_shape = x.shape
                
                if current_shape != expected_shape:
                    # print(f"Input shape mismatch. Expected: {expected_shape}, Got: {current_shape}")
                    if len(expected_shape) == 4:
                        x = np.transpose(x, (0, 2, 3, 1))
                        # print(f"Reshaped input to: {x.shape}")
            
            return x
            
        except Exception as e:
            raise RuntimeError(f"Error preparing input: {str(e)}")


--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\ml_models\fall_detector.py
--------------------------------------------------------------------------------
from collections import deque
import time

import cv2
import numpy as np

from src.infrastructure.ml_models.evs_fall import evs_fall_exist


class PeopleFall:
    def __init__(self, fall_model_path,  model_type='onnx', buffer_size=60):
        """Initialize PeopleFall

        Args:
            fall_model: Fall detection model instance.
            buffer_size: Size of the buffer for smoothing fall and existence detection.
        """
        self.fall_model_path = fall_model_path
        self.Frame_Array = deque(maxlen=buffer_size)
        self.idx_a = np.array([ii for ii in range(buffer_size) if ii % 2 == 0])

        # Placeholder for the fall detector instance
        self.model_type = model_type
        self.evs_fall_detector = evs_fall_exist(self.fall_model_path,self.model_type, num_threads=1)

        # Buffers for fall and existence smoothing
        self.FALL_Arr = deque([False] * 10, maxlen=10)
        self.EXIST_Arr = deque([False] * 10, maxlen=10)

    def process_frame(self, frame):
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        self.Frame_Array.append(gray_frame)
        if len(self.Frame_Array) == 60:
            start_time = time.time()
            
            Frame_list = list(self.Frame_Array)
           
            # Run the fall detection model
            output_fall, output_exists = self.evs_fall_detector(Frame_list, self.idx_a)
            # print(output_fall)
            pred_fall = np.squeeze(output_fall)
            pred_exists = np.squeeze(output_exists)

            # Update fall buffer
            if pred_fall >= 0.5:
                class_action = 'FALL'
                self.FALL_Arr.append(True)
            else:
                class_action = 'Normal'
                self.FALL_Arr.append(False)

            # Update existence buffer
            if pred_exists >= 0.5:
                class_exists = 'Person'
                self.EXIST_Arr.append(True)
            else:
                class_exists = 'No Person'
                self.EXIST_Arr.append(False)

            # Determine if a person exists
            class_person_exists = 'Person' if self.EXIST_Arr.count(True) > 5 else ''

            # print('Num of Fall: ', self.FALL_Arr.count(True))
            if self.FALL_Arr.count(True) >= 5:
                param = '*** FALL detected!  sending message.'
                
                self.FALL_Arr = deque([False] * 10, maxlen=10)  # Reset fall buffer
                self.Frame_Array.clear()  # Clear frame buffer
            else:
                self.Frame_Array = deque(Frame_list[5:], maxlen=60)
               
            end_time = time.time()
            print("Fall detect time: ", (round((end_time - start_time) * 1000, 2)))
        
            return class_action
        else:
            return "Normal"
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\ml_models\onnx_detect.py
--------------------------------------------------------------------------------
import cv2
import numpy as np
import onnxruntime as ort
from src.domain.entities.entities import ProcessingParams
    
class ONNXDetector:
    def __init__(self, model_path: str, params: ProcessingParams):
        self.session = ort.InferenceSession(
            model_path,
            providers=["CUDAExecutionProvider", "CPUExecutionProvider"]
        )
        self.params = params
        
        # Get model input details
        model_inputs = self.session.get_inputs()
        self.input_shape = model_inputs[0].shape
        self.input_width = self.input_shape[2]
        self.input_height = self.input_shape[3]

    def preprocess(self, img):
        self.img_height, self.img_width = img.shape[:2]
        
        # Convert BGR to RGB and resize
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (self.input_width, self.input_height))
        
        # Normalize and transpose
        img = np.array(img) / 255.0
        img = np.transpose(img, (2, 0, 1))
        img = np.expand_dims(img, axis=0).astype(np.float32)
        
        return img

    def postprocess(self, img, outputs):
        outputs = np.transpose(np.squeeze(outputs[0]))
        rows = outputs.shape[0]

        boxes = []
        scores = []
        
        # Scale factors
        x_factor = self.img_width / self.input_width
        y_factor = self.img_height / self.input_height

        for i in range(rows):
            score = outputs[i][4]  # Person score
            
            if score >= self.params.yolo_conf:
                x, y, w, h = outputs[i][0:4]
                
                # Convert to corner format
                x1 = (x - w/2) * x_factor
                y1 = (y - h/2) * y_factor
                x2 = (x + w/2) * x_factor
                y2 = (y + h/2) * y_factor
                
                boxes.append([x1, y1, x2, y2])
                scores.append(score)

        # Apply NMS
        if boxes:
            boxes = np.array(boxes)
            scores = np.array(scores)
            indices = cv2.dnn.NMSBoxes(
                boxes.tolist(), 
                scores.tolist(), 
                self.params.yolo_conf, 
                self.params.yolo_iou
            )
            
            # Convert to normalized coordinates
            boxes_n = boxes[indices].copy()
            boxes_n[:, [0, 2]] /= self.img_width
            boxes_n[:, [1, 3]] /= self.img_height
            
            return boxes[indices], boxes_n, len(indices)
        
        return np.array([]), np.array([]), 0

    def detect_and_track(self, frame):
        # Preprocess
        img_data = self.preprocess(frame)
        
        # Inference
        outputs = self.session.run(None, {self.session.get_inputs()[0].name: img_data})
        
        # Postprocess
        boxes, boxes_n, current_count = self.postprocess(frame, outputs)
        
        return boxes, boxes_n, current_count
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\ml_models\yolo_detector.py
--------------------------------------------------------------------------------
from typing import List
import numpy as np
from ultralytics import YOLO
from ...domain.entities.detection import Detection, BoundingBox, Keypoints, Frame
from ...application.interfaces.detector import IDetector

class YoloDetector(IDetector):
    def __init__(self, model_path: str, conf_threshold: float = 0.5):
        self.model = YOLO(model_path)
        self.conf_threshold = conf_threshold

    def detect(self, frame: Frame) -> List[Detection]:
        results = self.model.predict(
            frame.data, 
            conf=self.conf_threshold,
            verbose=False
        )[0]
        
        detections = []
        for r in results.boxes.data:
            x1, y1, x2, y2, conf, cls_id = r
            bbox = BoundingBox(x1, y1, x2, y2, conf, int(cls_id))
            
            keypoints = None
            if hasattr(results, 'keypoints') and results.keypoints is not None:
                kpts = results.keypoints[0].data
                points = [(float(x), float(y)) for x, y, conf in kpts]
                confidences = [float(conf) for _, _, conf in kpts]
                keypoints = Keypoints(points, confidences)
                
            detections.append(Detection(bbox=bbox, keypoints=keypoints))
            
        return detections
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\setup\component_setup.py
--------------------------------------------------------------------------------
# src/infrastructure/setup/component_setup.py

import logging
from typing import Tuple, Any, Optional
from src.application.services.config_service import SystemConfig
from src.infrastructure.event_camera.iterators.bias_events_iterator import BiasEventsIterator
from src.infrastructure.event_camera.iterators.video_frame_iterator import VideoFrameIterator
from src.infrastructure.event_camera.processors.event_processor import EventProcessor
from src.infrastructure.ml_models.yolo_detector import YoloDetector
from src.infrastructure.tracking.byte_tracker import ByteTracker
from src.infrastructure.counting.line_counter import LineCounter
from src.infrastructure.counting.frame_counter import FrameCounter
from src.infrastructure.visualization.display import DisplayProcessor
from src.application.services.video_detection_service import VideoDetectionService
from src.application.services.detection_service import DetectionService

logger = logging.getLogger(__name__)

def setup_input_components(config: SystemConfig) -> Tuple[Any, Optional[EventProcessor]]:
    """Initialize input components based on input type"""
    if config.input.type == "video":
        iterator = VideoFrameIterator(
            input_source=config.input.file_path,
            target_fps=config.input.target_fps if hasattr(config.input, 'target_fps') else None,
            resize_dims=(config.input.width, config.input.height)
        )
        processor = None
        logger.info("Initialized video frame iterator")
    else:  # event camera
        iterator = BiasEventsIterator(
            delta_t=10000,
            input_filename=config.input.file_path if config.input.file_path else None,
            bias_file=config.input.bias_file if hasattr(config.input, 'bias_file') else None
        )
        processor = EventProcessor(
            width=config.input.width,
            height=config.input.height,
            crop_coordinates=None
        )
        logger.info("Initialized event iterator and processor")
    
    return iterator, processor

def setup_detector(config: SystemConfig) -> YoloDetector:
    """Initialize YOLO detector"""
    detector = YoloDetector(
        model_path=config.model.path,
        conf_threshold=config.model.confidence_threshold
    )
    logger.info(f"Loaded detection model from {config.model.path}")
    return detector

def setup_tracker(config: SystemConfig) -> ByteTracker:
    """Initialize object tracker"""
    tracker = ByteTracker(
        max_age=config.tracking.max_age,
        min_hits=config.tracking.min_hits,
        iou_threshold=config.tracking.iou_threshold
    )
    logger.info("Initialized tracker")
    return tracker

def setup_counter(config: SystemConfig) -> Any:
    """Initialize appropriate counter based on configuration"""
    if config.counting.type == "line":
        counter = LineCounter()
        logger.info("Initialized line counter")
    else:
        counter = FrameCounter(
            alpha=config.counting.alpha,
            count_threshold=config.counting.count_threshold,
            temporal_window=config.counting.temporal_window
        )
        logger.info("Initialized frame counter")
    return counter

def setup_display(config: SystemConfig) -> DisplayProcessor:
    """Initialize display processor"""
    display = DisplayProcessor(
        track_history_length=config.visualization.track_history_length,
        display_fps=config.visualization.display_fps
    )
    logger.info("Initialized display")
    return display

def setup_detection_service(
    config: SystemConfig,
    iterator: Any,
    processor: Optional[EventProcessor],
    detector: YoloDetector,
    tracker: ByteTracker,
    counter: Any
) -> Any:
    """Initialize appropriate detection service based on input type"""
    if config.input.type == "video":
        service = VideoDetectionService(
            frame_iterator=iterator,
            detector=detector,
            tracker=tracker,
            counter=counter
        )
        logger.info("Initialized video detection service")
    else:
        service = DetectionService(
            event_iterator=iterator,
            detector=detector,
            tracker=tracker,
            counter=counter,
            event_processor=processor
        )
        logger.info("Initialized event detection service")
    
    return service

def setup_all_components(config: SystemConfig) -> Tuple[Any, Any, YoloDetector, ByteTracker, Any, DisplayProcessor]:
    """Initialize all system components based on configuration"""
    try:
        # Initialize input components
        iterator, processor = setup_input_components(config)
        
        # Initialize other components
        detector = setup_detector(config)
        tracker = setup_tracker(config)
        counter = setup_counter(config)
        display = setup_display(config)
        
        return iterator, processor, detector, tracker, counter, display

    except Exception as e:
        logger.error(f"Error during component initialization: {str(e)}")
        raise
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\tracking\byte_tracker.py
--------------------------------------------------------------------------------
from typing import List, Tuple
import numpy as np
from scipy.optimize import linear_sum_assignment
from src.domain.entities.detection import BoundingBox, Detection
from src.application.interfaces.tracker import ITracker
from .track import Track

class ByteTracker(ITracker):
    """ByteTrack implementation for multi-object tracking"""
    def __init__(
        self,
        max_age: int = 30,
        min_hits: int = 3,
        iou_threshold: float = 0.3,
        track_thresh: float = 0.5,
        track_high_thresh: float = 0.6,
        track_low_thresh: float = 0.1
    ):
        self.max_age = max_age
        self.min_hits = min_hits
        self.iou_threshold = iou_threshold
        self.track_thresh = track_thresh
        self.track_high_thresh = track_high_thresh
        self.track_low_thresh = track_low_thresh
        self.tracks: List[Track] = []
        self.track_id = 0

    def update(self, detections: List[Detection]) -> List[Detection]:
        """Update tracks with new detections"""
        # Split detections by confidence
        high_conf_dets = [d for d in detections if d.bbox.confidence >= self.track_high_thresh]
        low_conf_dets = [d for d in detections if self.track_low_thresh <= d.bbox.confidence < self.track_high_thresh]

        # Get predictions from existing tracks
        track_boxes = []
        for track in self.tracks:
            track_boxes.append(track.predict())

        # Initialize unmatched tracks list
        unmatched_tracks_a = list(range(len(track_boxes)))  # Add this line

        # First association with high confidence detections
        if track_boxes and high_conf_dets:
            matches_a, unmatched_tracks_a, unmatched_dets_high = \
                self._match_detections_to_tracks(high_conf_dets, track_boxes)

            # Update matched tracks
            for track_idx, det_idx in matches_a:
                self.tracks[track_idx].update(high_conf_dets[det_idx])

            # Handle unmatched tracks
            for track_idx in unmatched_tracks_a:
                self.tracks[track_idx].time_since_update += 1
        else:
            unmatched_dets_high = list(range(len(high_conf_dets)))
            for track in self.tracks:
                track.time_since_update += 1

        # Second association with low confidence detections
        if unmatched_tracks_a and low_conf_dets:  # This will now always be properly defined
            unmatched_track_boxes = [track_boxes[i] for i in unmatched_tracks_a]
            matches_b, unmatched_tracks_b, unmatched_dets_low = \
                self._match_detections_to_tracks(low_conf_dets, unmatched_track_boxes)

            # Update matched tracks
            for local_track_idx, det_idx in matches_b:
                global_track_idx = unmatched_tracks_a[local_track_idx]
                self.tracks[global_track_idx].update(low_conf_dets[det_idx])

        # Initialize new tracks
        if high_conf_dets:  # Add this check
            for det_idx in unmatched_dets_high:
                self._initiate_track(high_conf_dets[det_idx])

        # Remove dead tracks
        self.tracks = [
            track for track in self.tracks
            if track.time_since_update < self.max_age
        ]

        # Return current tracked objects
        results = []
        for track in self.tracks:
            if track.time_since_update > 1 or track.hits < self.min_hits:
                continue

            bbox = track.predict()
            if bbox is None:
                continue

            results.append(Detection(
                bbox=bbox,
                track_id=track.track_id,
                keypoints=track.detection.keypoints
            ))

        return results

    def _match_detections_to_tracks(
        self,
        detections: List[Detection],
        track_boxes: List[BoundingBox]
    ) -> Tuple[List[Tuple[int, int]], List[int], List[int]]:
        """Match detections to tracks using IoU and Hungarian algorithm"""
        if not track_boxes or not detections:
            return [], list(range(len(track_boxes))), list(range(len(detections)))

        # Calculate IoU matrix
        iou_matrix = np.zeros((len(track_boxes), len(detections)))
        for t, track_box in enumerate(track_boxes):
            for d, detection in enumerate(detections):
                iou_matrix[t, d] = self._calculate_iou(track_box, detection.bbox)

        # Use Hungarian algorithm for optimal assignment
        track_indices, detection_indices = linear_sum_assignment(-iou_matrix)

        matches = []
        unmatched_tracks = list(range(len(track_boxes)))
        unmatched_detections = list(range(len(detections)))

        # Filter matches by IoU threshold
        for t, d in zip(track_indices, detection_indices):
            if iou_matrix[t, d] >= self.iou_threshold:
                matches.append((t, d))
                if t in unmatched_tracks:
                    unmatched_tracks.remove(t)
                if d in unmatched_detections:
                    unmatched_detections.remove(d)

        return matches, unmatched_tracks, unmatched_detections

    def _initiate_track(self, detection: Detection) -> None:
        """Create and initialize a new track"""
        if detection.bbox.confidence >= self.track_thresh:
            self.tracks.append(Track(detection, self.track_id))
            self.track_id += 1

    def _calculate_iou(self, box1: BoundingBox, box2: BoundingBox) -> float:
        """Calculate Intersection over Union between two boxes"""
        x1 = max(box1.x1, box2.x1)
        y1 = max(box1.y1, box2.y1)
        x2 = min(box1.x2, box2.x2)
        y2 = min(box1.y2, box2.y2)

        intersection = max(0, x2 - x1) * max(0, y2 - y1)
        box1_area = (box1.x2 - box1.x1) * (box1.y2 - box1.y1)
        box2_area = (box2.x2 - box2.x1) * (box2.y2 - box2.y1)
        union = box1_area + box2_area - intersection

        return intersection / union if union > 0 else 0
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\tracking\kalman_filter.py
--------------------------------------------------------------------------------
from typing import List, Tuple, Optional
import numpy as np
from scipy.optimize import linear_sum_assignment
from src.domain.entities.detection import BoundingBox, Detection
from src.application.interfaces.tracker import ITracker

class KalmanFilter:
    """Kalman Filter for bounding box tracking"""
    def __init__(self):
        # State: [x, y, w, h, dx, dy, dw, dh]
        self.state_dim = 8
        self.meas_dim = 4  # Measurement: [x, y, w, h]

        # State transition matrix
        self.F = np.eye(self.state_dim)
        self.F[:4, 4:] = np.eye(4)  # Add velocity components

        # Measurement matrix
        self.H = np.zeros((self.meas_dim, self.state_dim))
        self.H[:4, :4] = np.eye(4)

        # Process noise
        self.Q = np.eye(self.state_dim) * 0.1
        self.Q[4:, 4:] *= 4.0  # Larger noise for velocity components

        # Measurement noise
        self.R = np.eye(self.meas_dim) * 1.0

        # Error covariance
        self.P = np.eye(self.state_dim) * 10.0

        self.x = None

    def initialize(self, measurement: np.ndarray) -> None:
        """Initialize state with first measurement"""
        self.x = np.zeros(self.state_dim)
        self.x[:4] = measurement
        self.x[4:] = 0  # Initialize velocities to 0

    def predict(self) -> np.ndarray:
        """Predict next state"""
        if self.x is None:
            return None
            
        # Predict state
        self.x = self.F @ self.x
        self.P = self.F @ self.P @ self.F.T + self.Q
        
        return self.x[:4]  # Return predicted measurement

    def update(self, measurement: np.ndarray) -> np.ndarray:
        """Update state with measurement"""
        # Calculate Kalman gain
        S = self.H @ self.P @ self.H.T + self.R
        K = self.P @ self.H.T @ np.linalg.inv(S)

        # Update state
        y = measurement - (self.H @ self.x)
        self.x = self.x + (K @ y)
        self.P = (np.eye(self.state_dim) - K @ self.H) @ self.P

        return self.x[:4]
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\tracking\track.py
--------------------------------------------------------------------------------
from typing import Optional
import numpy as np
from src.domain.entities.detection import Detection, BoundingBox
from .kalman_filter import KalmanFilter

class Track:
    """Track class for managing single object track"""
    def __init__(self, detection: Detection, track_id: int):
        self.track_id = track_id
        self.kalman = KalmanFilter()
        
        # Initialize Kalman filter state
        bbox = detection.bbox
        measurement = np.array([
            bbox.x1,
            bbox.y1,
            bbox.x2 - bbox.x1,  # width
            bbox.y2 - bbox.y1   # height
        ])
        self.kalman.initialize(measurement)
        
        self.time_since_update = 0
        self.hits = 1
        self.detection = detection
        self.confidence = detection.bbox.confidence

    def predict(self) -> Optional[BoundingBox]:
        """Predict next state"""
        prediction = self.kalman.predict()
        if prediction is None:
            return None

        x, y, w, h = prediction
        return BoundingBox(
            x1=float(x),
            y1=float(y),
            x2=float(x + w),
            y2=float(y + h),
            confidence=self.confidence,
            class_id=self.detection.bbox.class_id
        )

    def update(self, detection: Detection) -> None:
        """Update track with detected box"""
        self.detection = detection
        self.confidence = detection.bbox.confidence
        
        bbox = detection.bbox
        measurement = np.array([
            bbox.x1,
            bbox.y1,
            bbox.x2 - bbox.x1,
            bbox.y2 - bbox.y1
        ])
        
        self.kalman.update(measurement)
        self.hits += 1
        self.time_since_update = 0
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\utils\logging_config.py
--------------------------------------------------------------------------------
import logging
import os
from datetime import datetime

def setup_logger(name, log_file=None):
    """
    Thiết lập logger với định dạng và handlers phù hợp.
    
    Args:
        name (str): Tên của logger
        log_file (str, optional): Đường dẫn file log. Nếu None, sẽ tạo file theo timestamp
    """
    if log_file is None:
        # Tạo thư mục logs nếu chưa tồn tại
        os.makedirs('logs', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = f'logs/event_radar_{timestamp}.log'

    # Tạo logger
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    # Tạo file handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.DEBUG)

    # Tạo console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    # Định dạng log
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    # Thêm handlers vào logger
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger 
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\utils\module_process.py
--------------------------------------------------------------------------------
import logging
import traceback
import cv2
import time
import numpy as np
from queue import Full, Empty

from src.infrastructure.utils.system_utils import restart_usb_devices
from src.infrastructure.ml_models.fall_detector import PeopleFall
from src.infrastructure.ml_models.onnx_detect import ONNXDetector
from src.domain.value_objects.config import RadarConfig
from src.domain.entities.entities import ProcessingParams
from src.core.filters import ActivityNoiseFilterGPU
from src.core.radar import ActivityMonitor, RadarViewer
from src.infrastructure.event_camera.iterators.bias_events_iterator import BiasEventsIterator
from src.infrastructure.event_camera.processors.event_processor import EventProcessor
from src.infrastructure.utils.utils import convert_events_data, draw_radar_background, make_binary_histo
import cupy as cp
import os
from multiprocessing.shared_memory import SharedMemory
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
os.environ['PYTHONWARNINGS'] = 'ignore'

MAX_BBOXES = 3
SENTINEL = -1.0

def get_bbox_shm():
    shm_name = "bbox_shm"
    size = MAX_BBOXES * 4 * 5
    try:
        shm = SharedMemory(name=shm_name)
    except FileNotFoundError:
        shm = SharedMemory(name=shm_name, create=True, size=size)
    return shm
     
def event_process(frame_queue, frame_queue_frame, event_queue , running, input_filename, camera_resolution, config_path):
    try:
        w, h = camera_resolution
        processor = EventProcessor(
            width=w,
            height=h,
            crop_coordinates=None
        )
        
        iterator = None if not input_filename else BiasEventsIterator(
            delta_t=33000,
            input_filename=input_filename,
            bias_file=config_path
        )
        
        zero_event_counter = 0
        max_zero_events = 10
        error_counter = 0
        max_errors = 3
        
        while running.is_set():
            try:
                if iterator is None and not input_filename:
                    try:
                        iterator = BiasEventsIterator(
                            delta_t=33000,
                            input_filename=input_filename,
                            bias_file=config_path
                        )
                        error_counter = 0  
                    except Exception as e:
                        print(f"Failed to create iterator: {e}")
                        error_counter += 1
                        if error_counter >= max_errors:
                            restart_usb_devices()
                            error_counter = 0
                        time.sleep(1)
                        continue
                
                for events in iterator:
                    
                    if not running.is_set():
                        break
                    
                    start_time = time.time()
                    
                    event_count = len(events)
                    # print(event_count)

                    if event_count == 0:
                        zero_event_counter += 1
                        if zero_event_counter >= max_zero_events:
                            print("Too many zero events, reloading USB...")
                            if iterator:
                                del iterator
                                iterator = None
                            if restart_usb_devices():
                                print("USB restarted successfully")
                            else:
                                print("Failed to restart USB")
                            zero_event_counter = 0
                            break
                    else:
                        zero_event_counter = 0 
                    if event_queue.full():
                        try:
                            event_queue.get_nowait()
                        except Empty:
                            pass
                    try:
                        event_queue.put((events, time.time()))
                    except Full:
                        pass
                    event_frame = processor.create_frame(events)
                    # cv2.imwrite("event_frame.png", event_frame.data)
                    end_time = time.time()
                    processing_time = round((end_time - start_time) * 1000, 2)
                    
                    
                    
                    if frame_queue.full():
                        try:
                            frame_queue.get_nowait()
                        except Empty:
                            pass

                    try:
                        frame_queue.put((event_frame, time.time(), processing_time))
                    except Full:
                        pass
                    

                    if frame_queue_frame.full():
                        try:
                            frame_queue_frame.get_nowait()
                        except Empty:
                            pass

                    try:
                        frame_queue_frame.put((event_frame, time.time(), processing_time))
                    except Full:
                        pass
                    
                if input_filename:  # File mode
                    print("File processing completed")
                    frame_queue.put(("EOF", time.time()))
                    break
                else:  # Camera mode
                    if iterator:
                        del iterator
                        iterator = None

            except Exception as e:
                print(f"Iterator error: {e}")
                if "TimeHigh discrepancy" in str(e):
                    print("TimeHigh discrepancy detected, reloading USB...")
                    if iterator:
                        del iterator
                        iterator = None
                    restart_usb_devices()
                    continue
                
                error_counter += 1
                if error_counter >= max_errors:
                    print(f"Too many errors ({error_counter}), restarting USB...")
                    restart_usb_devices()
                    error_counter = 0
                
                if iterator:
                    del iterator
                    iterator = None
                
                if not input_filename:  # Chỉ retry với camera mode
                    time.sleep(1)
                else:
                    break

    except Exception as e:
        print(traceback.print_exc())
        logging.error(f"Error in event process: {e}")
    finally:
        if iterator:
            del iterator
        running.clear()

def people_detection_process_onnx(frame_queue, people_queue, running, model_path, args: ProcessingParams):
    try:
        # Initialize detector
        detector = ONNXDetector(model_path, args)
        last_frame = None
        
        while running.is_set():
            try:
                # Get frame from queue
                event_frame, timestamp, _ = frame_queue.get(timeout=0.1)
                frame = event_frame.data
                last_frame = frame
            except Empty:
                if last_frame is None:
                    continue
                frame = last_frame
                timestamp = time.time()
            
            # Process frame
            start_time = time.time()
            
            # Run detection
            boxes, boxes_n, current_count = detector.detect_and_track(frame)
            
            # Create tracked frame
            tracked_frame = frame.copy()
            
            # Draw detections
            if current_count > 0:
                for box in boxes:
                    x1, y1, x2, y2 = box.astype(int)
                    cv2.rectangle(tracked_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(tracked_frame, "Person", (x1, y1 - 10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            # Calculate processing time
            end_time = time.time()
            processing_time = round((end_time - start_time) * 1000, 2)
            
            # cv2.imwrite("tracked_frame.png", tracked_frame)
            
            # Handle queue operations
            if people_queue.full():
                try:
                    people_queue.get_nowait()
                except Empty:
                    pass
            
            try:
                people_queue.put((tracked_frame, boxes_n, current_count, timestamp, processing_time))
            except Full:
                people_queue.put((frame, [], 0, timestamp, 0))
                continue
                
    except Exception as e:
        traceback.print_exc()
        logging.error(f"Error in people detection process: {e}")
    finally:
        running.clear()
    
def person_direction_process(event_queue, radar_queue, running, args: ProcessingParams):
    bbox_shm = get_bbox_shm()
    boxes_shared = np.ndarray((MAX_BBOXES, 4), dtype=np.float32, buffer=bbox_shm.buf)
    last_valid_angles = []
    no_movement_counter = 0
    last_frame_time = time.time()
    last_peak_time = time.time()
    last_bbox = None
    last_bbox_time = 0
    frame_width = args.camera_width
    frame_height = args.camera_height
    radar_height = int(frame_height * 0.95)
    radar_width = int(frame_width * 1.47)
    config = RadarConfig()
    lateral_fov_deg = np.rad2deg(config.lateral_fov)
    pixel_margin = int(config.trim_angle_deg / (lateral_fov_deg / frame_width))
    activity_monitor = ActivityMonitor(config, frame_width)
    radar_viewer = RadarViewer(config, radar_width, radar_height)
    if config.use_filter:
        activity_filter = ActivityNoiseFilterGPU(
            width=frame_width,
            height=frame_height,
            threshold=config.delta_t,
            neighborhood_size=args.neighborhood_size,
            time_tolerance=args.time_tolerance
        )
    else:
        activity_filter = None

    def update_radar_visualization(ev_rate_gpu: cp.ndarray) -> np.ndarray:
        nonlocal last_frame_time, last_valid_angles, last_peak_time
        radar_img = np.copy(radar_viewer.linear_bins_cpu)
        draw_radar_background(radar_img, radar_width, radar_height, config.lateral_fov)
        frame_delta = time.time() - last_frame_time
        last_frame_time = time.perf_counter()
        if no_movement_counter > 10 and last_valid_angles:
            angles_to_show = [float(a + v) for a, v in zip(last_valid_angles, radar_viewer.angle_velocities)]
        else:
            angles_found = radar_viewer.compute_view(ev_rate_gpu, radar_img, frame_delta)
            angles_to_show = radar_viewer.stabilize_angles(angles_found)
            if angles_to_show:
                last_valid_angles = angles_to_show.copy()
                last_peak_time = time.time()  
        if (not angles_to_show or len(angles_to_show) == 0) and (time.time() - last_peak_time > 1):
            radar_viewer.tracked_peaks.clear()
            last_valid_angles = []
            draw_radar_background(radar_img, radar_width, radar_height, config.lateral_fov)
        
        return radar_img

    try:
        last_events = None
        while running.is_set():
            try:
                events_data, timestamp = event_queue.get(timeout=0.1)
                events = convert_events_data(events_data)
                event_image = make_binary_histo(events, width=args.camera_width, height=args.camera_height)
                # cv2.imwrite("filter.png", event_image)
            except Empty:
                if last_events is None:
                    continue
                events = last_events
                timestamp = time.time()
            last_events = events
            start_time = time.time()
            if activity_filter:
                processed_events = activity_filter.process_events_gpu(events)
                if processed_events is None or processed_events["x"].size == 0:
                    continue
                x_gpu = processed_events['x']
                y_gpu = processed_events['y']
            else:
                x_gpu = cp.asarray(events['x'], dtype=cp.int32)
                y_gpu = cp.asarray(events['y'], dtype=cp.int32)
            if pixel_margin > 0:
                mask = (x_gpu >= pixel_margin) & (x_gpu < (frame_width - pixel_margin))
                x_filtered_gpu = x_gpu[mask]
                y_filtered_gpu = y_gpu[mask]
            else:
                x_filtered_gpu = x_gpu
                y_filtered_gpu = y_gpu
            current_time = time.time()
            valid_boxes = [box for box in boxes_shared if not np.allclose(box, [SENTINEL]*4)]
            count = len(valid_boxes)
            if valid_boxes:
                last_bbox = np.array(valid_boxes)
                last_bbox_time = current_time
                radar_viewer.update_max_objects(max(2,count))
            else:
                if last_bbox is not None and (current_time - last_bbox_time) < 1:
                    pass
                else:
                    boxes_shared[:] = SENTINEL
                    last_bbox = None
                    x_filtered_gpu = cp.asarray([], dtype=cp.int32)
                    y_filtered_gpu = cp.asarray([], dtype=cp.int32)
                    

            activity_monitor.process_events(x_filtered_gpu)
            activity_monitor.reset()
            ev_rate_gpu = activity_monitor.get_ev_rate_per_bin(return_to_cpu=False)
            total_events = activity_monitor.get_total_event_count()
            if total_events < config.low_event_threshold:
                no_movement_counter += 1
                if no_movement_counter > 5:
                    last_valid_angles = []
                    radar_viewer.angle_velocities = {}
            else:
                no_movement_counter = 0
            radar_img = update_radar_visualization(ev_rate_gpu)
            proc_time = round((time.time() - start_time) * 1000, 2)
            if radar_queue.full():
                try:
                    radar_queue.get_nowait()
                except Empty:
                    pass
            try:
                print(proc_time)
                radar_queue.put((radar_img, timestamp, proc_time))
            except Full:
                continue
    except Exception as e:
        traceback.print_exc()
        logging.error(f"Error in person direction process: {e}")
    finally:
        running.clear()

def fall_detection_process(frame_queue, fall_queue, running, fall_model_path):
    try:
        fall_detector = PeopleFall(fall_model_path)
        last_frame = None
        
        while running.is_set():
            try:
                event_frame, timestamp, _ = frame_queue.get(timeout=0.1)
                frame = event_frame.data
                h, w, _ = frame.shape
                if h != 320 or w != 320:
                    frame = cv2.resize(frame, (320, 320))
                    
                last_frame = frame
            except Empty:
                if last_frame is None:
                    continue
                frame = last_frame
                timestamp = time.time()

            start_time = time.time()
            # Run fall detection
            fall_status = fall_detector.process_frame(frame)
            end_time = time.time()
            processing_time = round((end_time - start_time) * 1000, 2)
            if fall_queue.full():
                try:
                    fall_queue.get_nowait()
                except Empty:
                    pass

            try:
                fall_queue.put((fall_status, timestamp, processing_time))
            except Full:
                continue

    except Exception as e:
        traceback.print_exc()
        logging.error(f"Error in fall detection process: {e}")
    finally:
        running.clear()
        
def visualization_process(people_queue, fall_queue, radar_queue, result_queue, running, frame_height, args: ProcessingParams):
    TARGET_HEIGHT = frame_height
    fall_display_counter = 0

    last_stats_update = time.time()
    current_fps = 0
    current_people_time = 0
    current_radar_time = 0 
    current_fall_time = 0
    update_interval = 1.0  
    
    try:
        while running.is_set():
            try:
                # Get data from queues
                frame, boxes, count, p_timestamp, people_time = people_queue.get(timeout=0.01)
                frame_radar, r_timestamp, radar_time = radar_queue.get(timeout=0.01)
                try:
                    fall_status, f_timestamp, fall_time = fall_queue.get_nowait()
                except Empty:
                    fall_status = "Normal"
                    fall_time = 0

                # 1. Process event image (left side)
                event_target_height = TARGET_HEIGHT
                event_target_width = TARGET_HEIGHT  # Keep it square as original
                
                event_image_resized = cv2.resize(frame, 
                                            (event_target_width, event_target_height),
                                            interpolation=cv2.INTER_AREA)
                
                # 2. Process radar image
                radar_target_height = int(TARGET_HEIGHT * 2/3)
                radar_aspect_ratio = frame_radar.shape[1] / frame_radar.shape[0]
                radar_target_width = int(radar_target_height * radar_aspect_ratio)
                
                if radar_target_width > event_target_width:
                    radar_target_width = event_target_width
                    radar_target_height = int(radar_target_width / radar_aspect_ratio)
                
                radar_image_resized = cv2.resize(frame_radar,
                    (radar_target_width, radar_target_height),
                    interpolation=cv2.INTER_AREA)
                
                # 3. Create right side canvas
                right_side = np.zeros((TARGET_HEIGHT, event_target_width, 3), dtype=np.uint8)
    
                # Draw radar on top
                x_offset = (event_target_width - radar_target_width) // 2
                y_offset = 0
                right_side[y_offset:y_offset+radar_target_height, 
                        x_offset:x_offset+radar_target_width] = radar_image_resized

                # Stats area
                stats_height = TARGET_HEIGHT - radar_target_height
                stats_y = radar_target_height
                
                # Split stats area into two parts
                stats_width = event_target_width
                stats_mid = stats_width // 2
                
                # Fill background
                right_side[stats_y:, :] = [255, 255, 255]
                
                # Calculate FPS
                current_time = time.time()
                if current_time - last_stats_update >= update_interval:
                    max_time = max(people_time, 0, fall_time)
                    if max_time > 0:
                        current_fps = 1000 / max_time
                    else:
                        current_fps = 0
                    
                    current_people_time = people_time
                    current_radar_time = 0
                    current_fall_time = fall_time
                    
                    last_stats_update = current_time

                # Setup font params based on TARGET_HEIGHT
                font = cv2.FONT_HERSHEY_SIMPLEX
                base_font_scale = TARGET_HEIGHT / 720.0  # Scale based on 720p
                text_color = (0, 0, 0)
                line_gap = int(40 * base_font_scale)
                margin = int(20 * base_font_scale)
                base_thickness = max(1, int(2 * base_font_scale))

                # Left side stats
                stats_x = margin
                text_y = stats_y + int(40 * base_font_scale)

                # Status with color
                
                print("count fall: ", fall_display_counter)
                if fall_status == "FALL":
                    fall_display_counter += 1

                if fall_display_counter > 3:
                    fall_text = "Status: Fall Detected!"
                    fall_color = (0, 0, 255)
                    cv2.putText(right_side, fall_text,
                              (margin, text_y), font,
                              0.8 * base_font_scale, fall_color, base_thickness + 1, cv2.LINE_AA)
                    fall_display_counter += 1
                    if fall_display_counter > 20:
                        fall_display_counter = 0
                else:
                    fall_text = "Status: Normal"
                    fall_color = (0, 255, 0)
                    cv2.putText(right_side, fall_text,
                              (margin, text_y), font,
                              0.8 * base_font_scale, fall_color, base_thickness + 1, cv2.LINE_AA)

                # People count
                text_y += line_gap
                cv2.putText(right_side, f"People: {count}",
                           (stats_x, text_y), font,
                           1.2 * base_font_scale, text_color, base_thickness + 1, cv2.LINE_AA)

                # FPS
                if hasattr(args, 'show_fps') and args.show_fps:
                    text_y += line_gap
                    cv2.putText(right_side, f"FPS: {current_fps:.1f}",
                              (stats_x, text_y), font,
                              0.8 * base_font_scale, text_color, base_thickness , cv2.LINE_AA)

                # Right side timing stats
                if hasattr(args, 'show_timing') and args.show_timing:
                    # Draw vertical divider
                    divider_thickness = max(1, int(2 * base_font_scale))
                    cv2.line(right_side,
                            (stats_mid, stats_y),
                            (stats_mid, TARGET_HEIGHT),
                            (200, 200, 200), divider_thickness)

                    timing_x = stats_mid + margin
                    text_y = stats_y + int(40 * base_font_scale)

                    # Title
                    cv2.putText(right_side, "Processing Time (ms):",
                              (timing_x, text_y), font,
                              0.7 * base_font_scale, text_color, base_thickness, cv2.LINE_AA)

                    # Timing details
                    text_y += line_gap
                    detail_scale = 0.7 * base_font_scale
                    cv2.putText(right_side, f"Detection: {current_people_time:.1f}",
                              (timing_x, text_y), font,
                              detail_scale, text_color, base_thickness, cv2.LINE_AA)

                    text_y += line_gap
                    cv2.putText(right_side, f"Radar: {current_radar_time:.1f}",
                              (timing_x, text_y), font,
                              detail_scale, text_color, base_thickness, cv2.LINE_AA)

                    text_y += line_gap
                    cv2.putText(right_side, f"Fall: {current_fall_time:.1f}",
                              (timing_x, text_y), font,
                              detail_scale, text_color, base_thickness, cv2.LINE_AA)

                # Combine images
                combined_image = np.hstack((event_image_resized, right_side))

                # Handle queue
                if result_queue.full():
                    try:
                        result_queue.get_nowait()
                    except Empty:
                        pass

                try:
                    result_queue.put((combined_image, frame, count, fall_status, time.time()))
                except Full:
                    continue

            except Empty:
                continue

    except Exception as e:
        logging.error(f"Error in visualization process: {e}")
    finally:
        running.clear()

def all_one_thread(result_queue, input_filename, camera_resolution, model_path, args, frame_height, config_path, fall_model_path):
    try:
        bbox_shm = get_bbox_shm()
        boxes_shared = np.ndarray((MAX_BBOXES, 4), dtype=np.float32, buffer=bbox_shm.buf)
        detector = ONNXDetector(model_path, args)
        fall_detector = PeopleFall(fall_model_path)
        TARGET_HEIGHT = frame_height
        fall_display_counter = 0
        fall_status = "Normal"
        
        last_valid_angles = []
        no_movement_counter = 0
        last_frame_time = time.time()
        current_bbox = []
        last_peak_time = time.time()
        frame_width = args.camera_width
        frame_height = args.camera_height
        radar_height = int(frame_height * 0.95)
        radar_width = int(frame_width * 1.47)
        
        config = RadarConfig()
        lateral_fov_deg = np.rad2deg(config.lateral_fov)
        pixel_margin = int(config.trim_angle_deg / (lateral_fov_deg / frame_width))
        activity_monitor = ActivityMonitor(config, frame_width)
        radar_viewer = RadarViewer(config, radar_width, radar_height)
        
        if config.use_filter:
            activity_filter = ActivityNoiseFilterGPU(
                width=frame_width,
                height=frame_height,
                threshold=config.delta_t,
                neighborhood_size=args.neighborhood_size,
                time_tolerance=args.time_tolerance
            )
        else:
            activity_filter = None
            
        def update_radar_visualization(ev_rate_gpu: cp.ndarray) -> np.ndarray:
            nonlocal last_frame_time, last_valid_angles, last_peak_time
            radar_img = np.copy(radar_viewer.linear_bins_cpu)
            draw_radar_background(radar_img, radar_width, radar_height, config.lateral_fov)
            frame_delta = time.time() - last_frame_time
            last_frame_time = time.perf_counter()
            if no_movement_counter > 10 and last_valid_angles:
                angles_to_show = [float(a + v) for a, v in zip(last_valid_angles, radar_viewer.angle_velocities)]
            else:
                angles_found = radar_viewer.compute_view(ev_rate_gpu, radar_img, frame_delta)
                angles_to_show = radar_viewer.stabilize_angles(angles_found)
                if angles_to_show:
                    last_valid_angles = angles_to_show.copy()
                    last_peak_time = time.time()  
            if (not angles_to_show or len(angles_to_show) == 0) and (time.time() - last_peak_time > 1):
                radar_viewer.tracked_peaks.clear()
                last_valid_angles = []
                draw_radar_background(radar_img, radar_width, radar_height, config.lateral_fov)
            
            return radar_img
        
        w, h = camera_resolution
        processor = EventProcessor(
            width=w,
            height=h,
            crop_coordinates=None
        )
        
        iterator = BiasEventsIterator(
            delta_t=33000,
            input_filename=input_filename,
            bias_file=config_path
        )
        
        frame_count = 0
        total_time = 0
        for events in iterator:
            start_time = time.time()
            
            event_frame = processor.create_frame(events)
            frame = event_frame.data
            
            boxes, boxes_n, current_count = detector.detect_and_track(frame)
            
            tracked_frame = frame.copy()
            if current_count > 0:
                for box in boxes:
                    x1, y1, x2, y2 = box.astype(int)
                    cv2.rectangle(tracked_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(tracked_frame, "Person", (x1, y1 - 10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                    
            events_filtered = convert_events_data(events.copy())
            
                
            if activity_filter:
                processed_events = activity_filter.process_events_gpu(events_filtered)
                if processed_events is None or processed_events["x"].size == 0:
                    x_gpu = cp.array([], dtype=cp.int32)
                    y_gpu = cp.array([], dtype=cp.int32)
                else:
                    x_gpu = processed_events['x']
                    y_gpu = processed_events['y']
            else:
                x_gpu = cp.asarray(events_filtered['x'], dtype=cp.int32)
                y_gpu = cp.asarray(events_filtered['y'], dtype=cp.int32)
                
            if pixel_margin > 0 and x_gpu.size > 0:
                mask = (x_gpu >= pixel_margin) & (x_gpu < (frame_width - pixel_margin))
                x_filtered_gpu = x_gpu[mask]
                y_filtered_gpu = y_gpu[mask]
            else:
                x_filtered_gpu = x_gpu
                y_filtered_gpu = y_gpu
                
            current_time = time.time()
            valid_boxes = [box for box in boxes_shared if not np.allclose(box, [SENTINEL]*4)]
            count = len(valid_boxes)
            if valid_boxes:
                last_bbox = np.array(valid_boxes)
                last_bbox_time = current_time
                radar_viewer.update_max_objects(max(2,count))
            else:
                if last_bbox is not None and (current_time - last_bbox_time) < 1:
                    pass
                else:
                    boxes_shared[:] = SENTINEL
                    last_bbox = None
                    x_filtered_gpu = cp.asarray([], dtype=cp.int32)
                    y_filtered_gpu = cp.asarray([], dtype=cp.int32)

                    
            activity_monitor.process_events(x_filtered_gpu)
            activity_monitor.reset()
            ev_rate_gpu = activity_monitor.get_ev_rate_per_bin(return_to_cpu=False)
            total_events = activity_monitor.get_total_event_count()
            
            if total_events < config.low_event_threshold:
                no_movement_counter += 1
                if no_movement_counter > 5:
                    last_valid_angles = []
                    radar_viewer.angle_velocities = {}
            else:
                no_movement_counter = 0
                
            radar_img = update_radar_visualization(ev_rate_gpu)
            
            frame_fall = frame.copy()
            h, w, _ = frame_fall.shape
            if h != 320 or w != 320:
                frame_fall = cv2.resize(frame_fall, (320, 320))
            
            fall_status = fall_detector.process_frame(frame_fall)
            
            event_target_height = TARGET_HEIGHT
            event_target_width = TARGET_HEIGHT
            
            event_image_resized = cv2.resize(tracked_frame, 
                                        (event_target_width, event_target_height),
                                        interpolation=cv2.INTER_AREA)
            
            radar_target_height = int(TARGET_HEIGHT * 2/3)
            radar_aspect_ratio = radar_img.shape[1] / radar_img.shape[0]
            radar_target_width = int(radar_target_height * radar_aspect_ratio)
            
            if radar_target_width > event_target_width:
                radar_target_width = event_target_width
                radar_target_height = int(radar_target_width / radar_aspect_ratio)
            
            radar_image_resized = cv2.resize(radar_img,
                (radar_target_width, radar_target_height),
                interpolation=cv2.INTER_AREA)
            
            right_side = np.zeros((TARGET_HEIGHT, event_target_width, 3), dtype=np.uint8)
            
            x_offset = (event_target_width - radar_target_width) // 2
            y_offset = 0
            right_side[y_offset:y_offset+radar_target_height, 
                    x_offset:x_offset+radar_target_width] = radar_image_resized
            
            stats_height = TARGET_HEIGHT - radar_target_height
            stats_y = radar_target_height
            stats_width = event_target_width
            stats_mid = stats_width // 2
            
            right_side[stats_y:, :] = [255, 255, 255]
            
            font = cv2.FONT_HERSHEY_SIMPLEX
            base_font_scale = TARGET_HEIGHT / 720.0
            text_color = (0, 0, 0)
            line_gap = int(40 * base_font_scale)
            margin = int(20 * base_font_scale)
            base_thickness = max(1, int(2 * base_font_scale))
            
            stats_x = margin
            text_y = stats_y + int(40 * base_font_scale)
            
            print("count fall: ", fall_display_counter)
            
            if fall_status == "FALL":
                fall_display_counter += 1
                
            if fall_display_counter > 3:
                fall_text = "Status: Fall Detected!"
                fall_color = (0, 0, 255)
                cv2.putText(right_side, fall_text,
                            (margin, text_y), font,
                            0.8 * base_font_scale, fall_color, base_thickness + 1, cv2.LINE_AA)
                fall_display_counter += 1
                if fall_display_counter > 20:
                    fall_display_counter = 0
            else:
                fall_text = "Status: Normal"
                fall_color = (0, 255, 0)
                cv2.putText(right_side, fall_text,
                            (margin, text_y), font,
                            0.8 * base_font_scale, fall_color, base_thickness + 1, cv2.LINE_AA)
                            
            text_y += line_gap
            cv2.putText(right_side, f"People: {current_count}",
                        (stats_x, text_y), font,
                        1.2 * base_font_scale, text_color, base_thickness + 1, cv2.LINE_AA)
            
            combined_image = np.hstack((event_image_resized, right_side))
            
            end_time = time.time()
            processing_time = round((end_time - start_time) * 1000, 2)
            
            frame_count += 1
            total_time += processing_time
            avg_time = total_time / frame_count
            
            if hasattr(args, 'show_fps') and args.show_fps:
                fps = 1000 / avg_time if avg_time > 0 else 0
                text_y += line_gap
                cv2.putText(combined_image, f"FPS: {fps:.1f}",
                          (stats_x, text_y), font,
                          0.8 * base_font_scale, text_color, base_thickness, cv2.LINE_AA)
                
                if frame_count % 100 == 0:
                    print(f"Frame: {frame_count}, Avg Processing Time: {avg_time:.1f}ms, FPS: {fps:.1f}")
            
            while True:
                try:
                    result_queue.put((combined_image, frame, count, fall_status, time.time()), block=True, timeout=0.1)
                    break
                except Full:
                    time.sleep(0.01) 
        
        print(f"File processing completed. Processed {frame_count} frames")
        print(f"Average processing time: {total_time/frame_count:.2f}ms")
        
        result_queue.put(("EOF", None, 0, "Normal", time.time()))
        
    except Exception as e:
        print(traceback.print_exc())
        logging.error(f"Error in event process: {e}")
    finally:
        if 'iterator' in locals() and iterator:
            del iterator
   
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\utils\system_utils.py
--------------------------------------------------------------------------------
import multiprocessing
import sys
import time


def restart_usb_devices():
    try:
        import subprocess
        print("Starting Setup ...")
        subprocess.run(['pnputil', '/restart-device', 'USB\\VID*'], 
                      capture_output=True, 
                      text=True, 
                      creationflags=subprocess.CREATE_NO_WINDOW)
        time.sleep(2)  
        return True
    except Exception as e:
        print(f"Failed to restart USB: {e}")
        return False

def signal_handler(signum, frame):
    print(f"\nReceived signal {signum}")
    try:
        current_process = multiprocessing.current_process()
        for child in current_process.children():
            child.terminate()
        
        for child in current_process.children():
            child.join(timeout=3)
    except Exception as e:
        print(f"Error in signal handler: {e}")
    
    sys.exit(0)

def cleanup_old_processes():
    import psutil
    current_process = psutil.Process()
    process_name = current_process.name()
    
    for proc in psutil.process_iter(['pid', 'name']):
        try:
            if proc.info['name'] == process_name and proc.pid != current_process.pid:
                psutil.Process(proc.pid).terminate()
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\utils\utils.py
--------------------------------------------------------------------------------
# utils.py
import cupy as cp
import numpy as np
import cv2
import time
from typing import Tuple

from src.core.filters import BoundingBox

def convert_tracker_boxes(events, normalized_boxes, frame_width: int, frame_height: int, expansion_ratio: float = -0.2, max_threads: int = 3):
    bb_list = [BoundingBox(det[0], det[1], det[2], det[3], frame_width, frame_height, expansion_ratio)
               for det in normalized_boxes]
    x = events['x']
    y = events['y'].astype(np.int32)
    print("Bbox length:", len(normalized_boxes))
    masks = [((x >= bb.x_min) & (x <= bb.x_max) & (y >= bb.y_min) & (y <= bb.y_max))
             for bb in bb_list[:max_threads]]
    combined_mask = np.logical_or.reduce(masks)
    x_filtered = x[combined_mask]
    y_filtered = y[combined_mask]
    filtered_size = len(x_filtered)
    valid_indices = (x >= 0) & (x < frame_width) & (y >= 0) & (y < frame_height)
    filtered_events = np.empty(filtered_size, dtype=events.dtype)
    filtered_events['t'] = events['t'][valid_indices][:filtered_size]
    filtered_events['p'] = events['p'][valid_indices][:filtered_size]
    filtered_events['x'] = x_filtered
    filtered_events['y'] = y_filtered
    return filtered_events

def convert_events_data(events: np.ndarray,):
    if events.dtype != [('x', '<u2'), ('y', '<u2'), ('p', '<i2'), ('t', '<i8')]:
            events = np.array(events, 
                            dtype=[('x', '<u2'), ('y', '<u2'), 
                                  ('p', '<i2'), ('t', '<i8')])
    return events


def resize_with_padding(image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
    target_width, target_height = target_size
    original_height, original_width = image.shape[:2]
    scale = min(target_width / original_width, target_height / original_height)
    new_width = int(original_width * scale)
    new_height = int(original_height * scale)
    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
    padded_image = np.zeros((target_height, target_width, 3), dtype=np.uint8)
    x_offset = (target_width - new_width) // 2
    y_offset = (target_height - new_height) // 2
    padded_image[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = resized_image
    return padded_image

def pad_frames(event_frame: np.ndarray, radar_frame: np.ndarray, target_height: int) -> Tuple[np.ndarray, np.ndarray]:
    def pad_frame(frame: np.ndarray, target_h: int) -> np.ndarray:
        h, w = frame.shape[:2]
        if h < target_h:
            pad_bottom = target_h - h
            return cv2.copyMakeBorder(frame, 0, pad_bottom, 0, 0,
                                      cv2.BORDER_CONSTANT, value=(0,0,0))
        return frame
    padded_event = pad_frame(event_frame, target_height)
    padded_radar = pad_frame(radar_frame, target_height)
    return padded_event, padded_radar

def filter_noise_gpu(events_gpu: cp.ndarray, threshold: float = 0.1) -> cp.ndarray:
    if events_gpu.size < 2:
        return events_gpu
    temporal_mask = cp.concatenate(([False], events_gpu['t'][1:] - events_gpu['t'][:-1] > threshold))
    spatial_density = cp.histogramdd(
        cp.column_stack((events_gpu['x'], events_gpu['y'])),
        bins=[32, 32]
    )[0]
    density_mask = spatial_density[events_gpu['y'], events_gpu['x']] > cp.mean(spatial_density)
    return events_gpu[temporal_mask & density_mask]

def process_events_batch(events: np.ndarray, batch_size: int = 1000000) -> np.ndarray:
    total_events = len(events)
    processed_events = []
    for i in range(0, total_events, batch_size):
        batch = events[i:min(i + batch_size, total_events)]
        batch_gpu = cp.asarray(batch)
        filtered_batch = filter_noise_gpu(batch_gpu)
        processed_events.extend(cp.asnumpy(filtered_batch))
    return np.array(processed_events, dtype=events.dtype)

class FPSCounter:
    def __init__(self, avg_window: int = 30):
        self.avg_window = avg_window
        self.frame_times = []
        self.last_time = time.time()
        self.fps = 0.0

    def update(self):
        current_time = time.time()
        elapsed = current_time - self.last_time
        self.last_time = current_time
        self.frame_times.append(elapsed)
        if len(self.frame_times) > self.avg_window:
            self.frame_times.pop(0)
        total_time = sum(self.frame_times)
        self.fps = len(self.frame_times) / total_time if total_time > 0 else 0.0

    def get_fps(self) -> float:
        return self.fps

def draw_radar_background(radar_image_cpu: np.ndarray, width: int, height: int, lateral_fov: float) -> None:
    radar_image_cpu.fill(0)
    center = (width // 2, height)
    max_radius = height
    num_circles = 5
    angles = np.linspace(-lateral_fov / 2, lateral_fov / 2, num=7)
    for angle_rad in angles:
        end_x = int(center[0] + max_radius * np.sin(angle_rad))
        end_y = int(center[1] - max_radius * np.cos(angle_rad))
        cv2.line(radar_image_cpu, center, (end_x, end_y), 
                (0, 255, 0), 1, cv2.LINE_AA)
        if angle_rad != 0:
            angle_deg = int(np.rad2deg(angle_rad + lateral_fov / 2) + 0.5)
            label_x = int(center[0] + (max_radius - 30) * np.sin(angle_rad))
            label_y = int(center[1] - (max_radius - 30) * np.cos(angle_rad))
            cv2.putText(radar_image_cpu, f"{angle_deg}",
                      (label_x, label_y),
                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
    for i in range(num_circles):
        radius = int(max_radius * (i + 1) / num_circles)
        for j in range(len(angles) - 1):
            start_angle = angles[j]
            end_angle = angles[j + 1]
            arc_angles = np.linspace(start_angle, end_angle, num=50)
            points = np.array([[int(center[0] + radius * np.sin(a)), 
                              int(center[1] - radius * np.cos(a))] 
                             for a in arc_angles], dtype=np.int32)
            if len(points) > 1:
                for k in range(len(points) - 1):
                    cv2.line(radar_image_cpu, 
                            tuple(points[k]), 
                            tuple(points[k + 1]), 
                            (0, 255, 0), 1, cv2.LINE_AA)
        # distance = int((i + 1) * 100 / num_circles)
        # cv2.putText(radar_image_cpu, f"{distance}cm", 
        #            (center[0] + 10, center[1] - radius),
        #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

def interpolate_bin(histo_cpu: np.ndarray, peak_idx: int) -> float:
    if peak_idx <= 0 or peak_idx >= len(histo_cpu) -1:
        return float(peak_idx)
    left = histo_cpu[peak_idx -1]
    center = histo_cpu[peak_idx]
    right = histo_cpu[peak_idx +1]
    denominator = (left - 2 * center + right)
    if denominator == 0:
        return float(peak_idx)
    delta = 0.5 * (left - right) / denominator
    return float(peak_idx) + delta
def make_binary_histo(events, img=None, width=304, height=240):
    """
    simple display function that shows negative events as blacks dots and positive as white one
    on a gray background
    args :
        - events structured numpy array
        - img (numpy array, height x width x 3) optional array to paint event on.
        - width int
        - height int
    return:
        - img numpy array, height x width x 3)
    """
    if img is None:
        img = 127 * np.ones((height, width, 3), dtype=np.uint8)
    else:
        # if an array was already allocated just paint it grey
        img[...] = 127
    if events.size:
        assert events['x'].max() < width, "out of bound events: x = {}, w = {}".format(events['x'].max(), width)
        assert events['y'].max() < height, "out of bound events: y = {}, h = {}".format(events['y'].max(), height)

        img[events['y'], events['x'], :] = 255 * events['p'][:, None]
    return img


def process_events_batch(events, activity_filter, use_filter):
    if use_filter and activity_filter is not None:
        processed_events = activity_filter.process_events_gpu(events)
        if processed_events is None or processed_events.size == 0:
            return None
        return processed_events
    return events

def process_gpu_operations(x_gpu, y_gpu, frame_width, frame_height, pixel_margin):
    combined_mask = (
        (x_gpu >= pixel_margin) &
        (x_gpu < frame_width - pixel_margin) &
        (y_gpu < frame_height)
    )
    return x_gpu[combined_mask], y_gpu[combined_mask]

def create_event_image_gpu(x_gpu, y_gpu, frame_height, frame_width):
    event_image_gpu = cp.zeros((frame_height, frame_width, 3), dtype=cp.uint8)
    valid_indices = (y_gpu < frame_height) & (x_gpu < frame_width)
    y_valid = y_gpu[valid_indices]
    x_valid = x_gpu[valid_indices]
    if y_valid.size > 0:
        event_image_gpu[y_valid, x_valid] = 255
    return event_image_gpu

def resize_with_padding(image, target_size):
    """
    Resize the image to target size while keeping the aspect ratio and padding with black.
    """
    h, w = image.shape[:2]
    scale = min(target_size[1] / w, target_size[0] / h)
    new_w = int(w * scale)
    new_h = int(h * scale)
    resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
    
    delta_w = target_size[1] - new_w
    delta_h = target_size[0] - new_h
    top, bottom = delta_h // 2, delta_h - (delta_h // 2)
    left, right = delta_w // 2, delta_w - (delta_w // 2)
    
    padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))
    return padded_image

def resize_image(image, target_size):
    """
    Resize the image to target size while keeping the aspect ratio.
    No padding is applied.
    """
    h, w = image.shape[:2]
    scale = min(target_size[1] / w, target_size[0] / h)
    new_w = int(w * scale)
    new_h = int(h * scale)
    resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
    return resized_image

def process_visualization(event_image_bgr, radar_image_cpu, frame_width, combined_height, target_height):
    """
    Create a composite image with configurable output height
    """
    start_time = time.time()
    
    # Use configurable target height
    TARGET_HEIGHT = target_height
    
    # Record timing for each operation
    timings = {}
    
    # 1. Process event image (left side)
    t_start = time.time()
    event_target_height = TARGET_HEIGHT
    event_target_width = TARGET_HEIGHT  # Keep it square as original
    
    event_image_resized = cv2.resize(event_image_bgr, 
                                   (event_target_width, event_target_height),
                                   interpolation=cv2.INTER_AREA)
    timings['event_resize'] = (time.time() - t_start) * 1000
    
    # 2. Process radar image
    t_start = time.time()
    radar_target_height = int(TARGET_HEIGHT * 2/3)
    radar_aspect_ratio = radar_image_cpu.shape[1] / radar_image_cpu.shape[0]
    radar_target_width = int(radar_target_height * radar_aspect_ratio)
    
    if radar_target_width > event_target_width:
        radar_target_width = event_target_width
        radar_target_height = int(radar_target_width / radar_aspect_ratio)
    
    radar_image_resized = cv2.resize(radar_image_cpu,
                                   (radar_target_width, radar_target_height),
                                   interpolation=cv2.INTER_AREA)
    timings['radar_resize'] = (time.time() - t_start) * 1000
    
    # 3. Create right side canvas
    t_start = time.time()
    right_side = np.zeros((TARGET_HEIGHT, event_target_width, 3), dtype=np.uint8)
    
    x_offset = (event_target_width - radar_target_width) // 2
    y_offset = 0
    
    right_side[y_offset:y_offset+radar_target_height, 
              x_offset:x_offset+radar_target_width] = radar_image_resized
    
    right_side[radar_target_height:, :] = [255, 255, 255]
    timings['compose_right'] = (time.time() - t_start) * 1000
    
    # 4. Combine images
    t_start = time.time()
    combined_image = np.hstack((event_image_resized, right_side))
    timings['combine'] = (time.time() - t_start) * 1000
    
    total_time = time.time() - start_time
    timings['total'] = total_time * 1000
    
    return combined_image, timings

def convert_events_dtype(events):
    converted_events = np.empty(len(events), dtype=[
        ('x', '<u2'),
        ('y', '<u2'),
        ('p', 'u1'),
        ('t', '<u4')
    ])
    if 't' in events.dtype.names:
        converted_events['t'] = events['t']
    else:
        converted_events['t'] = events['timestamp']
    converted_events['x'] = events['x']
    converted_events['y'] = events['y']
    if 'p' in events.dtype.names:
        converted_events['p'] = events['p']
    else:
        converted_events['p'] = events['polarity']
    
    return converted_events

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\infrastructure\visualization\display.py
--------------------------------------------------------------------------------
import cv2
import numpy as np
from typing import List, Optional, Dict, Union
from src.domain.value_objects.counting_result import CountingResult
from src.domain.entities.detection import Frame
from src.domain.value_objects.counting_zone import CountingZone

class DisplayProcessor:
    def __init__(self,
                 window_name: str = "EVS Detection",
                 track_history_length: int = 30,
                 display_fps: bool = True):
        self.window_name = window_name
        self.colors = np.random.randint(0, 255, size=(100, 3))
        self.track_paths: Dict[int, List] = {}
        self.track_history_length = track_history_length
        self.display_fps = display_fps

    def display_frame(self,
                     frame: Frame,
                     counting_data: Optional[Union[List[CountingZone], CountingResult]] = None,
                     display_tracks: bool = True) -> bool:
        """
        Display frame with detections and counting information.
        Args:
            frame: Frame to display
            counting_data: Either List[CountingZone] for line counting or CountingResult for frame counting
            display_tracks: Whether to display tracking paths
        """
        display_img = frame.data.copy()

        if frame.detections:
            self._draw_detections(display_img, frame.detections, display_tracks)

        # Draw counting information based on type
        if counting_data is not None:
            if isinstance(counting_data, list):  # CountingZone list
                self._draw_counting_zones(display_img, counting_data)
            else:  # CountingResult
                self._draw_counting_result(display_img, counting_data)

        # Draw FPS if enabled
        if self.display_fps and frame.fps > 0:
            cv2.putText(
                display_img,
                f"FPS: {frame.fps:.1f}",
                (20, 40),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 255, 0),
                2
            )

        cv2.imshow(self.window_name, display_img)
        return not (cv2.waitKey(1) & 0xFF == ord('q'))
    
    def _draw_detections(self, image: np.ndarray, detections: List, display_tracks: bool) -> None:
        """
        Draw detections, tracking IDs, and track paths on the image.
        For non-tracked detections, just draw the bounding box.
        """
        for det in detections:
            if det.track_id is not None:
                color = self.colors[det.track_id % len(self.colors)].tolist()
            else:
                color = (0, 255, 0) 
                
            # Draw bounding box
            x1, y1 = int(det.bbox.x1), int(det.bbox.y1)
            x2, y2 = int(det.bbox.x2), int(det.bbox.y2)
            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)

            # Draw confidence score
            conf_text = f"{det.bbox.confidence:.2f}"
            cv2.putText(
                image,
                "Person",
                (x1, y1 - 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                color,
                2
            )

            if det.track_id is not None:
                id_text = f"ID: {det.track_id}"
                cv2.putText(
                    image,
                    id_text,
                    (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    color,
                    2
                )

                # Handle track path if display_tracks is enabled
                if display_tracks:
                    center = ((x1 + x2) // 2, (y1 + y2) // 2)
                    
                    # Initialize track path if needed
                    if det.track_id not in self.track_paths:
                        self.track_paths[det.track_id] = []
                    
                    # Update track path
                    self.track_paths[det.track_id].append(center)
                    
                    # Limit path length using track_history_length
                    if len(self.track_paths[det.track_id]) > self.track_history_length:
                        self.track_paths[det.track_id].pop(0)

                    # Draw track path
                    points = np.array(self.track_paths[det.track_id])
                    if len(points) > 1:  # Only draw if we have at least 2 points
                        for i in range(1, len(points)):
                            # Make line thinner towards older points
                            thickness = int(np.sqrt(float(i + 1) * 2))
                            cv2.line(
                                image,
                                tuple(points[i-1]),
                                tuple(points[i]),
                                color,
                                thickness
                            )

            if display_tracks:
                current_tracks = {det.track_id for det in detections if det.track_id is not None}
                self.track_paths = {
                    k: v for k, v in self.track_paths.items() 
                    if k in current_tracks
                }
                
    def _draw_counting_zones(self, image: np.ndarray, zones: List[CountingZone]) -> None:
        for zone in zones:
            # Draw zone polygon
            points = np.array(zone.points, np.int32)
            cv2.polylines(image, [points], True, (0, 255, 255), 2)

            # Draw counts
            text = f"{zone.name} - In: {zone.in_count}, Out: {zone.out_count}"
            text_pos = (zone.points[0][0], zone.points[0][1] - 10)
            cv2.putText(
                image,
                text,
                text_pos,
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (0, 255, 255),
                2
            )

    def _draw_counting_result(self, image: np.ndarray, result: CountingResult) -> None:
        # Draw current count
        cv2.putText(
            image,
            f"Current Count: {result.current_count}",
            (20, 80),
            cv2.FONT_HERSHEY_SIMPLEX,
            1,
            (0, 255, 255),
            1
        )
        
        # Draw smoothed count if available
        # if hasattr(result, 'smoothed_count'):
        #     cv2.putText(
        #         image,
        #         f"Smoothed Count: {result.smoothed_count}",
        #         (20, 120),
        #         cv2.FONT_HERSHEY_SIMPLEX,
        #         1,
        #         (0, 255, 255),
        #         2
        #     )

        # Draw max count
        # cv2.putText(
        #     image,
        #     f"Max Count: {result.max_count}",
        #     (20, 160),
        #     cv2.FONT_HERSHEY_SIMPLEX,
        #     1,
        #     (0, 255, 255),
        #     2
        # )
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\interface\cli\main.py
--------------------------------------------------------------------------------
# code coming soon
--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\interface\gui\gui.py
--------------------------------------------------------------------------------
import json
from queue import Empty
import time
import traceback
import cv2
import numpy as np
from PyQt5.QtWidgets import (QMainWindow, QWidget, QVBoxLayout, 
                           QHBoxLayout, QPushButton, QLabel, QFileDialog, 
                           QSpinBox, QDoubleSpinBox, QCheckBox, QLineEdit,
                           QGroupBox, QFormLayout, QMessageBox, QScrollArea, 
                           QSizePolicy, QComboBox, QRadioButton, QDialog
                           )
from PyQt5.QtCore import Qt, QThread, pyqtSignal, QTimer, QEventLoop
from PyQt5.QtGui import QImage, QPixmap

from src.infrastructure.utils.system_utils import restart_usb_devices
from src.interface.gui.dialog.loading_dialog import LoadingDialog
from src.interface.gui.dialog.login_dialog import LoginDialog
from src.interface.gui.styles import STYLE_SHEET
from src.application.services.main_service import LogicFactoryService
from src.domain.value_objects.config import RadarConfig
from src.domain.entities.entities import ProcessingParams
from src.infrastructure.utils.logging_config import setup_logger

import os
os.environ['CUPY_ACCELERATORS'] = 'cutensor'
os.environ['CUDA_PATH'] = r'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1'

logger = setup_logger('gui_logger')

    



class ProcessingThread(QThread):
    frame_ready = pyqtSignal(np.ndarray)
    finished = pyqtSignal(bool, str)
    
    def __init__(self, base_path, config, input_path, output_path, params: ProcessingParams):
        super().__init__()
        self.config = config
        self.base_path = base_path
        self.input_path = input_path
        self.output_path = output_path  
        self.params = params
        self.running = True
        
        self.video_writer = None
        self.combined_width = 1920
        self.combined_height = 1080

    def run(self):
        self.stream_processor = None
        video_writer = None
        
        try:
            combined_width = None
            combined_height = None

            if self.output_path and self.output_path.strip():
                try:
                    output_dir = os.path.dirname(self.output_path)
                    if output_dir:
                        os.makedirs(output_dir, exist_ok=True)

                except Exception as e:
                    print(f"Output directory error: {str(e)}")
                    self.finished.emit(False, f"Output directory error: {str(e)}")
                    return

            # Rest of the code remains the same
            self.stream_processor = LogicFactoryService(
                self.base_path,
                frame_queue_size=5,
                resize_dims=(self.params.camera_width, self.params.camera_height),
                target_fps=30,
                input_filename=self.input_path,
                args=self.params
            )

            if self.input_path != "":
                print("Run file")
                result_queue = self.stream_processor.start_ui_one_thread()
            else:
                print("Run camera")
                result_queue = self.stream_processor.start_ui()

            last_result = None
            frame_count = 0
            frames_to_write = []  
            
            while self.running:
                try:
                    try:
                        result = result_queue.get(timeout=0.1)
                        
                        if isinstance(result[0], str) and result[0] == "EOF":
                            print("Reached end of file")
                            break
                        
                        last_result = result
                    except Empty:
                        if last_result is None:
                            time.sleep(0.05)
                            continue
                        result = last_result

                    tracked_frame, _, _, _, _ = result

                    if combined_width is None or combined_height is None:
                        combined_height, combined_width = tracked_frame.shape[:2]
                        print(f"Detected frame size: {combined_width}x{combined_height}")

                        if self.output_path and self.output_path.strip():
                            try:
                                codecs = [
                                    cv2.VideoWriter_fourcc(*'mp4v'),
                                    cv2.VideoWriter_fourcc(*'avc1'),
                                    cv2.VideoWriter_fourcc(*'XVID'),
                                    cv2.VideoWriter_fourcc(*'MJPG')
                                ]

                                video_writer = None
                                for codec in codecs:
                                    try:
                                        video_writer = cv2.VideoWriter(
                                            self.output_path,
                                            codec,
                                            20.0,  # FPS
                                            (combined_width, combined_height),
                                            True  
                                        )
                                        
                                        if video_writer.isOpened():
                                            print(f"Successfully opened video writer with codec {codec}")
                                            break
                                    except Exception as codec_error:
                                        print(f"Failed with codec {codec}: {codec_error}")

                                if video_writer is None or not video_writer.isOpened():
                                    raise IOError("Could not open video writer with any codec")
                            
                            except Exception as e:
                                print(f"Video writer error: {str(e)}")
                                self.finished.emit(False, f"Video writer error: {str(e)}")
                                return

                    # Emit frame for UI display
                    self.frame_ready.emit(tracked_frame)
                    
                    # Collect frames to write
                    if video_writer is not None:
                        try:
                            frames_to_write.append(tracked_frame)
                            frame_count += 1
                        except Exception as write_error:
                            print(f"Frame preparation error: {write_error}")

                    if not self.running:
                        break

                except Exception as processing_error:
                    print(f"Processing loop error: {processing_error}")
                    break

            # Write collected frames
            if video_writer is not None and frames_to_write:
                print(f"Writing {len(frames_to_write)} frames to video")
                for frame in frames_to_write:
                    video_writer.write(frame)

            # Successful completion
            self.finished.emit(True, f"Processing completed. Frames processed: {frame_count}")

        except Exception as e:
            error_message = f"Unexpected error during processing: {str(e)}"
            print(error_message)
            traceback.print_exc()
            self.finished.emit(False, error_message)

        finally:
            try:
                # Stop stream processor
                if self.stream_processor:
                    self.stream_processor.stop()
                
                # Release video writer
                if video_writer:
                    video_writer.release()
                
                # Verify file
                if self.output_path and os.path.exists(self.output_path):
                    file_size = os.path.getsize(self.output_path)
                    print(f"Video file saved. Size: {file_size} bytes")
                    
                    try:
                        cap = cv2.VideoCapture(self.output_path)
                        if cap.isOpened():
                            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                            print(f"Total frames in video: {total_frames}")
                            cap.release()
                        else:
                            print("Cannot open the saved video file")
                    except Exception as verify_error:
                        print(f"Video verification error: {verify_error}")
                else:
                    print(f"Video file not found at: {self.output_path}")
            
            except Exception as cleanup_error:
                print(f"Cleanup error: {cleanup_error}")     
                      
    def stop(self):
        self.running = False
        
class EventProcessorGUI(QMainWindow):
    def __init__(self, base_path=""):
        super().__init__()
        self.is_admin = False
        self.setWindowTitle("WQ Tivi - Deep Learning Laboratory")
        self.setGeometry(100, 100, 1600, 900)
        self.setStyleSheet(STYLE_SHEET)
        self.base_path = base_path
        
        main_widget = QWidget()
        self.setCentralWidget(main_widget)
        layout = QHBoxLayout(main_widget)
        layout.setSpacing(20)
        
        left_scroll = QScrollArea()
        left_scroll.setWidgetResizable(True)
        left_scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAsNeeded)
        left_scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAsNeeded)
        left_scroll.setMaximumWidth(400)
        
        left_widget = QWidget()
        left_layout = QVBoxLayout(left_widget)
        left_layout.setContentsMargins(20, 20, 20, 20)
        
        self.create_io_section(left_layout)
        self.create_processing_section(left_layout)
        self.create_camera_section(left_layout)
        self.create_model_section(left_layout)
        self.create_performance_section(left_layout)
        self.create_control_section(left_layout)
        
        left_scroll.setWidget(left_widget)
        layout.addWidget(left_scroll)
        
        right_panel = QWidget()
        right_layout = QVBoxLayout(right_panel)
        right_layout.setContentsMargins(20, 20, 20, 20)
        
        self.admin_link = QLabel()  # Save reference to admin link
        self.admin_link.setText('<a href="#" style="color: #3498db;">Is Admin?</a>')
        self.admin_link.setOpenExternalLinks(False)
        self.admin_link.linkActivated.connect(self.handle_admin_click)
        self.admin_link.setVisible(False)
        right_layout.addWidget(self.admin_link, alignment=Qt.AlignRight)
        
        
        self.create_display_section(right_layout)
        layout.addWidget(right_panel)
        
        self.processing_thread = None
        self.config = RadarConfig()
        
        if not self.check_admin_file():
            self.update_ui_visibility()
            
        self.loading_dialog = None
    
    def check_admin_file(self):
        try:
            admin_file = os.path.join(self.base_path, "admin.json")
            if os.path.exists(admin_file):
                with open(admin_file, 'r') as f:
                    admin_data = json.load(f)
                    if admin_data.get('username') == 'deeplab' and admin_data.get('password') == 'dlmsllab321@':
                        self.is_admin = True
                        self.show_fps.setChecked(True)
                        self.show_timing.setChecked(True)
                        return True
            return False
        except Exception as e:
            print(f"Error checking admin file: {e}")
            return False
    
    def handle_admin_click(self):
        if self.is_admin:
            # Handle logout
            self.is_admin = False
            self.show_fps.setChecked(False)
            self.show_timing.setChecked(False)
            self.admin_link.setText('<a href="#" style="color: #3498db;">Is Admin?</a>')
            self.update_ui_visibility()
            QMessageBox.information(self, "Success", "Logged out successfully")
        else:
            # Handle login
            self.show_login()
    
    def show_login(self):
        dialog = LoginDialog(self.base_path, self)
        if dialog.exec_() == QDialog.Accepted:
            self.is_admin = True
            self.show_fps.setChecked(True)
            self.show_timing.setChecked(True)
            self.admin_link.setText('<a href="#" style="color: #3498db;">Logout</a>')
            self.update_ui_visibility()
            QMessageBox.information(self, "Success", "Logged in as admin")
            
    def update_ui_visibility(self):
        # Show/hide advanced settings based on admin status
        # Processing Parameters
        for i in range(self.proc_layout.rowCount()):
            label_item = self.proc_layout.itemAt(i, QFormLayout.LabelRole)
            field_item = self.proc_layout.itemAt(i, QFormLayout.FieldRole)
            
            if label_item and field_item:
                label = label_item.widget()
                field = field_item.widget()
                
                # Always show Output Height
                if label.text() == "Output Height:":
                    continue
                    
                # Show/hide other parameters based on admin status
                label.setVisible(self.is_admin)
                field.setVisible(self.is_admin)
        
        # Model Settings
        self.model_group.setVisible(self.is_admin)
        
        # Performance Metrics
        self.perf_group.setVisible(self.is_admin)
        # pass

    def create_io_section(self, parent_layout):
        io_group = QGroupBox("Input/Output")
        io_layout = QVBoxLayout()
        
        # Input source selection
        input_source_layout = QHBoxLayout()
        self.camera_radio = QRadioButton("Camera")
        self.file_radio = QRadioButton("Input File")
        self.camera_radio.setChecked(True)
        
        input_source_layout.addWidget(self.camera_radio)
        input_source_layout.addWidget(self.file_radio)
        io_layout.addLayout(input_source_layout)
        
        # Input file selection
        input_file_layout = QHBoxLayout()
        self.input_path = QLineEdit()
        self.input_path.setPlaceholderText("Input file...")
        self.input_path.setEnabled(False)
        
        browse_btn = QPushButton("Browse")
        browse_btn.setMaximumWidth(80)
        browse_btn.clicked.connect(self.browse_input_file)
        browse_btn.setEnabled(False)
        self.browse_input_btn = browse_btn
        
        input_file_layout.addWidget(self.input_path)
        input_file_layout.addWidget(browse_btn)
        io_layout.addLayout(input_file_layout)
        
        # Add spacing between input and output
        # io_layout.addSpacing(10)  # Add 10 pixels spacing
        
        # Output file selection
        output_layout = QHBoxLayout()
        self.output_path = QLineEdit()
        self.output_path.setPlaceholderText("Output file (output.mp4)")
        
        output_browse_btn = QPushButton("Browse")
        output_browse_btn.setMaximumWidth(80)
        output_browse_btn.clicked.connect(self.browse_output_file)
        
        output_layout.addWidget(self.output_path)
        output_layout.addWidget(output_browse_btn)
        io_layout.addLayout(output_layout)
        
        # Connect signals
        self.camera_radio.toggled.connect(self.on_input_source_changed)
        self.file_radio.toggled.connect(self.on_input_source_changed)
        
        io_group.setLayout(io_layout)
        parent_layout.addWidget(io_group)
        
    def on_input_source_changed(self):
        is_file_mode = self.file_radio.isChecked()
        self.input_path.setEnabled(is_file_mode)
        self.browse_input_btn.setEnabled(is_file_mode)
        
        if not is_file_mode:
            self.input_path.clear()  # Clear input path when switching to camera

    def create_processing_section(self, parent_layout):
        proc_group = QGroupBox("Processing Parameters")
        self.proc_layout = QFormLayout()  # Save reference to layout

        
        self.output_height = QSpinBox()
        self.output_height.setRange(480, 4096)
        self.output_height.setValue(720)
        self.output_height.setSingleStep(120)
        self.proc_layout.addRow("Output Height:", self.output_height)
        
        params = [
            ("Trim Angle:", "trim_angle", QDoubleSpinBox, (0.1, 10.0, 1.0)),
            ("Max Objects:", "max_objects", QSpinBox, (1, 10, 3)),
            ("Neighborhood Size:", "neighborhood_size", QSpinBox, (1, 10, 3)),
            ("Time Tolerance:", "time_tolerance", QSpinBox, (100, 5000, 1000)),
            ("Buffer Size:", "buffer_size", QSpinBox, (64, 1024, 256))
        ]
        
        for label, attr_name, widget_class, values in params:
            widget = widget_class()
            widget.setRange(values[0], values[1])
            widget.setValue(values[2])
            setattr(self, attr_name, widget)
            self.proc_layout.addRow(label, widget)
        
        self.use_filter = QCheckBox()
        self.use_filter.setChecked(True)
        self.use_filter.setStyleSheet("""
            QCheckBox::indicator:checked {
                background-color: #2ecc71;
                border: 2px solid #27ae60;
            }
        """)
        
        self.proc_layout.addRow("Use Filter:", self.use_filter)
        
        # self.use_bbox = QCheckBox()
        # self.use_bbox.setChecked(True)
        # self.use_bbox.setStyleSheet("""
        #     QCheckBox::indicator:checked {
        #         background-color: #2ecc71;
        #         border: 2px solid #27ae60;
        #     }
        # """)
        
        # self.proc_layout.addRow("Use Bbox:", self.use_bbox)
        
        proc_group.setLayout(self.proc_layout)
        parent_layout.addWidget(proc_group)

    def create_camera_section(self, parent_layout):
        camera_group = QGroupBox("Camera Settings")
        camera_layout = QFormLayout()
        
        self.resolutions = ["320x320", "480x480", "640x640", "480x640", "640x480", 
                          "800x600", "1024x768", "1280x720", "1280x960", "1440x1080", 
                          "1920x1080", "2560x1440"]
        
        self.resolution_combo = QComboBox()
        self.resolution_combo.addItems(self.resolutions)
        self.resolution_combo.setCurrentText("320x320")
        self.resolution_combo.setStyleSheet("""
            QComboBox {
                padding: 5px;
                border: 2px solid #e0e0e0;
                border-radius: 4px;
                background-color: white;
                min-width: 120px;
            }
            QComboBox::drop-down {
                border: none;
                padding-right: 10px;
            }
            QComboBox::down-arrow {
                image: none;
                border: none;
                border-left: 2px solid #999;
                border-bottom: 2px solid #999;
                width: 8px;
                height: 8px;
                transform: rotate(-45deg);
                margin-right: 8px;
            }
            QComboBox:hover {
                border-color: #3498db;
            }
        """)
        self.resolution_combo.currentTextChanged.connect(self.update_resolution)
        camera_layout.addRow("Resolution:", self.resolution_combo)
        
        camera_group.setLayout(camera_layout)
        parent_layout.addWidget(camera_group)
        
        self.current_width = 320
        self.current_height = 320

    def create_model_section(self, parent_layout):
        self.model_group = QGroupBox("Model Settings")
        model_layout = QFormLayout()
        
        self.yolo_conf = QDoubleSpinBox()
        self.yolo_conf.setRange(0.1, 1.0)
        self.yolo_conf.setValue(0.6)
        self.yolo_conf.setSingleStep(0.1)
        model_layout.addRow("Confidence:", self.yolo_conf)
        
        self.yolo_iou = QDoubleSpinBox()
        self.yolo_iou.setRange(0.1, 1.0)
        self.yolo_iou.setValue(0.5)
        self.yolo_iou.setSingleStep(0.1)
        model_layout.addRow("IOU:", self.yolo_iou)
        
        self.model_group.setLayout(model_layout)
        parent_layout.addWidget(self.model_group)

    def create_performance_section(self, parent_layout):
        self.perf_group = QGroupBox("Performance Metrics")
        perf_layout = QFormLayout()
        
        self.show_fps = QCheckBox()
        self.show_fps.setChecked(False)
        perf_layout.addRow("Show FPS:", self.show_fps)
        
        self.show_timing = QCheckBox()
        self.show_timing.setChecked(False)
        perf_layout.addRow("Show Timing:", self.show_timing)
        
        self.perf_group.setLayout(perf_layout)
        parent_layout.addWidget(self.perf_group)

    def create_control_section(self, parent_layout):
        control_group = QGroupBox("Controls")
        control_layout = QVBoxLayout()
        
        self.start_button = QPushButton("▶ Start")
        self.start_button.clicked.connect(self.start_processing)
        
        self.stop_button = QPushButton("⬛ Stop")
        self.stop_button.clicked.connect(self.stop_processing)
        self.stop_button.setEnabled(False)
        
        self.stop_button.setStyleSheet("""
            QPushButton {
                background-color: #e74c3c;
            }
            QPushButton:hover {
                background-color: #c0392b;
            }
        """)
                
        self.status_label = QLabel("Ready")
        self.status_label.setObjectName("status_label")
        self.status_label.setAlignment(Qt.AlignCenter)
        
        control_layout.addWidget(self.start_button)
        control_layout.addWidget(self.stop_button)
        control_layout.addWidget(self.status_label)
        
        control_group.setLayout(control_layout)
        parent_layout.addWidget(control_group)

    def create_display_section(self, parent_layout):
        display_group = QGroupBox("Video Display")
        display_layout = QVBoxLayout()
        
        scroll_area = QScrollArea()
        scroll_area.setWidgetResizable(True)
        scroll_area.setStyleSheet("""
            QScrollArea {
                border: none;
                background: transparent;
            }
            QScrollBar {
                background: #f0f0f0;
            }
            QScrollBar:horizontal {
                height: 12px;
            }
            QScrollBar:vertical {
                width: 12px;
            }
        """)
        container = QWidget()
        container_layout = QVBoxLayout(container)
        
        self.display_label = QLabel()
        self.display_label.setAlignment(Qt.AlignCenter)
        self.display_label.setMinimumSize(640, 480)
        self.display_label.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        self.display_label.setStyleSheet("""
            QLabel {
                background-color: #2c3e50;
                border-radius: 8px;
                padding: 10px;
            }
        """)
        container_layout.addWidget(self.display_label)
        scroll_area.setWidget(container)
        display_layout.addWidget(scroll_area)
        display_group.setLayout(display_layout)
        parent_layout.addWidget(display_group)

    def update_resolution(self, resolution_text):
        width, height = map(int, resolution_text.split('x'))
        self.current_width = width
        self.current_height = height

    def browse_input_file(self):
        file_path, _ = QFileDialog.getOpenFileName(self, "Select Input", "", "Event Files (*.raw *.dat)")
        if file_path:
            self.input_path.setText(file_path)

    def browse_output_file(self):
        file_path, _ = QFileDialog.getSaveFileName(self, "Save Output", "", "Video Files (*.mp4)")
        if file_path and not file_path.endswith('.mp4'):
            file_path += '.mp4'
        if file_path:
            self.output_path.setText(file_path)
            
    def validate_paths(self):
        """Validate input and output paths before processing"""
        # If using camera, skip input validation
        if self.camera_radio.isChecked():
            # Only validate output path if provided
            if self.output_path.text():
                if not self.output_path.text().lower().endswith(('.mp4', '.avi')):
                    QMessageBox.warning(
                        self,
                        "Invalid Output",
                        "Output file must be .mp4 or .avi format.",
                        QMessageBox.Ok
                    )
                    return False
                
                # Check if output directory exists
                output_dir = os.path.dirname(self.output_path.text())
                if output_dir and not os.path.exists(output_dir):
                    QMessageBox.warning(
                        self,
                        "Invalid Output",
                        "Output directory does not exist.",
                        QMessageBox.Ok
                    )
                    return False
            return True
        
        # File mode validation
        input_path = self.input_path.text()
        
        # Validate input file
        if not input_path:
            QMessageBox.warning(
                self,
                "Missing Input",
                "Please select an input file.",
                QMessageBox.Ok
            )
            return False
            
        if not os.path.exists(input_path):
            QMessageBox.warning(
                self,
                "Invalid Input",
                "Input file does not exist.",
                QMessageBox.Ok
            )
            return False
            
        if not input_path.lower().endswith(('.raw', '.dat')):
            QMessageBox.warning(
                self,
                "Invalid Input",
                "Input file must be a .raw or .dat file.",
                QMessageBox.Ok
            )
            return False
        
        return True

    def start_processing(self):
        self.loading_dialog = LoadingDialog(
            self, 
            "Starting processing...", 
            duration=6000  
        )
        self.loading_dialog.show()

        loop = QEventLoop()
        
        timer = QTimer()
        timer.setSingleShot(True)
        timer.timeout.connect(lambda: (
            restart_usb_devices(),
            loop.quit()
        ))
        timer.start(10)  
        
        loop.exec_()

        if not self.validate_paths():
            return
        
        # Prepare processing parameters
        params = ProcessingParams(
            neighborhood_size=self.neighborhood_size.value(),
            time_tolerance=self.time_tolerance.value(),
            buffer_size=self.buffer_size.value(),
            use_filter=self.use_filter.isChecked(),
            output_height=self.output_height.value(),
            show_fps=self.show_fps.isChecked(),
            show_timing=self.show_timing.isChecked(),
            yolo_conf=self.yolo_conf.value(),
            yolo_iou=self.yolo_iou.value(),
            camera_width=self.current_width,
            camera_height=self.current_height
        )
        
        output_path = self.output_path.text().strip()
        
        # Create processing thread
        self.processing_thread = ProcessingThread(self.base_path,
            self.config,
            "" if self.camera_radio.isChecked() else self.input_path.text(),
            output_path, 
            params
        )
        
        # Connect signals
        self.processing_thread.frame_ready.connect(self.update_display)
        self.processing_thread.finished.connect(self.processing_finished)
        
        # Start thread
        self.processing_thread.start()
        
        # Disable buttons
        self.start_button.setEnabled(False)
        self.stop_button.setEnabled(True)
        self.status_label.setText("Processing...")

    def stop_processing(self):
        self.loading_dialog = LoadingDialog(
            self, 
            "Stopping processing...", 
            duration=3000 
        )
        self.loading_dialog.show()

        loop = QEventLoop()
        
        timer = QTimer()
        timer.setSingleShot(True)
        timer.timeout.connect(lambda: (
            self._stop_processing_internal(),
            loop.quit()
        ))
        timer.start(10)  
        
        loop.exec_()

    def _stop_processing_internal(self):
        if self.processing_thread and self.processing_thread.isRunning():
            # Stop processing
            self.processing_thread.running = False
            
            self.processing_thread.wait(5000) 
            
            if self.processing_thread.isRunning():
                print("Forcefully terminating processing thread")
                self.processing_thread.terminate()
            
            # Stop stream processor
            if hasattr(self.processing_thread, 'stream_processor'):
                self.processing_thread.stream_processor.stop()
            
            # Reset UI
            self.status_label.setText("Stopped")
            self.start_button.setEnabled(True)
            self.stop_button.setEnabled(False)

    def processing_finished(self, success, message):
        if hasattr(self, 'loading_dialog') and self.loading_dialog:
            pass
        
        # Update UI
        self.status_label.setText(message)
        self.start_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        
        # Show error message if processing failed
        if not success:
            QMessageBox.critical(self, "Error", message)

    def closeEvent(self, event):
        # Close loading dialog if it's open
        if hasattr(self, 'loading_dialog') and self.loading_dialog:
            self.loading_dialog.close()
        
        # Stop processing
        self.stop_processing()
        event.accept()
        
    def update_display(self, frame):
        try:
            self.frame_buffer = frame.copy()  # Create a copy to avoid memory conflicts
            
            frame = cv2.cvtColor(self.frame_buffer, cv2.COLOR_BGR2RGB)
            height, width, channel = frame.shape
            bytes_per_line = 3 * width
            
            # Use a try-except block for QImage creation
            try:
                q_image = QImage(frame.data, width, height, bytes_per_line, QImage.Format_RGB888)
                scaled_pixmap = QPixmap.fromImage(q_image).scaled(
                    self.display_label.size(),
                    Qt.KeepAspectRatio,
                    Qt.SmoothTransformation
                )
                self.display_label.setPixmap(scaled_pixmap)
            except Exception as e:
                print(f"QImage creation error: {e}")
                
        except Exception as e:
            print(f"Frame processing error: {e}")

    def processing_finished(self, success, message):
        self.status_label.setText(message)
        self.start_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        if not success:
            QMessageBox.critical(self, "Error", message)

    def closeEvent(self, event):
        self.stop_processing()
        event.accept()


--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\interface\gui\styles.py
--------------------------------------------------------------------------------
STYLE_SHEET = """
QMainWindow {
    background-color: #f0f0f0;
}

QGroupBox {
    background-color: white;
    border: 2px solid #e0e0e0;
    border-radius: 8px;
    margin-top: 1em;
    font-weight: bold;
}

QGroupBox::title {
    subcontrol-origin: margin;
    left: 10px;
    padding: 0 5px 0 5px;
    color: #2c3e50;
}

QPushButton {
    background-color: #3498db;
    color: white;
    border: none;
    padding: 8px 16px;
    border-radius: 4px;
    font-weight: bold;
    min-width: 100px;
}

QPushButton:hover {
    background-color: #2980b9;
}

QPushButton:disabled {
    background-color: #bdc3c7;
}

QLineEdit {
    padding: 6px;
    border: 2px solid #e0e0e0;
    border-radius: 4px;
    background-color: white;
}

QSpinBox, QDoubleSpinBox {
    padding: 5px;
    border: 2px solid #e0e0e0;
    border-radius: 4px;
    background-color: white;
}

QLabel {
    color: #2c3e50;
}

QCheckBox {
    spacing: 8px;
}

QCheckBox::indicator {
    width: 18px;
    height: 18px;
}

#status_label {
    font-weight: bold;
    color: #27ae60;
    padding: 10px;
    background-color: #f8f9fa;
    border-radius: 4px;
}

QRadioButton {
    spacing: 8px;
    color: #2c3e50;
}

QRadioButton::indicator {
    width: 18px;
    height: 18px;
}

QRadioButton::indicator:checked {
    background-color: #3498db;
    border: 2px solid #2980b9;
    border-radius: 9px;
}

QRadioButton::indicator:unchecked {
    background-color: white;
    border: 2px solid #bdc3c7;
    border-radius: 9px;
}
"""

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\interface\gui\dialog\loading_dialog.py
--------------------------------------------------------------------------------
from PyQt5.QtWidgets import (QVBoxLayout,  QLabel, QDialog, QProgressBar)
from PyQt5.QtCore import Qt, QTimer

class LoadingDialog(QDialog):
    def __init__(self, parent=None, message="Processing...", duration=3000):
        super().__init__(parent)
        self.setWindowTitle("Processing")
        self.setModal(True)  
        self.setFixedSize(300, 150)
        self.setWindowFlags(
            Qt.Window | 
            Qt.WindowStaysOnTopHint | 
            Qt.CustomizeWindowHint
        )
        
        layout = QVBoxLayout()
        
        # Message label
        self.message_label = QLabel(message)
        self.message_label.setAlignment(Qt.AlignCenter)
        layout.addWidget(self.message_label)
        
        # Progress bar
        self.progress_bar = QProgressBar()
        self.progress_bar.setRange(0, 0)  # Indeterminate progress bar
        layout.addWidget(self.progress_bar)
        
        self.setLayout(layout)
        
        # Style
        self.setStyleSheet("""
            QDialog {
                background-color: white;
                border-radius: 10px;
            }
            QLabel {
                font-size: 14px;
                color: #2c3e50;
            }
            QProgressBar {
                border: 2px solid #3498db;
                border-radius: 5px;
                text-align: center;
            }
            QProgressBar::chunk {
                background-color: #3498db;
            }
        """)
        
        # Timer to auto-close dialog
        self.close_timer = QTimer(self)
        self.close_timer.setSingleShot(True)
        self.close_timer.timeout.connect(self.close)
        self.close_timer.start(duration)

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\src\interface\gui\dialog\login_dialog.py
--------------------------------------------------------------------------------

from base64 import b64decode, b64encode
import hashlib
import json
import os
import time
from PyQt5.QtWidgets import (QVBoxLayout, QPushButton, QLabel, QLineEdit, 
                             QMessageBox, QDialog)
from PyQt5.QtCore import Qt

class CredentialsManager:
    def __init__(self, base_path):
        self.app_dir = base_path
        self.credentials_file = os.path.join(self.app_dir, "credentials.json")
        self.initialize_credentials()
    
    def initialize_credentials(self):
        """Initialize credentials if file doesn't exist"""
        try:
            if not os.path.exists(self.credentials_file):
                # Try to create directory if it doesn't exist
                os.makedirs(os.path.dirname(self.credentials_file), exist_ok=True)
                
                salt = os.urandom(32)
                password = "dlmsllab321@"
                hashed = self.hash_password(password, salt)
                
                credentials = {
                    "deeplab": {
                        "salt": b64encode(salt).decode('utf-8'),
                        "hash": b64encode(hashed).decode('utf-8')
                    }
                }
                
                try:
                    with open(self.credentials_file, 'w') as f:
                        json.dump(credentials, f, indent=4)
                    print(f"Created credentials file at: {self.credentials_file}")
                except PermissionError:
                    print("Warning: Could not create credentials file - Permission denied")
                except Exception as e:
                    print(f"Warning: Could not create credentials file - {str(e)}")
        except Exception as e:
            print(f"Error initializing credentials: {e}")
    
    def hash_password(self, password: str, salt: bytes) -> bytes:
        """Hash password with salt using PBKDF2"""
        return hashlib.pbkdf2_hmac(
            'sha256', 
            password.encode('utf-8'), 
            salt, 
            100000  # Number of iterations
        )
    
    def verify_credentials(self, username: str, password: str) -> bool:
        """Verify username and password"""
        try:
            print(f"Verifying credentials for username: {username}")
            
            with open(self.credentials_file, 'r') as f:
                credentials = json.load(f)
            print(f"Loaded credentials: {credentials.keys()}")
            
            if username not in credentials:
                print(f"Username {username} not found")
                return False
            
            stored = credentials[username]
            salt = b64decode(stored['salt'])
            stored_hash = b64decode(stored['hash'])
            
            hashed = self.hash_password(password, salt)
            
            result = stored_hash == hashed
            print(f"Verification result: {result}")
            return result
                
        except Exception as e:
            print(f"Error verifying credentials: {e}")
            return False
    
class LoginDialog(QDialog):
    MAX_ATTEMPTS = 3
    LOCKOUT_TIME = 300  # 5 minutes
    
    def __init__(self, base_path=None, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Admin Login")
        self.setModal(True)
        self.setFixedWidth(300)
        self.setStyleSheet("""
            QDialog {
                background-color: white;
            }
            QLineEdit {
                padding: 8px;
                border: 2px solid #e0e0e0;
                border-radius: 4px;
                margin: 5px 0px;
            }
            QPushButton {
                background-color: #3498db;
                color: white;
                border: none;
                padding: 8px;
                border-radius: 4px;
                min-width: 100px;
                margin-top: 10px;
            }
            QPushButton:hover {
                background-color: #2980b9;
            }
        """)
        
        self.credentials_manager = CredentialsManager(base_path)
        self.attempt_count = 0
        self.last_attempt_time = 0
        
        layout = QVBoxLayout(self)
        layout.setContentsMargins(20, 20, 20, 20)
        layout.setSpacing(10)
        
        # Title
        title = QLabel("Admin Authentication")
        title.setStyleSheet("font-size: 16px; font-weight: bold; margin-bottom: 10px;")
        layout.addWidget(title, alignment=Qt.AlignCenter)
        
        # Username
        self.username = QLineEdit()
        self.username.setPlaceholderText("Username")
        layout.addWidget(self.username)
        
        # Password
        self.password = QLineEdit()
        self.password.setPlaceholderText("Password")
        self.password.setEchoMode(QLineEdit.Password)
        self.password.returnPressed.connect(self.check_credentials)  # Allow Enter key
        layout.addWidget(self.password)
        
        # Login button
        self.login_btn = QPushButton("Login")
        self.login_btn.clicked.connect(self.check_credentials)
        layout.addWidget(self.login_btn)
        
        self.setLayout(layout)
    
    def check_lockout(self) -> bool:
        if self.attempt_count >= self.MAX_ATTEMPTS:
            time_since_last = time.time() - self.last_attempt_time
            if time_since_last < self.LOCKOUT_TIME:
                remaining = int(self.LOCKOUT_TIME - time_since_last)
                QMessageBox.warning(
                    self,
                    "Account Locked",
                    f"Too many failed attempts. Please try again in {remaining} seconds."
                )
                return True
            else:
                self.attempt_count = 0
        return False
    
    def check_credentials(self):
        if self.check_lockout():
            return
            
        username = self.username.text().strip()
        password = self.password.text()
        
        if not username or not password:
            QMessageBox.warning(self, "Error", "Please enter both username and password")
            return
        
        if self.credentials_manager.verify_credentials(username, password):
            self.accept()
        else:
            self.attempt_count += 1
            self.last_attempt_time = time.time()
            remaining_attempts = self.MAX_ATTEMPTS - self.attempt_count
            
            QMessageBox.warning(
                self, 
                "Error", 
                f"Invalid credentials. {remaining_attempts} attempts remaining."
            )
            
            self.password.clear()   

--------------------------------------------------------------------------------
File: C:\Users\ngoct\source\working\evs-proj\tests\sum_code.py
--------------------------------------------------------------------------------
import os

def find_py_files(root_dir):
   py_files = []
   for root, dirs, files in os.walk(root_dir):
       for file in files:
           if file.endswith('.py'):
               py_files.append(os.path.join(root, file))
   return py_files

def combine_py_contents(files, output='combined_py_files.txt'):
   with open(output, 'w', encoding='utf-8') as out:
       for file in files:
           out.write(f'\n{"-"*80}\n')
           out.write(f'File: {file}\n')
           out.write(f'{"-"*80}\n')
           
           try:
               with open(file, 'r', encoding='utf-8') as f:
                   out.write(f.read())
           except Exception as e:
               out.write(f'Error reading file: {str(e)}\n')

py_files = find_py_files(r"C:\Users\ngoct\source\working\evs-proj")
# py_files = ['main.py', 'gui.py', 'onnx_fall/evs_fall.py']
print(py_files)
combine_py_contents(py_files, 'evs.txt')